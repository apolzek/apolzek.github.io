{
    "version": "https://jsonfeed.org/version/1",
    "title": "apolzek",
    "home_page_url": "https://apolzek.github.io/",
    "feed_url": "https://apolzek.github.io/feed.json",
    "description": null,
    "icon": "https://apolzek.github.io/apple-touch-icon.png",
    "favicon": "https://apolzek.github.io/favicon.ico",
    "expired": false,
    
    "author":  {
        "name": "apolzek",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "https://apolzek.github.io/2025/02/24/blackbox-exporter-or-network-exporter",
            "title": "blackbox_exporter or network_exporter ?",
            "summary": "Outside in with blackbox_exporter, Inside out with network_exporter",
            "content_text": "saving your time: Prometheus exporters are applications that extract data from services or processes and expose it in Prometheus format. This article provides a brief analysis of blackbox_exporter and network_exporter, highlighting their use cases and the key differences between themIntroductionThis type of article can be somewhat controversial because there are multiple approaches to working with these exporters. My goal is to share my perspective on where each should be used and the results you can expect.First, itâ€™s important to clarify that weâ€™re dealing with network exporters. While they greatly enhance observability, I believe there are more professional ways to analyze a network. That said, the insights provided by both can help reliability engineers and software engineers quickly and easily understand their environments.So, which one should you use? My answer is simple and direct: both !!blackbox_exporterThe blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP, ICMP and gRPC.As the name suggests, blackbox_exporter is designed to simulate an external user calling an endpoint within your environment. It mimics a simple user request, monitoring the availability and performance of external endpoints such as websites, APIs, or network services. Two key pieces of information it provides are SSL certificate verification and latency measurement. In short, I consider it an external probe that makes requests to something inside your environment. You can use blackbox_exporter inside or outside your main environment, the idea is that it is positioned at the source location to perform checks on a destination, analyzing a real flow.multi-target exporterThe Blackbox Exporter follows the multi-target exporter pattern, allowing Prometheus to probe multiple targets without requiring an agent on each one. It works by receiving targets and a query configuration as parameters, performing network probes (such as HTTP, ICMP, or DNS), and exposing the results as metrics. This approach is particularly useful for monitoring external services, network devices, and internet-facing endpoints.To configure it, the Blackbox Exporter is deployed and set up with specific probe modules in blackbox.yml. Prometheus is then configured to send requests to it via the /probe endpoint, specifying targets dynamically. The exporter executes the probes and returns the results as Prometheus metrics, which can be used for alerting and visualization.Ex: http://localhost:9115/probe?target=github.com&amp;module=http_2xx&amp;debug=true[Prometheus] â†’ [Blackbox Exporter] â†’ [Target]      â†‘              |      |              â†“   Metrics       Probing (HTTP, ICMP, etc.)  (Targets)The blackbox configuration is where you put which monitoring modules you want to use, such as HTTP. The configuration of the targets themselves is in prometheus. Ex:modules:  http_2xx:    prober: http    http:      preferred_ip_protocol: \"ip4\"  http_post_2xx:    prober: http    http:      method: POST  tcp_connect:    prober: tcp  grpc:    prober: grpc    grpc:      tls: true      preferred_ip_protocol: \"ip4\"  ssh_banner:    prober: tcp    tcp:      query_response:      - expect: \"^SSH-2.0-\"      - send: \"SSH-2.0-blackbox-ssh-check\"  ssh_banner_extract:    prober: tcp    timeout: 5s    tcp:      query_response:      - expect: \"^SSH-2.0-([^ -]+)(?: (.*))?$\"        labels:        - name: ssh_version          value: \"${1}\"        - name: ssh_comments          value: \"${2}\"  icmp:    prober: icmp  icmp_ttl5:    prober: icmp    timeout: 5s    icmp:      ttl: 5network_exporternetwork_exporter can also be used in more than one way, but I usually use it for an inside-out analysis. In other words, I use it to track all external dependencies of my application or any workload Iâ€™m running, be it an external API, a database, an endpoint accessed via VPN or proxy, and so on. Itâ€™s particularly useful when an external dependency goes down or when thereâ€™s a network issue along the way, like a lost ACL or hardware failure. While it doesnâ€™t support SSL checks, some of the metrics from blackbox_exporter also exist in network_exporter ðŸ˜ŠExported metrics:ping_up Exporter stateping_targets Number of active targetsping_status: Ping Statusping_rtt_seconds{type=best}: Best round trip time in secondsping_rtt_seconds{type=worst}: Worst round trip time in secondsping_rtt_seconds{type=mean}: Mean round trip time in secondsping_rtt_seconds{type=sum}: Sum round trip time in secondsping_rtt_seconds{type=sd}: Squared deviation in secondsping_rtt_seconds{type=usd}: Standard deviation without correction in secondsping_rtt_seconds{type=csd}: Standard deviation with correction (Bessel's) in secondsping_rtt_seconds{type=range}: Range in secondsping_rtt_snt_count: Packet sent count totalping_rtt_snt_fail_count: Packet sent fail count totalping_rtt_snt_seconds: Packet sent time total in secondsping_loss_percent: Packet loss in percentmtr_up Exporter statemtr_targets Number of active targetsmtr_hops Number of route hopsmtr_rtt_seconds{type=last}: Last round trip time in secondsmtr_rtt_seconds{type=best}: Best round trip time in secondsmtr_rtt_seconds{type=worst}: Worst round trip time in secondsmtr_rtt_seconds{type=mean}: Mean round trip time in secondsmtr_rtt_seconds{type=sum}: Sum round trip time in secondsmtr_rtt_seconds{type=sd}: Squared deviation in secondsmtr_rtt_seconds{type=usd}: Standard deviation without correction in secondsmtr_rtt_seconds{type=csd}: Standard deviation with correction (Bessel's) in secondsmtr_rtt_seconds{type=range}: Range in secondsmtr_rtt_seconds{type=loss}: Packet loss in percentmtr_rtt_snt_count: Packet sent count totalmtr_rtt_snt_fail_count: Packet sent fail count totalmtr_rtt_snt_seconds: Packet sent time total in secondstcp_up Exporter statetcp_targets Number of active targetstcp_connection_status Connection Statustcp_connection_seconds Connection time in secondshttp_get_up Exporter statehttp_get_targets Number of active targetshttp_get_status HTTP Status Code and Connection Statushttp_get_content_bytes HTTP Get Content Size in byteshttp_get_seconds{type=DNSLookup}: DNSLookup connection drill down time in secondshttp_get_seconds{type=TCPConnection}: TCPConnection connection drill down time in secondshttp_get_seconds{type=TLSHandshake}: TLSHandshake connection drill down time in secondshttp_get_seconds{type=TLSEarliestCertExpiry}: TLSEarliestCertExpiry cert expiration time in epochhttp_get_seconds{type=TLSLastChainExpiry}: TLSLastChainExpiry cert expiration time in epochhttp_get_seconds{type=ServerProcessing}: ServerProcessing connection drill down time in secondshttp_get_seconds{type=ContentTransfer}: ContentTransfer connection drill down time in secondshttp_get_seconds{type=Total}: Total connection time in secondsPoCThis PoC showcases monitoring in a Kubernetes environment using network_exporter to analyze connectivity from inside out and blackbox_exporter to simulate external access and assess service availability from outside in.The dependencies of my application, which is behind a service, are a database and an external API. I want information about bothnetwork_exporter  --&gt; Database                  --&gt; External api                  --&gt; Partner websiteTo check if my API is working as expected, I will make a request using blackbox_exporter, simulating an HTTP callblackbox_exporter  --&gt; Ingress                   --&gt; Partner websitewhat are we going to buildYou will test the connectivity and performance of services within a Kubernetes cluster using Blackbox Exporter and Network Exporter to monitor interactions between pods, an external API, and a database, including traffic through an Ingress and Service.flowchart TB subgraph subGraph0[\"Kubernetes cluster\"]        poda(\"pod-a\")        podb(\"pod-b\")        Service[\"Service\"]        network_exporter[\"network_exporter\"]        Ingress[\"Ingress\"]  end subgraph subGraph1[\"VM\"]        DB[(\"Database\")]  end    Ingress --&gt; Service    Service --&gt; poda &amp; podb    poda --&gt; DB &amp; external[\"external api\"]    podb --&gt; DB &amp; external    blackbox_exporter[\"blackbox_exporter\"] --&gt; Ingress    network_exporter --&gt; external &amp; DB    style network_exporter stroke:#2962FF    style DB stroke:#00C853    style external stroke:#D50000    style blackbox_exporter stroke:#2962FF    linkStyle 7 stroke:#D50000,fill:noneCreating a local Kubernetes environmentkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: worker- role: worker- role: control-plane  kubeadmConfigPatches:  - |    kind: InitConfiguration    nodeRegistration:      kubeletExtraArgs:        node-labels: \"ingress-ready=true\"        authorization-mode: \"AlwaysAllow\"  extraPortMappings:  - containerPort: 80    hostPort: 80    protocol: TCP  - containerPort: 443    hostPort: 443    protocol: TCPInstructionsAll details can be found in this repository.Follow the step by step instructions in the READMECreating a basic view in grafanaReferenceshttps://github.com/syepes/network_exporterhttps://github.com/prometheus/blackbox_exporterhttps://github.com/prometheus/blackbox_exporter/blob/master/CONFIGURATION.mdhttps://grafana.com/grafana/dashboards/15297-prometheus-network-exporter/",
            "content_html": "<p><strong>saving your time</strong>: <em>Prometheus exporters are applications that extract data from services or processes and expose it in Prometheus format. This article provides a brief analysis of blackbox_exporter and network_exporter, highlighting their use cases and the key differences between them</em></p><p><img src=\"/assets/img/prometheus-multi-service-exporter.svg\" alt=\"image.png\" /></p><h2 id=\"introduction\">Introduction</h2><p>This type of article can be somewhat controversial because there are multiple approaches to working with these exporters. My goal is to share my perspective on where each should be used and the results you can expect.</p><p>First, itâ€™s important to clarify that weâ€™re dealing with <strong>network exporters</strong>. While they greatly enhance observability, I believe there are more professional ways to analyze a network. That said, the insights provided by both can help reliability engineers and software engineers quickly and easily understand their environments.</p><p>So, which one should you use? My answer is simple and direct: <strong>both</strong> !!</p><p><img src=\"/assets/img/bobsponja-patrick.webp\" alt=\"image.png\" /></p><h2 id=\"blackbox_exporter\">blackbox_exporter</h2><p><em>The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP, ICMP and gRPC.</em></p><p>As the name suggests, <strong>blackbox_exporter</strong> is designed to simulate an external user calling an endpoint within your environment. It mimics a simple user request, monitoring the availability and performance of external endpoints such as websites, APIs, or network services. Two key pieces of information it provides are SSL certificate verification and latency measurement. In short, I consider it an <strong>external probe</strong> that makes requests to something inside your environment. You can use blackbox_exporter inside or outside your main environment, the idea is that it is positioned at the source location to perform checks on a destination, analyzing a real flow.</p><h3 id=\"multi-target-exporter\">multi-target exporter</h3><p>The Blackbox Exporter follows the multi-target exporter pattern, allowing Prometheus to probe multiple targets without requiring an agent on each one. It works by receiving targets and a query configuration as parameters, performing network probes (such as HTTP, ICMP, or DNS), and exposing the results as metrics. This approach is particularly useful for monitoring external services, network devices, and internet-facing endpoints.</p><p>To configure it, the Blackbox Exporter is deployed and set up with specific probe modules in blackbox.yml. Prometheus is then configured to send requests to it via the /probe endpoint, specifying targets dynamically. The exporter executes the probes and returns the results as Prometheus metrics, which can be used for alerting and visualization.</p><p>Ex: <em>http://localhost:9115/probe?target=github.com&amp;module=http_2xx&amp;debug=true</em></p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[Prometheus] â†’ [Blackbox Exporter] â†’ [Target]      â†‘              |      |              â†“   Metrics       Probing (HTTP, ICMP, etc.)  (Targets)</code></pre></div></div><p>The blackbox configuration is where you put which monitoring modules you want to use, such as HTTP. The <a href=\"https://github.com/prometheus/blackbox_exporter/blob/master/example.yml\">configuration of the targets</a> themselves is in prometheus. Ex:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>modules:  http_2xx:    prober: http    http:      preferred_ip_protocol: \"ip4\"  http_post_2xx:    prober: http    http:      method: POST  tcp_connect:    prober: tcp  grpc:    prober: grpc    grpc:      tls: true      preferred_ip_protocol: \"ip4\"  ssh_banner:    prober: tcp    tcp:      query_response:      - expect: \"^SSH-2.0-\"      - send: \"SSH-2.0-blackbox-ssh-check\"  ssh_banner_extract:    prober: tcp    timeout: 5s    tcp:      query_response:      - expect: \"^SSH-2.0-([^ -]+)(?: (.*))?$\"        labels:        - name: ssh_version          value: \"${1}\"        - name: ssh_comments          value: \"${2}\"  icmp:    prober: icmp  icmp_ttl5:    prober: icmp    timeout: 5s    icmp:      ttl: 5</code></pre></div></div><h2 id=\"network_exporter\">network_exporter</h2><p><strong>network_exporter</strong> can also be used in more than one way, but I usually use it for an <strong>inside-out</strong> analysis. In other words, I use it to track all external dependencies of my application or any workload Iâ€™m running, be it an external API, a database, an endpoint accessed via VPN or proxy, and so on. Itâ€™s particularly useful when an external dependency goes down or when thereâ€™s a network issue along the way, like a lost ACL or hardware failure. While it doesnâ€™t support SSL checks, some of the metrics from blackbox_exporter also exist in <a href=\"https://github.com/syepes/network_exporter\">network_exporter</a> ðŸ˜Š</p><p>Exported metrics:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ping_up Exporter stateping_targets Number of active targetsping_status: Ping Statusping_rtt_seconds{type=best}: Best round trip time in secondsping_rtt_seconds{type=worst}: Worst round trip time in secondsping_rtt_seconds{type=mean}: Mean round trip time in secondsping_rtt_seconds{type=sum}: Sum round trip time in secondsping_rtt_seconds{type=sd}: Squared deviation in secondsping_rtt_seconds{type=usd}: Standard deviation without correction in secondsping_rtt_seconds{type=csd}: Standard deviation with correction (Bessel's) in secondsping_rtt_seconds{type=range}: Range in secondsping_rtt_snt_count: Packet sent count totalping_rtt_snt_fail_count: Packet sent fail count totalping_rtt_snt_seconds: Packet sent time total in secondsping_loss_percent: Packet loss in percentmtr_up Exporter statemtr_targets Number of active targetsmtr_hops Number of route hopsmtr_rtt_seconds{type=last}: Last round trip time in secondsmtr_rtt_seconds{type=best}: Best round trip time in secondsmtr_rtt_seconds{type=worst}: Worst round trip time in secondsmtr_rtt_seconds{type=mean}: Mean round trip time in secondsmtr_rtt_seconds{type=sum}: Sum round trip time in secondsmtr_rtt_seconds{type=sd}: Squared deviation in secondsmtr_rtt_seconds{type=usd}: Standard deviation without correction in secondsmtr_rtt_seconds{type=csd}: Standard deviation with correction (Bessel's) in secondsmtr_rtt_seconds{type=range}: Range in secondsmtr_rtt_seconds{type=loss}: Packet loss in percentmtr_rtt_snt_count: Packet sent count totalmtr_rtt_snt_fail_count: Packet sent fail count totalmtr_rtt_snt_seconds: Packet sent time total in secondstcp_up Exporter statetcp_targets Number of active targetstcp_connection_status Connection Statustcp_connection_seconds Connection time in secondshttp_get_up Exporter statehttp_get_targets Number of active targetshttp_get_status HTTP Status Code and Connection Statushttp_get_content_bytes HTTP Get Content Size in byteshttp_get_seconds{type=DNSLookup}: DNSLookup connection drill down time in secondshttp_get_seconds{type=TCPConnection}: TCPConnection connection drill down time in secondshttp_get_seconds{type=TLSHandshake}: TLSHandshake connection drill down time in secondshttp_get_seconds{type=TLSEarliestCertExpiry}: TLSEarliestCertExpiry cert expiration time in epochhttp_get_seconds{type=TLSLastChainExpiry}: TLSLastChainExpiry cert expiration time in epochhttp_get_seconds{type=ServerProcessing}: ServerProcessing connection drill down time in secondshttp_get_seconds{type=ContentTransfer}: ContentTransfer connection drill down time in secondshttp_get_seconds{type=Total}: Total connection time in seconds</code></pre></div></div><h2 id=\"poc\">PoC</h2><p>This PoC showcases monitoring in a Kubernetes environment using network_exporter to analyze connectivity from inside out and blackbox_exporter to simulate external access and assess service availability from outside in.</p><p>The dependencies of my application, which is behind a service, are a database and an external API. I want information about both</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>network_exporter  --&gt; Database                  --&gt; External api                  --&gt; Partner website</code></pre></div></div><p>To check if my API is working as expected, I will make a request using blackbox_exporter, simulating an HTTP call</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>blackbox_exporter  --&gt; Ingress                   --&gt; Partner website</code></pre></div></div><h3 id=\"what-are-we-going-to-build\">what are we going to build</h3><p>You will test the connectivity and performance of services within a Kubernetes cluster using Blackbox Exporter and Network Exporter to monitor interactions between pods, an external API, and a database, including traffic through an Ingress and Service.</p><p><img src=\"/assets/img/blackbox-x-network.png\" alt=\"image.png\" /></p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>flowchart TB subgraph subGraph0[\"Kubernetes cluster\"]        poda(\"pod-a\")        podb(\"pod-b\")        Service[\"Service\"]        network_exporter[\"network_exporter\"]        Ingress[\"Ingress\"]  end subgraph subGraph1[\"VM\"]        DB[(\"Database\")]  end    Ingress --&gt; Service    Service --&gt; poda &amp; podb    poda --&gt; DB &amp; external[\"external api\"]    podb --&gt; DB &amp; external    blackbox_exporter[\"blackbox_exporter\"] --&gt; Ingress    network_exporter --&gt; external &amp; DB    style network_exporter stroke:#2962FF    style DB stroke:#00C853    style external stroke:#D50000    style blackbox_exporter stroke:#2962FF    linkStyle 7 stroke:#D50000,fill:none</code></pre></div></div><h3 id=\"creating-a-local-kubernetes-environment\">Creating a local Kubernetes environment</h3><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: worker- role: worker- role: control-plane  kubeadmConfigPatches:  - |    kind: InitConfiguration    nodeRegistration:      kubeletExtraArgs:        node-labels: \"ingress-ready=true\"        authorization-mode: \"AlwaysAllow\"  extraPortMappings:  - containerPort: 80    hostPort: 80    protocol: TCP  - containerPort: 443    hostPort: 443    protocol: TCP</code></pre></div></div><h3 id=\"instructions\">Instructions</h3><p>All details can be found in this <a href=\"https://github.com/apolzek/shared/tree/main/content/003\">repository</a>.Follow the step by step instructions in the README</p><h3 id=\"creating-a-basic-view-in-grafana\">Creating a basic view in grafana</h3><h2 id=\"references\">References</h2><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://github.com/syepes/network_exporterhttps://github.com/prometheus/blackbox_exporterhttps://github.com/prometheus/blackbox_exporter/blob/master/CONFIGURATION.mdhttps://grafana.com/grafana/dashboards/15297-prometheus-network-exporter/</code></pre></div></div><!-- https://www.mermaidchart.com/app/projects/9763d075-f3a7-4f83-b567-d2373878b316/diagrams/00be2e0c-174b-4a07-8163-659f7f9a2be5/version/v0.1/edit -->",
            "url": "https://apolzek.github.io/2025/02/24/blackbox-exporter-or-network-exporter",
            
            
            
            "tags": ["prometheus","exporters","network","monitoring"],
            
            "date_published": "2025-02-24T00:00:00+00:00",
            "date_modified": "2025-02-24T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2025/02/19/some-tips-for-linux-shell",
            "title": "some tips for linux shell",
            "summary": "A growing collection of linux hacks",
            "content_text": "saving your time: some tips for linux shellIndex  Index          Looping: command vs script                  command          script                    Signals                  SIGINT          SIGTERM          SIGHUP                    Background processes      Debugging      String Manipulation and Substitution      Shell Associative Arrays      YAML files      Check input      shellcheck      journalctl      Looping: command vs scriptcommandfor i in {1..5}; do echo $i; donecount=1; while [ $count -le 5 ]; do echo $count; ((count++)); donescript#!/bin/bashecho \"Counting to 5 with a for loop:\"for i in {1..5}; do    echo $idoneecho \"Counting to 5 with a while loop:\"count=1while [ $count -le 5 ]; do    echo $count    ((count++))doneSignalsSome scenarios that make sense to deal with signals..  Cleaning Temporary Files  Graceful Interruption of Services  Avoiding Database Corruption  Releasing Network or Hardware Resources  Maintaining Consistent Variable State-            Signal      Description      How to Invoke                  SIGINT      Interrupts the process. Typically generated by pressing Ctrl+C in the terminal.      Ctrl + C or kill -2 &lt;pid&gt;              SIGTERM      Requests the graceful termination of the process. Used to terminate processes without forcing them.      kill -15 &lt;pid&gt;              SIGHUP      Indicates that the terminal connection was lost or that the process needs to reload configurations.      kill -1 &lt;pid&gt;              SIGKILL      Forces the immediate termination of a process. Cannot be ignored by the process.      kill -9 &lt;pid&gt;              SIGSTOP      Pauses the execution of a process (can be resumed later).      kill -19 &lt;pid&gt;      SIGINTThis script creates a temporary file and displays a goodbye message when it receives a SIGINT signal (e.g., when the user presses Ctrl+C).#!/bin/bash# Function that will be called when the script receives the SIGINT signalfunction exitMsg {    echo \"you sent a signal to end, byby !!\"    # command here    exit}# Sets up the trap to call the cleanup function when the script receives SIGINTtrap exitMsg SIGINTsleep 5echo \"Creating a temporary file...\"touch /tmp/apolzek || exit 1  # Creates a temporary file or exits with an errorSIGTERMThis script starts a web server that runs indefinitely and shuts down gracefully when it receives a SIGTERM signal. kill &lt;PID&gt;#!/bin/bash# Function called upon receiving SIGTERMfunction terminateMsg {    echo \"Received SIGTERM, shutting down the server gracefully...\"    # Here you can add commands to close connections or save the state    exit}# Sets up the trap for SIGTERMtrap terminateMsg SIGTERMecho \"Starting web server...\"while true; do    echo \"Server is running... (PID: $$)\"    sleep 2  # Simulates the server's running timedoneSIGHUPThis script starts a daemon that runs indefinitely and reloads its configuration when it receives a SIGHUP signal. kill -HUP &lt;PID&gt;#!/bin/bash# Function called upon receiving SIGHUPfunction reloadMsg {    echo \"Received SIGHUP, reloading configuration...\"    # Here you can add commands to reload the configurations    # Example: source /etc/mydaemon/config.conf}# Sets up the trap for SIGHUPtrap reloadMsg SIGHUPecho \"Starting my daemon...\"while true; do    echo \"Daemon is running... (PID: $$)\"    sleep 5  # Simulates the daemon's running timedoneBackground processes#!/bin/bashecho \"Starting background processes...\"# Process 1sleep 3 &amp;  # This simulates a long-running taskpid1=$!  # Get the process ID of process 1# Process 2sleep 9 &amp;  # This simulates a shorter taskpid2=$!  # Get the process ID of process 2# Wait for process 1 to finish and notifywait $pid1echo \"Process 1 has completed.\"# Wait for process 2 to finish and notifywait $pid2echo \"Process 2 has completed.\"echo \"All processes have finished.\"Debugging#!/bin/bashset -x  # Enable debugging modeecho \"Starting the script...\"echo \"Doing something...\"sleep 1echo \"Ending the script.\"set +x  # Disable debugging modeecho \"now debugging mode is disable\"echo \"did you understand ?\"using pipefail..#!/bin/bashset -xeuo pipefail# Uninitialized variable (throws an error with `set -u`)echo \"Attempting to access an uninitialized variable...\"echo \"Variable value: $UNINITIALIZED_VAR\"# This command will never be executed due to the previous errorecho \"End of script.\"String Manipulation and Substitution#!/bin/bash# Defining an original stringoriginal=\"Linux is amazing!\"# Converting to uppercaseuppercase=${original^^}echo \"Uppercase: $uppercase\"# Output: Uppercase: LINUX IS AMAZING!## another wayecho \"Linux is amazing!\" | tr '[:lower:]' '[:upper:]'echo \"Linux is amazing!\" | awk '{ print toupper($0) }'# Converting to lowercaselowercase=${original,,}echo \"Lowercase: $lowercase\"# Output: Lowercase: linux is amazing!# Replacing part of the stringmodified=${original//amazing/extravagant}echo \"Substitution: $modified\"# Output: Substitution: Linux is extravagant!# Extracting a substringsubstring=${original:7:9}  # Extracts \"is amazing\"echo \"Substring: $substring\"# Output: Substring: is amazing# Checking the length of the stringlength=${#original}echo \"Length of the string: $length characters\"# Output: Length of the string: 20 charactersShell Associative Arrays#!/bin/bash# Declare an associative arraydeclare -A user_info# Assign key-value pairsuser_info[name]=\"Alice\"user_info[email]=\"alice@example.com\"user_info[role]=\"Admin\"# Access elements by keyecho \"User Name: ${user_info[name]}\"echo \"User Email: ${user_info[email]}\"echo \"User Role: ${user_info[role]}\"# Looping over keys and valuesfor key in \"${!user_info[@]}\"; do    echo \"$key: ${user_info[$key]}\"doneYAML filesInstall yqsudo pacman -S yqCreate yaml exampleapiVersion: v1kind: ComplexConfigmetadata:  name: example-config  labels:    environment: production    version: \"1.0\"spec:  services:    - name: service1      type: LoadBalancer      ports:        - name: http          port: 80          targetPort: 8080        - name: https          port: 443          targetPort: 8443      hosts:        - host: \"service1.example.com\"          ip: \"192.168.1.10\"          regions:            - us-east-1            - eu-west-1    - name: service2      type: ClusterIP      ports:        - name: grpc          port: 50051          targetPort: 50051      hosts:        - host: \"service2.example.com\"          ip: \"192.168.1.20\"          regions:            - ap-south-1            - eu-central-1  config:    retries: 3    timeout: 5000  logging:    level: debug    format: json    outputs:      - type: file        path: \"/var/log/app.log\"      - type: stdoutfiltering datayq '.metadata.name' config.yamlyq '.spec.services[].name' config.yamlyq '.spec.services[1].type' config.yamlyq '.spec.services[] | select(.name == \"service1\") | .ports[] | {port, targetPort}' config.yamlyq '.spec.services[].hosts[] | {host, ip}' config.yamlyq '.spec.services[] | select(.name == \"service2\") | .hosts[].regions' config.yamlyq '.spec.logging.level' config.yamlyq -r '.metadata.name' config.yamlyq -r '.spec.services[].name' config.yamlyq -r '.spec.services[1].type' config.yamlyq -r '.spec.services[] | select(.name == \"service1\") | .ports[] | \"\\(.port) \\(.targetPort)\"' config.yamlyq -r '.spec.services[].hosts[] | \"\\(.host) \\(.ip)\"' config.yamlyq -r '.spec.services[] | select(.name == \"service2\") | .hosts[].regions[]' config.yamlyq -r '.spec.logging.level' config.yamlCheck input#!/bin/bash# Check if the argument is a directory.if [[ ! -d \"$1\" ]]; then    echo \"Error: $1 is not a directory.\"    exit 1fivalidating file path..#!/bin/bash# Check if the user provided a file path as an argumentif [[ -z \"$1\" ]]; then    echo \"No file path provided. Please enter the file path:\"    read -r file_pathelse    file_path=\"$1\"fi# Check if the file existsif [[ -f \"$file_path\" ]]; then    echo \"File exists, proceeding with backup.\"else    echo \"File does not exist. Please check the path and try again.\"    exit 1fishellcheckshellcheck is a powerful static analysis tool for shell scripts. It helps identify common issues like syntax errors, unused variables, and unsafe practices. Using shellcheck can save time and prevent bugs in your scripts.shellcheck example.shOutput from shellcheckexample.sh:5:7: note: Double quote to prevent globbing and word splitting. [SC2086]example.sh:7:7: warning: Undefined variable: UNDEFINED_VAR. [SC2154]example.sh:10:7: error: Missing 'fi' to end 'if' statement. [SC1073]journalctl# Show only recent logsjournalctl -u nginx --since \"1 hour ago\"# View logs from the current boot sessionjournalctl -b# View logs from the previous bootjournalctl -b -1# Follow logs in real timejournalctl -f# Filtering logs by servicejournalctl -u mysql -p err --since \"30 minutes ago\"# Filter by Priorityjournalctl -p err# Logs from a specific date rangejournalctl --since \"2023-01-01\" --until \"2023-01-02\"# Only show failed services on startupjournalctl -b --priority=3# Check kernel messages on bootjournalctl -k -b# Output logs in JSON formatjournalctl -o json-pretty# Free up disk space by clearing old logssudo journalctl --vacuum-time=2weeks",
            "content_html": "<p><strong>saving your time</strong>: <em>some tips for linux shell</em></p><p><img src=\"/assets/gif/typing.webp\" alt=\"typing\" /></p><h2 id=\"index\">Index</h2><ul>  <li><a href=\"#index\">Index</a>    <ul>      <li><a href=\"#looping-command-vs-script\">Looping: command vs script</a>        <ul>          <li><a href=\"#command\">command</a></li>          <li><a href=\"#script\">script</a></li>        </ul>      </li>      <li><a href=\"#signals\">Signals</a>        <ul>          <li><a href=\"#sigint\">SIGINT</a></li>          <li><a href=\"#sigterm\">SIGTERM</a></li>          <li><a href=\"#sighup\">SIGHUP</a></li>        </ul>      </li>      <li><a href=\"#background-processes\">Background processes</a></li>      <li><a href=\"#debugging\">Debugging</a></li>      <li><a href=\"#string-manipulation-and-substitution\">String Manipulation and Substitution</a></li>      <li><a href=\"#shell-associative-arrays\">Shell Associative Arrays</a></li>      <li><a href=\"#yaml-files\">YAML files</a></li>      <li><a href=\"#check-input\">Check input</a></li>      <li><a href=\"#shellcheck\">shellcheck</a></li>      <li><a href=\"#journalctl\">journalctl</a></li>    </ul>  </li></ul><h3 id=\"looping-command-vs-script\">Looping: command vs script</h3><h4 id=\"command\">command</h4><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..5<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do </span><span class=\"nb\">echo</span> <span class=\"nv\">$i</span><span class=\"p\">;</span> <span class=\"k\">done</span><span class=\"nv\">count</span><span class=\"o\">=</span>1<span class=\"p\">;</span> <span class=\"k\">while</span> <span class=\"o\">[</span> <span class=\"nv\">$count</span> <span class=\"nt\">-le</span> 5 <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">do </span><span class=\"nb\">echo</span> <span class=\"nv\">$count</span><span class=\"p\">;</span> <span class=\"o\">((</span>count++<span class=\"o\">))</span><span class=\"p\">;</span> <span class=\"k\">done</span></code></pre></div></div><h4 id=\"script\">script</h4><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Counting to 5 with a for loop:\"</span><span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..5<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"nv\">$i</span><span class=\"k\">done</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Counting to 5 with a while loop:\"</span><span class=\"nv\">count</span><span class=\"o\">=</span>1<span class=\"k\">while</span> <span class=\"o\">[</span> <span class=\"nv\">$count</span> <span class=\"nt\">-le</span> 5 <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"nv\">$count</span>    <span class=\"o\">((</span>count++<span class=\"o\">))</span><span class=\"k\">done</span></code></pre></div></div><h3 id=\"signals\">Signals</h3><p>Some scenarios that make sense to deal with signals..</p><ul>  <li>Cleaning Temporary Files</li>  <li>Graceful Interruption of Services</li>  <li>Avoiding Database Corruption</li>  <li>Releasing Network or Hardware Resources</li>  <li>Maintaining Consistent Variable State-</li></ul><table>  <thead>    <tr>      <th>Signal</th>      <th>Description</th>      <th>How to Invoke</th>    </tr>  </thead>  <tbody>    <tr>      <td><a href=\"#sigint\">SIGINT</a></td>      <td>Interrupts the process. Typically generated by pressing <code class=\"language-plaintext highlighter-rouge\">Ctrl+C</code> in the terminal.</td>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + C</code> or <code class=\"language-plaintext highlighter-rouge\">kill -2 &lt;pid&gt;</code></td>    </tr>    <tr>      <td><a href=\"#sigterm\">SIGTERM</a></td>      <td>Requests the graceful termination of the process. Used to terminate processes without forcing them.</td>      <td><code class=\"language-plaintext highlighter-rouge\">kill -15 &lt;pid&gt;</code></td>    </tr>    <tr>      <td><a href=\"#sighup\">SIGHUP</a></td>      <td>Indicates that the terminal connection was lost or that the process needs to reload configurations.</td>      <td><code class=\"language-plaintext highlighter-rouge\">kill -1 &lt;pid&gt;</code></td>    </tr>    <tr>      <td><a href=\"#sigkill\">SIGKILL</a></td>      <td>Forces the immediate termination of a process. Cannot be ignored by the process.</td>      <td><code class=\"language-plaintext highlighter-rouge\">kill -9 &lt;pid&gt;</code></td>    </tr>    <tr>      <td><a href=\"#sigstop\">SIGSTOP</a></td>      <td>Pauses the execution of a process (can be resumed later).</td>      <td><code class=\"language-plaintext highlighter-rouge\">kill -19 &lt;pid&gt;</code></td>    </tr>  </tbody></table><h4 id=\"sigint\">SIGINT</h4><p>This script creates a temporary file and displays a goodbye message when it receives a SIGINT signal (e.g., when the user presses Ctrl+C).</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function that will be called when the script receives the SIGINT signal</span><span class=\"k\">function </span>exitMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"you sent a signal to end, byby !!\"</span>    <span class=\"c\"># command here</span>    <span class=\"nb\">exit</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap to call the cleanup function when the script receives SIGINT</span><span class=\"nb\">trap </span>exitMsg SIGINT<span class=\"nb\">sleep </span>5<span class=\"nb\">echo</span> <span class=\"s2\">\"Creating a temporary file...\"</span><span class=\"nb\">touch</span> /tmp/apolzek <span class=\"o\">||</span> <span class=\"nb\">exit </span>1  <span class=\"c\"># Creates a temporary file or exits with an error</span></code></pre></div></div><h4 id=\"sigterm\">SIGTERM</h4><p>This script starts a web server that runs indefinitely and shuts down gracefully when it receives a SIGTERM signal. <code class=\"language-plaintext highlighter-rouge\">kill &lt;PID&gt;</code></p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function called upon receiving SIGTERM</span><span class=\"k\">function </span>terminateMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"Received SIGTERM, shutting down the server gracefully...\"</span>    <span class=\"c\"># Here you can add commands to close connections or save the state</span>    <span class=\"nb\">exit</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap for SIGTERM</span><span class=\"nb\">trap </span>terminateMsg SIGTERM<span class=\"nb\">echo</span> <span class=\"s2\">\"Starting web server...\"</span><span class=\"k\">while </span><span class=\"nb\">true</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"Server is running... (PID: </span><span class=\"nv\">$$</span><span class=\"s2\">)\"</span>    <span class=\"nb\">sleep </span>2  <span class=\"c\"># Simulates the server's running time</span><span class=\"k\">done</span></code></pre></div></div><h4 id=\"sighup\">SIGHUP</h4><p>This script starts a daemon that runs indefinitely and reloads its configuration when it receives a SIGHUP signal. <code class=\"language-plaintext highlighter-rouge\">kill -HUP &lt;PID&gt;</code></p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function called upon receiving SIGHUP</span><span class=\"k\">function </span>reloadMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"Received SIGHUP, reloading configuration...\"</span>    <span class=\"c\"># Here you can add commands to reload the configurations</span>    <span class=\"c\"># Example: source /etc/mydaemon/config.conf</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap for SIGHUP</span><span class=\"nb\">trap </span>reloadMsg SIGHUP<span class=\"nb\">echo</span> <span class=\"s2\">\"Starting my daemon...\"</span><span class=\"k\">while </span><span class=\"nb\">true</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"Daemon is running... (PID: </span><span class=\"nv\">$$</span><span class=\"s2\">)\"</span>    <span class=\"nb\">sleep </span>5  <span class=\"c\"># Simulates the daemon's running time</span><span class=\"k\">done</span></code></pre></div></div><h3 id=\"background-processes\">Background processes</h3><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Starting background processes...\"</span><span class=\"c\"># Process 1</span><span class=\"nb\">sleep </span>3 &amp;  <span class=\"c\"># This simulates a long-running task</span><span class=\"nv\">pid1</span><span class=\"o\">=</span><span class=\"nv\">$!</span>  <span class=\"c\"># Get the process ID of process 1</span><span class=\"c\"># Process 2</span><span class=\"nb\">sleep </span>9 &amp;  <span class=\"c\"># This simulates a shorter task</span><span class=\"nv\">pid2</span><span class=\"o\">=</span><span class=\"nv\">$!</span>  <span class=\"c\"># Get the process ID of process 2</span><span class=\"c\"># Wait for process 1 to finish and notify</span><span class=\"nb\">wait</span> <span class=\"nv\">$pid1</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Process 1 has completed.\"</span><span class=\"c\"># Wait for process 2 to finish and notify</span><span class=\"nb\">wait</span> <span class=\"nv\">$pid2</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Process 2 has completed.\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"All processes have finished.\"</span></code></pre></div></div><h3 id=\"debugging\">Debugging</h3><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">set</span> <span class=\"nt\">-x</span>  <span class=\"c\"># Enable debugging mode</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Starting the script...\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Doing something...\"</span><span class=\"nb\">sleep </span>1<span class=\"nb\">echo</span> <span class=\"s2\">\"Ending the script.\"</span><span class=\"nb\">set</span> +x  <span class=\"c\"># Disable debugging mode</span><span class=\"nb\">echo</span> <span class=\"s2\">\"now debugging mode is disable\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"did you understand ?\"</span></code></pre></div></div><p>using pipefail..</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">set</span> <span class=\"nt\">-xeuo</span> pipefail<span class=\"c\"># Uninitialized variable (throws an error with `set -u`)</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Attempting to access an uninitialized variable...\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Variable value: </span><span class=\"nv\">$UNINITIALIZED_VAR</span><span class=\"s2\">\"</span><span class=\"c\"># This command will never be executed due to the previous error</span><span class=\"nb\">echo</span> <span class=\"s2\">\"End of script.\"</span></code></pre></div></div><h3 id=\"string-manipulation-and-substitution\">String Manipulation and Substitution</h3><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Defining an original string</span><span class=\"nv\">original</span><span class=\"o\">=</span><span class=\"s2\">\"Linux is amazing!\"</span><span class=\"c\"># Converting to uppercase</span><span class=\"nv\">uppercase</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">^^</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Uppercase: </span><span class=\"nv\">$uppercase</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Uppercase: LINUX IS AMAZING!</span><span class=\"c\">## another way</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Linux is amazing!\"</span> | <span class=\"nb\">tr</span> <span class=\"s1\">'[:lower:]'</span> <span class=\"s1\">'[:upper:]'</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Linux is amazing!\"</span> | <span class=\"nb\">awk</span> <span class=\"s1\">'{ print toupper($0) }'</span><span class=\"c\"># Converting to lowercase</span><span class=\"nv\">lowercase</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">,,</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Lowercase: </span><span class=\"nv\">$lowercase</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Lowercase: linux is amazing!</span><span class=\"c\"># Replacing part of the string</span><span class=\"nv\">modified</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">//amazing/extravagant</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Substitution: </span><span class=\"nv\">$modified</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Substitution: Linux is extravagant!</span><span class=\"c\"># Extracting a substring</span><span class=\"nv\">substring</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span>:7:9<span class=\"k\">}</span>  <span class=\"c\"># Extracts \"is amazing\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Substring: </span><span class=\"nv\">$substring</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Substring: is amazing</span><span class=\"c\"># Checking the length of the string</span><span class=\"nv\">length</span><span class=\"o\">=</span><span class=\"k\">${#</span><span class=\"nv\">original</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Length of the string: </span><span class=\"nv\">$length</span><span class=\"s2\"> characters\"</span><span class=\"c\"># Output: Length of the string: 20 characters</span></code></pre></div></div><h3 id=\"shell-associative-arrays\">Shell Associative Arrays</h3><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Declare an associative array</span><span class=\"nb\">declare</span> <span class=\"nt\">-A</span> user_info<span class=\"c\"># Assign key-value pairs</span>user_info[name]<span class=\"o\">=</span><span class=\"s2\">\"Alice\"</span>user_info[email]<span class=\"o\">=</span><span class=\"s2\">\"alice@example.com\"</span>user_info[role]<span class=\"o\">=</span><span class=\"s2\">\"Admin\"</span><span class=\"c\"># Access elements by key</span><span class=\"nb\">echo</span> <span class=\"s2\">\"User Name: </span><span class=\"k\">${</span><span class=\"nv\">user_info</span><span class=\"p\">[name]</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"User Email: </span><span class=\"k\">${</span><span class=\"nv\">user_info</span><span class=\"p\">[email]</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"User Role: </span><span class=\"k\">${</span><span class=\"nv\">user_info</span><span class=\"p\">[role]</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"c\"># Looping over keys and values</span><span class=\"k\">for </span>key <span class=\"k\">in</span> <span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"p\">!user_info[@]</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"</span><span class=\"nv\">$key</span><span class=\"s2\">: </span><span class=\"k\">${</span><span class=\"nv\">user_info</span><span class=\"p\">[</span><span class=\"nv\">$key</span><span class=\"p\">]</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"k\">done</span></code></pre></div></div><h3 id=\"yaml-files\">YAML files</h3><p>Install yq</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>pacman <span class=\"nt\">-S</span> yq</code></pre></div></div><p>Create yaml example</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">ComplexConfig</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">example-config</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">environment</span><span class=\"pi\">:</span> <span class=\"s\">production</span>    <span class=\"na\">version</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">1.0\"</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">services</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">service1</span>      <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">LoadBalancer</span>      <span class=\"na\">ports</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">http</span>          <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">80</span>          <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">8080</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">https</span>          <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">443</span>          <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">8443</span>      <span class=\"na\">hosts</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">host</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">service1.example.com\"</span>          <span class=\"na\">ip</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">192.168.1.10\"</span>          <span class=\"na\">regions</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"s\">us-east-1</span>            <span class=\"pi\">-</span> <span class=\"s\">eu-west-1</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">service2</span>      <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">ClusterIP</span>      <span class=\"na\">ports</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">grpc</span>          <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">50051</span>          <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">50051</span>      <span class=\"na\">hosts</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">host</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">service2.example.com\"</span>          <span class=\"na\">ip</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">192.168.1.20\"</span>          <span class=\"na\">regions</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"s\">ap-south-1</span>            <span class=\"pi\">-</span> <span class=\"s\">eu-central-1</span>  <span class=\"na\">config</span><span class=\"pi\">:</span>    <span class=\"na\">retries</span><span class=\"pi\">:</span> <span class=\"m\">3</span>    <span class=\"na\">timeout</span><span class=\"pi\">:</span> <span class=\"m\">5000</span>  <span class=\"na\">logging</span><span class=\"pi\">:</span>    <span class=\"na\">level</span><span class=\"pi\">:</span> <span class=\"s\">debug</span>    <span class=\"na\">format</span><span class=\"pi\">:</span> <span class=\"s\">json</span>    <span class=\"na\">outputs</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">file</span>        <span class=\"na\">path</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">/var/log/app.log\"</span>      <span class=\"pi\">-</span> <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">stdout</span></code></pre></div></div><p>filtering data</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>yq <span class=\"s1\">'.metadata.name'</span> config.yamlyq <span class=\"s1\">'.spec.services[].name'</span> config.yamlyq <span class=\"s1\">'.spec.services[1].type'</span> config.yamlyq <span class=\"s1\">'.spec.services[] | select(.name == \"service1\") | .ports[] | {port, targetPort}'</span> config.yamlyq <span class=\"s1\">'.spec.services[].hosts[] | {host, ip}'</span> config.yamlyq <span class=\"s1\">'.spec.services[] | select(.name == \"service2\") | .hosts[].regions'</span> config.yamlyq <span class=\"s1\">'.spec.logging.level'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.metadata.name'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.services[].name'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.services[1].type'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.services[] | select(.name == \"service1\") | .ports[] | \"\\(.port) \\(.targetPort)\"'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.services[].hosts[] | \"\\(.host) \\(.ip)\"'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.services[] | select(.name == \"service2\") | .hosts[].regions[]'</span> config.yamlyq <span class=\"nt\">-r</span> <span class=\"s1\">'.spec.logging.level'</span> config.yaml</code></pre></div></div><h3 id=\"check-input\">Check input</h3><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Check if the argument is a directory.</span><span class=\"k\">if</span> <span class=\"o\">[[</span> <span class=\"o\">!</span> <span class=\"nt\">-d</span> <span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span> <span class=\"o\">]]</span><span class=\"p\">;</span> <span class=\"k\">then    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"Error: </span><span class=\"nv\">$1</span><span class=\"s2\"> is not a directory.\"</span>    <span class=\"nb\">exit </span>1<span class=\"k\">fi</span></code></pre></div></div><p>validating file path..</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Check if the user provided a file path as an argument</span><span class=\"k\">if</span> <span class=\"o\">[[</span> <span class=\"nt\">-z</span> <span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span> <span class=\"o\">]]</span><span class=\"p\">;</span> <span class=\"k\">then    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"No file path provided. Please enter the file path:\"</span>    <span class=\"nb\">read</span> <span class=\"nt\">-r</span> file_path<span class=\"k\">else    </span><span class=\"nv\">file_path</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span><span class=\"k\">fi</span><span class=\"c\"># Check if the file exists</span><span class=\"k\">if</span> <span class=\"o\">[[</span> <span class=\"nt\">-f</span> <span class=\"s2\">\"</span><span class=\"nv\">$file_path</span><span class=\"s2\">\"</span> <span class=\"o\">]]</span><span class=\"p\">;</span> <span class=\"k\">then    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"File exists, proceeding with backup.\"</span><span class=\"k\">else    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"File does not exist. Please check the path and try again.\"</span>    <span class=\"nb\">exit </span>1<span class=\"k\">fi</span></code></pre></div></div><h3 id=\"shellcheck\">shellcheck</h3><p>shellcheck is a powerful static analysis tool for shell scripts. It helps identify common issues like syntax errors, unused variables, and unsafe practices. Using shellcheck can save time and prevent bugs in your scripts.</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>shellcheck example.sh</code></pre></div></div><p><em>Output from shellcheck</em></p><div class=\"language-text highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>example.sh:5:7: note: Double quote to prevent globbing and word splitting. [SC2086]example.sh:7:7: warning: Undefined variable: UNDEFINED_VAR. [SC2154]example.sh:10:7: error: Missing 'fi' to end 'if' statement. [SC1073]</code></pre></div></div><h3 id=\"journalctl\">journalctl</h3><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Show only recent logs</span>journalctl <span class=\"nt\">-u</span> nginx <span class=\"nt\">--since</span> <span class=\"s2\">\"1 hour ago\"</span><span class=\"c\"># View logs from the current boot session</span>journalctl <span class=\"nt\">-b</span><span class=\"c\"># View logs from the previous boot</span>journalctl <span class=\"nt\">-b</span> <span class=\"nt\">-1</span><span class=\"c\"># Follow logs in real time</span>journalctl <span class=\"nt\">-f</span><span class=\"c\"># Filtering logs by service</span>journalctl <span class=\"nt\">-u</span> mysql <span class=\"nt\">-p</span> err <span class=\"nt\">--since</span> <span class=\"s2\">\"30 minutes ago\"</span><span class=\"c\"># Filter by Priority</span>journalctl <span class=\"nt\">-p</span> err<span class=\"c\"># Logs from a specific date range</span>journalctl <span class=\"nt\">--since</span> <span class=\"s2\">\"2023-01-01\"</span> <span class=\"nt\">--until</span> <span class=\"s2\">\"2023-01-02\"</span><span class=\"c\"># Only show failed services on startup</span>journalctl <span class=\"nt\">-b</span> <span class=\"nt\">--priority</span><span class=\"o\">=</span>3<span class=\"c\"># Check kernel messages on boot</span>journalctl <span class=\"nt\">-k</span> <span class=\"nt\">-b</span><span class=\"c\"># Output logs in JSON format</span>journalctl <span class=\"nt\">-o</span> json-pretty<span class=\"c\"># Free up disk space by clearing old logs</span><span class=\"nb\">sudo </span>journalctl <span class=\"nt\">--vacuum-time</span><span class=\"o\">=</span>2weeks</code></pre></div></div>",
            "url": "https://apolzek.github.io/2025/02/19/some-tips-for-linux-shell",
            
            
            
            "tags": ["linux","shell","script","bash","sh"],
            
            "date_published": "2025-02-19T00:00:00+00:00",
            "date_modified": "2025-02-19T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2025/02/19/playing-around-with-coredns",
            "title": "Playing around with CoreDNS",
            "summary": null,
            "content_text": "saving your time: In this post, we will explore how to set up a basic DNS server using CoreDNS and configure it to serve local domain names while forwarding other DNS queries to external resolversSummary  Download the Ubuntu Server ISO: Ubuntu Downloads  Install Ubuntu Server on VirtualBox  Set the virtual machineâ€™s network mode to bridge  Disable systemd-resolved  Download CoreDNS  Create a Corefile with the desired configurations  Create a DNS zone file for CoreDNS  On the host machine, add an entry in /etc/hosts mapping the VMâ€™s IP for DNS resolution  Test name resolution using a browser and digAfter installing the virtual machine (VM) and assigning an IP address via DHCP, follow the steps below (:Configure Ubuntu ServerLogin using SSH and download CoreDNSsudo systemctl stop systemd-resolvedwget https://github.com/coredns/coredns/releases/download/v1.10.1/coredns_1.10.1_linux_amd64.tgztar -xvzf coredns_1.10.1_linux_amd64.tgztouch admin.meudominio.localtouch CorefileCreate file Corefileadmin.meudominio.local {    file ./admin.meudominio.local    log    errors}.:53 {    forward . 8.8.8.8 1.1.1.1    cache    log    errors}Create file admin.meudominio.local$ORIGIN admin.meudominio.local.$TTL 3600@       IN  SOA  ns.admin.meudominio.local. admin.admin.meudominio.local. (            2025021901 ; serial            3600       ; refresh            1800       ; retry            1209600    ; expire            3600 )     ; minimum TTL@       IN  NS  ns.admin.meudominio.local.ns      IN  A   192.168.100.218@       IN  A   192.168.100.1   ; admin.meudominio.localdb      IN  A   192.168.100.1   ; add db.admin.meudominio.localapolzek IN  CNAME github.com.   ; add CNAMEstart CoreDNSsudo coredns -conf ./CorefileConfigure linux host (iâ€™m using ubuntu)Add the VMâ€™s IP to resolv.confnameserver 192.168.100.218 # VM with Ubuntu Server and CoreDNSTestingRun in the host terminaldig @192.168.100.218 apolzek.admin.meudominio.localdig @192.168.100.218 admin.meudominio.local",
            "content_html": "<p><strong>saving your time</strong>: <em>In this post, we will explore how to set up a basic DNS server using CoreDNS and configure it to serve local domain names while forwarding other DNS queries to external resolvers</em></p><h3 id=\"summary\">Summary</h3><ol>  <li>Download the Ubuntu Server ISO: <a href=\"https://ubuntu.com/download/alternative-downloads\">Ubuntu Downloads</a></li>  <li>Install Ubuntu Server on VirtualBox</li>  <li>Set the virtual machineâ€™s network mode to bridge</li>  <li>Disable <em>systemd-resolved</em></li>  <li>Download CoreDNS</li>  <li>Create a <em>Corefile</em> with the desired configurations</li>  <li>Create a DNS zone file for CoreDNS</li>  <li>On the host machine, add an entry in <code class=\"language-plaintext highlighter-rouge\">/etc/hosts</code> mapping the VMâ€™s IP for DNS resolution</li>  <li>Test name resolution using a browser and <code class=\"language-plaintext highlighter-rouge\">dig</code></li></ol><p><img src=\"/assets/img/ubuntu-vm-lab.png\" alt=\"ubuntu-vm\" /></p><p>After installing the virtual machine (VM) and assigning an IP address via DHCP, follow the steps below (:</p><h3 id=\"configure-ubuntu-server\">Configure Ubuntu Server</h3><p>Login using SSH and download CoreDNS</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sudo systemctl stop systemd-resolvedwget https://github.com/coredns/coredns/releases/download/v1.10.1/coredns_1.10.1_linux_amd64.tgztar -xvzf coredns_1.10.1_linux_amd64.tgztouch admin.meudominio.localtouch Corefile</code></pre></div></div><p>Create file <strong>Corefile</strong></p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>admin.meudominio.local {    file ./admin.meudominio.local    log    errors}.:53 {    forward . 8.8.8.8 1.1.1.1    cache    log    errors}</code></pre></div></div><p>Create file <strong>admin.meudominio.local</strong></p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ORIGIN admin.meudominio.local.$TTL 3600@       IN  SOA  ns.admin.meudominio.local. admin.admin.meudominio.local. (            2025021901 ; serial            3600       ; refresh            1800       ; retry            1209600    ; expire            3600 )     ; minimum TTL@       IN  NS  ns.admin.meudominio.local.ns      IN  A   192.168.100.218@       IN  A   192.168.100.1   ; admin.meudominio.localdb      IN  A   192.168.100.1   ; add db.admin.meudominio.localapolzek IN  CNAME github.com.   ; add CNAME</code></pre></div></div><p>start CoreDNS</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>sudo coredns -conf ./Corefile</code></pre></div></div><h3 id=\"configure-linux-host-im-using-ubuntu\">Configure linux host (iâ€™m using ubuntu)</h3><p>Add the VMâ€™s IP to resolv.conf</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>nameserver 192.168.100.218 # VM with Ubuntu Server and CoreDNS</code></pre></div></div><h3 id=\"testing\">Testing</h3><p>Run in the host terminal</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>dig @192.168.100.218 apolzek.admin.meudominio.localdig @192.168.100.218 admin.meudominio.local</code></pre></div></div><p><img src=\"/assets/img/lab-coredns.png\" alt=\"ubuntu-vm\" /></p><p><img src=\"/assets/gif/dig-corednslab.gif\" alt=\"gif-dig\" /></p>",
            "url": "https://apolzek.github.io/2025/02/19/playing-around-with-coredns",
            
            
            
            "tags": ["dns","coredns"],
            
            "date_published": "2025-02-19T00:00:00+00:00",
            "date_modified": "2025-02-19T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2025/02/10/extracting-metrics-from-iis-logs",
            "title": "Extracting metrics from IIS logs(iis_log_exporter)",
            "summary": null,
            "content_text": "saving your time: How I built an IIS prometheus exporter for a RED metrics dashboardIf youâ€™re from the new generation and have never worked with IIS, let me introduce you to this legendary web server. Itâ€™s a secure, easy-to-manage, modular, and extensible platform for hosting websites, services, and applications (by Microsoft). That saidâ€¦ IIS isnâ€™t exactly the hottest thing around these days and there are reasons for that ðŸ’¥. But, as the old saying goes: you donâ€™t have to like it, but you should at least learn to live with it!! ðŸ˜†My journey began when I was tasked with understanding the behavior of an IIS server, making it observable, and setting up alarms for potential issues. The first step was to install windows_exporter, which includes the IIS collector. While it provided many valuable metrics about IIS, it wasnâ€™t exactly what I needed. What I was really looking for were RED metrics, more focused on HTTP responses from IIS. I researched some repositories but couldnâ€™t find anything satisfactory. So, I decided to build my own exporter to gather these metrics.I spent some time exploring IIS and realized that the only data source that truly met my needs was the logs. Although it wasnâ€™t the most elegant solution, using them as a data source solved my problem and, of course, created other challenges. With that in mind, I developed a Python exporter that reads IIS log files from the corresponding folder, always pointing to and processing the logs of the day. The result is still in validation, but Iâ€™d like to share what I have so far. Take a look belowrepository: https://github.com/apolzek/iis_log_exporterThe log files contained the request path, the method used, the status code, and the response time. These were exactly the pieces of information I needed. Using this data, I identified key metrics.iis_requests_duration_seconds = Histogram(    \"iis_requests_duration_seconds\", \"Duration of HTTP requests in IIS\",    [\"method\", \"path\"])iis_exceptions_total = Counter(    \"iis_exceptions_total\", \"Total number of HTTP requests with 5xx status codes\",    [\"method\", \"path\", \"status_code\"])iis_requests_total = Counter(    \"iis_requests_total\", \"Total number of HTTP requests from IIS\",    [\"method\", \"path\", \"status_code\"])ðŸ’¬ From the beginning, my goal was never to replace the IIS module of windows_exporter, but rather to complement it with more detailed HTTP metrics for hosted applications. With this in mind, the final outcome I aim for is a dashboard that integrates the metrics from windows_exporter with those from iis_log_exporter. The main challenges I faced while building this exporter were, first, that I couldnâ€™t keep re-reading the log file, as it could grow indefinitely and cause metric inconsistencies. Second, I always needed to read only the log files for the current day. To solve the first issue, I implemented an offset system where I store the last scraped line in a file. For the second, I used the fileâ€™s creation date from the OS while relying on its filename within the folder.I know these approaches have potential failure points someone could manually add logs to the file, IIS configurations might generate logs missing key data that break the regex logic, and so on. That said, I recognize that this solution is still very tailored to my specific use case, but I hope it can evolve to support other legacy IIS systems out there.Below is the conventional solution I was already using ðŸ‘‡ collector.iisTIP: Integrating the windows_exporter, which already includes the IIS modules, with this exporter (named iis_log_exporter) can yield excellent results.. Good luck to us !!Pull requests are welcome in the exporterâ€™s repository !! ðŸ«¶ðŸ» bye-bye",
            "content_html": "<p><strong>saving your time</strong>: <em>How I built an IIS prometheus exporter for a RED metrics dashboard</em></p><p>If youâ€™re from the new generation and have never worked with IIS, let me introduce you to this <strong>legendary web server</strong>. Itâ€™s a secure, easy-to-manage, modular, and extensible platform for hosting websites, services, and applications (by Microsoft). That saidâ€¦ IIS isnâ€™t exactly the hottest thing around these days and there are reasons for that ðŸ’¥. But, as the old saying goes: <strong>you donâ€™t have to like it, but you should at least learn to live with it</strong>!! ðŸ˜†</p><p><img src=\"/assets/img/iis.jpg\" alt=\"iis\" /></p><p>My journey began when I was tasked with understanding the behavior of an IIS server, making it observable, and setting up alarms for potential issues. The first step was to install <a href=\"https://github.com/prometheus-community/windows_exporter\">windows_exporter</a>, which includes the <a href=\"https://github.com/prometheus-community/windows_exporter/blob/master/docs/collector.iis.md\">IIS collector</a>. While it provided many valuable metrics about IIS, it wasnâ€™t exactly what I needed. What I was really looking for were RED metrics, more focused on HTTP responses from IIS. I researched some repositories but couldnâ€™t find anything satisfactory. So, I decided to build my own exporter to gather these metrics.</p><p><img src=\"/assets/gif/indicators.webp\" alt=\"indicators\" /></p><p>I spent some time exploring IIS and realized that the only data source that truly met my needs was the <strong>logs</strong>. Although it wasnâ€™t the most elegant solution, using them as a data source solved my problem and, of course, created other challenges. With that in mind, I developed a Python exporter that reads IIS log files from the corresponding folder, always pointing to and processing the logs of the day. The result is still in validation, but Iâ€™d like to share what I have so far. Take a look below</p><p><img src=\"/assets/img/iis-dashboard-v2.png\" alt=\"iis\" /></p><p><strong>repository</strong>: <a href=\"https://github.com/apolzek/iis_log_exporter\">https://github.com/apolzek/iis_log_exporter</a></p><p>The log files contained the request path, the method used, the status code, and the response time. These were exactly the pieces of information I needed. Using this data, I identified key metrics.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>iis_requests_duration_seconds = Histogram(    \"iis_requests_duration_seconds\", \"Duration of HTTP requests in IIS\",    [\"method\", \"path\"])iis_exceptions_total = Counter(    \"iis_exceptions_total\", \"Total number of HTTP requests with 5xx status codes\",    [\"method\", \"path\", \"status_code\"])iis_requests_total = Counter(    \"iis_requests_total\", \"Total number of HTTP requests from IIS\",    [\"method\", \"path\", \"status_code\"])</code></pre></div></div><p>ðŸ’¬ From the beginning, my goal was never to <strong>replace</strong> the IIS module of <strong>windows_exporter</strong>, but rather to complement it with more detailed HTTP metrics for hosted applications. With this in mind, the final outcome I aim for is a dashboard that integrates the metrics from windows_exporter with those from iis_log_exporter. The main challenges I faced while building this exporter were, first, that I couldnâ€™t keep re-reading the log file, as it could grow indefinitely and cause metric inconsistencies. Second, I always needed to read only the log files for the current day. To solve the first issue, I implemented an <strong>offset</strong> system where I store the last scraped line in a file. For the second, I used the fileâ€™s creation date from the OS while relying on its filename within the folder.</p><p>I know these approaches have potential failure points someone could manually add logs to the file, IIS configurations might generate logs missing key data that break the regex logic, and so on. That said, I recognize that this solution is still very tailored to my specific use case, but I hope it can evolve to support other legacy IIS systems out there.</p><p>Below is the conventional solution I was already using ðŸ‘‡</p><p><img src=\"/assets/img/iis_collector.png\" alt=\"iis_collector\" /> <a href=\"https://github.com/prometheus-community/windows_exporter/blob/master/docs/collector.iis.md\">collector.iis</a></p><p><strong>TIP</strong>: Integrating the windows_exporter, which already includes the IIS modules, with this exporter (named iis_log_exporter) can yield excellent results.. Good luck to us !!</p><p><img src=\"/assets/gif/together.webp\" alt=\"together\" /></p><p>Pull requests are welcome in the exporterâ€™s repository !! ðŸ«¶ðŸ» bye-bye</p><!-- https://www.sysgauge.com/ -->",
            "url": "https://apolzek.github.io/2025/02/10/extracting-metrics-from-iis-logs",
            
            
            
            "tags": ["windows","metrics","exporter","observability","prometheus"],
            
            "date_published": "2025-02-10T00:00:00+00:00",
            "date_modified": "2025-02-10T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2024/12/29/the-biggest-reliability-engineering-challenges-in-2025",
            "title": "The biggest reliability engineering challenges in 2025(From my perspective)",
            "summary": null,
            "content_text": "saving your time: This is just my opinion on challenges in tech field, based on my work in the banking sector in Brazil as a reliability engineer(100% biased)Managing software with more softwareUsing software to support and evolve software comes at a cost. If you work in DevOps or reliability engineering, youâ€™re likely already familiar with tools like Containerd, Kubernetes, Helm, Terraform, Crossplane, Jenkins, ArgoCD, Ansible, KEDA, Jaeger, Istio, OpenTelemetry, and many others. Even with excellent organizations like CNCF providing structure, upgrading some of these tools in critical environments can become quite complex. This is especially true for those directly tied to workloads with a significant impact on products, such as Containerd, Kubernetes, and Istio. Other updates can also be risky, and their side effects may be even harder to identify. In short, I believe these solutions come with their own challenges. That doesnâ€™t mean you shouldnâ€™t use them, but itâ€™s crucial to keep in mind that adopting any of these tools means youâ€™ll have to deal with updates. And when those updates involve security, they often need to be done with urgency.Security vs. ProductivityThe topic of security has significantly complicated my daily work routine, while also generating a growing backlog for reliability and software engineering teams. Everyone knows that security must be embedded in every level of an organization, as well as within individuals. It plays a vital role in the business and goes beyond the technological sphere. Having worked with banks and fintechs, I understand that security is even more critical in these environments, where organizations face numerous regulatory requirements and compliance obligations. I see this topic as a significant challenge because it directly affects employeesâ€™ lives, who often feel unproductive or pressured by the bureaucracy applied to technology and processes. A simple analogy: imagine youâ€™re teaching your child to ride a bike but want to ensure they donâ€™t get hurt if they fall. You equip them with every possible safety device so that even if they fall, they wonâ€™t be injured. However, the excess of equipment makes it impossible for them to ride the bike. This isnâ€™t about debating whether we need security, thatâ€™s already clear. Instead, we should reflect on how we want to approach security and strike the right balance to ensure both safety and efficiency.ðŸ’¬ II believe this topic may have some relation to employee burnoutsBuilding economicallyIt is very common to see companies being overwhelmed by costs in cloud providers like AWS, Azure, and GCP, as well as by extremely expensive bills from observability vendors like Dynatrace, New Relic, and Datadog. The key point here is that we usually build first and only think about cost optimization later, instead of bringing this discussion to the beginning of projects. Once everything is built and running well, with the SLO achieved, reducing costs becomes much more complex. I see it as a major challenge to shift our mindset toward understanding and calculating costs based on technical decisions from the start of any project. I notice that few people truly appreciate efficiency those who deliver faster usually get more recognition. We should aim for sustainable solutions, considering cost progression over time and avoiding vendor lock-in.Fragile points in complex systems/architecturesThe use of complex architectures with microservices, non-relational databases, and messaging services is common in organizations that need to scale to serve a large number of users. While complexity is relative, dealing with systems composed of various subsystems and different levels of abstraction often makes decision-making challenging, especially during critical situations. I have witnessed scenarios where the feature toggle system of an application failed, directly impacting the applicationâ€™s functionality. I have also seen cases where a cache system failure brought an entire product down. These fragilities have cost me many sleepless nights. For 2025, the challenge I propose is to develop solutions that can scale without being overly sensitive to dependencies, operate with alternative paths, and recover from potential failures autonomously and efficiently.Solution = Shift left Costs + Security + ObservabilityHonestly, I think these are the main challenges for 2025 in my line of work. So, whatâ€™s the plan ?? We tackle them head-on. The idea is to deliver value, get stuff done, and have some fun along the way, working with Delphi or whipping up a new CI/CD pipeline. The best way to not let these challenges get to you ? Just do a solid job and keep it chill.Happy New Year ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰",
            "content_html": "<p><strong>saving your time</strong>: <em>This is just my opinion on challenges in tech field, based on my work in the banking sector in Brazil as a reliability engineer(100% biased)</em></p><h2 id=\"managing-software-with-more-software\">Managing software with more software</h2><p>Using software to support and evolve software comes at a cost. If you work in DevOps or reliability engineering, youâ€™re likely already familiar with tools like Containerd, Kubernetes, Helm, Terraform, Crossplane, Jenkins, ArgoCD, Ansible, KEDA, Jaeger, Istio, OpenTelemetry, and many others. Even with excellent organizations like CNCF providing structure, upgrading some of these tools in critical environments can become quite complex. This is especially true for those directly tied to workloads with a significant impact on products, such as Containerd, Kubernetes, and Istio. Other updates can also be risky, and their side effects may be even harder to identify. In short, I believe these solutions come with their own challenges. That doesnâ€™t mean you shouldnâ€™t use them, but itâ€™s crucial to keep in mind that adopting any of these tools means youâ€™ll have to deal with updates. And when those updates involve security, they often need to be done with urgency.</p><h2 id=\"security-vs-productivity\">Security vs. Productivity</h2><p>The topic of security has significantly complicated my daily work routine, while also generating a growing backlog for reliability and software engineering teams. Everyone knows that security must be embedded in every level of an organization, as well as within individuals. It plays a vital role in the business and goes beyond the technological sphere. Having worked with banks and fintechs, I understand that security is even more critical in these environments, where organizations face numerous regulatory requirements and compliance obligations. I see this topic as a significant challenge because it directly affects employeesâ€™ lives, who often feel unproductive or pressured by the bureaucracy applied to technology and processes. A simple analogy: imagine youâ€™re teaching your child to ride a bike but want to ensure they donâ€™t get hurt if they fall. You equip them with every possible safety device so that even if they fall, they wonâ€™t be injured. However, the excess of equipment makes it impossible for them to ride the bike. This isnâ€™t about debating whether we need security, thatâ€™s already clear. Instead, we should reflect on how we want to approach security and strike the right balance to ensure both safety and efficiency.</p><p>ðŸ’¬ <em>II believe this topic may have some relation to employee burnouts</em></p><h2 id=\"building-economically\">Building economically</h2><p>It is very common to see companies being overwhelmed by costs in cloud providers like AWS, Azure, and GCP, as well as by extremely expensive bills from observability vendors like Dynatrace, New Relic, and Datadog. The key point here is that we usually build first and only think about cost optimization later, instead of bringing this discussion to the beginning of projects. Once everything is built and running well, with the SLO achieved, reducing costs becomes much more complex. I see it as a major challenge to shift our mindset toward understanding and calculating costs based on technical decisions from the start of any project. I notice that few people truly appreciate efficiency those who deliver faster usually get more recognition. We should aim for sustainable solutions, considering cost progression over time and avoiding vendor lock-in.</p><h2 id=\"fragile-points-in-complex-systemsarchitectures\">Fragile points in complex systems/architectures</h2><p>The use of complex architectures with microservices, non-relational databases, and messaging services is common in organizations that need to scale to serve a large number of users. While complexity is relative, dealing with systems composed of various subsystems and different levels of abstraction often makes decision-making challenging, especially during critical situations. I have witnessed scenarios where the feature toggle system of an application failed, directly impacting the applicationâ€™s functionality. I have also seen cases where a cache system failure brought an entire product down. These fragilities have cost me many sleepless nights. For 2025, the challenge I propose is to develop solutions that can scale without being overly sensitive to dependencies, operate with alternative paths, and recover from potential failures autonomously and efficiently.</p><p><img src=\"https://raw.githubusercontent.com/apolzek/apolzek.github.io/refs/heads/main/assets/gif/done.webp\" alt=\"working\" /><strong>Solution</strong> = Shift left <strong>Costs</strong> + <strong>Security</strong> + <strong>Observability</strong></p><p>Honestly, I think these are the main challenges for 2025 in my line of work. So, whatâ€™s the plan ?? We tackle them head-on. The idea is to deliver value, get stuff done, and have some fun along the way, working with Delphi or whipping up a new CI/CD pipeline. The best way to not let these challenges get to you ? Just do a solid job and keep it chill.</p><p><strong>Happy New Year</strong> ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰</p>",
            "url": "https://apolzek.github.io/2024/12/29/the-biggest-reliability-engineering-challenges-in-2025",
            
            
            
            "tags": ["challenges","tech","banking","security"],
            
            "date_published": "2024-12-29T00:00:00+00:00",
            "date_modified": "2024-12-29T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2024/12/23/mvp-nanopods-day1",
            "title": "ðŸ’¡ MVP nanopods - day 1",
            "summary": null,
            "content_text": "saving your time: about a personal project similar to render.com to exercise the mindAbout this initiativeI recently saw a project on YouTube where the YouTuber created a complete product and put it into production. I liked the idea and thought about creating a real projectâ€¦ starting from localhost to production. I donâ€™t consider myself a good programmer, but Iâ€™m looking for a well-made bean and rice. My main focus is to launch a stable product with acceptable security standards and that simply implements the idea as it is now in my mind. I present to you the nanopods project. You can follow the progress of this through a series of articles on this blog!ðŸ—£ï¸ I intend to run this on-premisesI know there are already similar projects on the internet, but most of them focus on development and not on operations. I have two main goals. The first is to test technologies, discuss decisions and show concerns that usually arise when our project goes into production. The second is to perhaps make some money with this. My initial focus is Brazilian software developers. In general, I get along well with developers.Letâ€™s get started. What follows is information about the project. To be quite honest, what I intend to build already exists, something similar to render.com.. but with some differences.nanopods ðŸš€nanopods is not introducing an innovative solution but rather offering a new way of delivering container-based application hosting, designed for developers of all levels, SREs, DevOps, and QAs. By abstracting infrastructure and networking, it enables fast, cost-effective container deployment. Users can make applications publicly accessible or control access as needed. Unlike traditional solutions focused on large enterprises with strict SLAs, nanopods prioritizes simplicity and accessibility, creating an agile environment for rapid testing and iteration, without the complexity of managing traditional servers.Iâ€™m still thinking about the legal issues..About the ideaThe idea is basically a render.com with some differences. I want to do something more â€œapi firstâ€. I want to do it in a way that is similar to the way people work with containers locally and I also want to offer a way for them to have details about their applications by adding components to the infrastructure that abstract away the complexity of doing so.What do I need ?Considering that I have no money, no computing resources, and no advanced programming skills (that makes it too hard ðŸ¤£ðŸ¤£), I need to focus on something simple that works. I want to create a business abstraction on top of Kubernetes and put all the complexity into it using tools I have experience with. I need a cheap domain like nanopods.io. I need a payment method and reasonable bandwidth. I want to physically separate the servers where applications are from the servers where the end-user containers will be. I need to think about security and a business model that is viable for Brazilian developers.For the first pocâ€™s, the simpler the better. I intend to use python or golang for this. I will use kind as the local kubernetes environment, the local registry of my machine for the images and also my local network. The networking part is very important in this project, but first I want a functional MVP.to be continued..So, thatâ€™s it. Iâ€™ve reached the end of my first article. I hope Iâ€™ve been clear about my idea. In the end, even if everything goes wrong, I will have gained valuable knowledge. These notes may be useful to someone else, and even to my future self. Whatâ€™s coming next? A series of reflections, articles about random tools, and some not-so-professional code (:See you around !",
            "content_html": "<p><strong>saving your time</strong>: <em>about a personal project similar to render.com to exercise the mind</em></p><h2 id=\"about-this-initiative\">About this initiative</h2><p>I recently saw a project on YouTube where the YouTuber created a complete product and put it into production. I liked the idea and thought about creating a real projectâ€¦ starting from localhost to production. I donâ€™t consider myself a good programmer, but Iâ€™m looking for a well-made bean and rice. My main focus is to launch a stable product with acceptable security standards and that simply implements the idea as it is now in my mind. I present to you the <strong>nanopods</strong> project. You can follow the progress of this through a series of articles on this blog!</p><p>ðŸ—£ï¸ <em>I intend to run this on-premises</em></p><p>I know there are already similar projects on the internet, but most of them focus on <em>development</em> and not on <em>operations</em>. I have two main goals. The first is to test technologies, discuss decisions and show concerns that usually arise when our project goes into production. The second is to perhaps make some money with this. My initial focus is Brazilian software developers. In general, I get along well with developers.</p><p>Letâ€™s get started. What follows is information about the project. To be quite honest, what I intend to build already exists, something similar to <em>render.com</em>.. but with some differences.</p><h3 id=\"nanopods-\">nanopods ðŸš€</h3><p><strong>nanopods</strong> is <strong>not</strong> introducing an innovative solution but rather offering <strong>a new way of delivering container-based application hosting</strong>, designed for developers of all levels, SREs, DevOps, and QAs. By abstracting infrastructure and networking, it enables fast, cost-effective container deployment. Users can make applications publicly accessible or control access as needed. Unlike traditional solutions focused on large enterprises with strict SLAs, <strong>nanopods</strong> prioritizes simplicity and accessibility, creating an agile environment for rapid testing and iteration, without the complexity of managing traditional servers.</p><p>Iâ€™m still thinking about the legal issues..</p><h3 id=\"about-the-idea\">About the idea</h3><p>The idea is basically a <em>render.com</em> with some differences. I want to do something more â€œapi firstâ€. I want to do it in a way that is similar to the way people work with containers locally and I also want to offer a way for them to have details about their applications by adding components to the infrastructure that abstract away the complexity of doing so.</p><h3 id=\"what-do-i-need-\">What do I need ?</h3><p>Considering that I have no money, no computing resources, and no advanced programming skills (that makes it too hard ðŸ¤£ðŸ¤£), I need to focus on something simple that works. I want to create a business abstraction on top of Kubernetes and put all the complexity into it using tools I have experience with. I need a cheap domain like <strong>nanopods.io</strong>. I need a payment method and reasonable bandwidth. I want to physically separate the servers where applications are from the servers where the end-user containers will be. I need to think about security and a business model that is viable for Brazilian developers.</p><p>For the first pocâ€™s, the simpler the better. I intend to use python or golang for this. I will use kind as the local kubernetes environment, the local registry of my machine for the images and also my local network. The networking part is very important in this project, but first I want a functional MVP.</p><h3 id=\"to-be-continued\">to be continued..</h3><p>So, thatâ€™s it. Iâ€™ve reached the end of my first article. I hope Iâ€™ve been clear about my idea. In the end, even if everything goes wrong, I will have gained valuable knowledge. These notes may be useful to someone else, and even to my future self. Whatâ€™s coming next? A series of reflections, articles about random tools, and some not-so-professional code <strong>(</strong>:</p><p>See you around !</p><p><img src=\"https://raw.githubusercontent.com/apolzek/apolzek.github.io/refs/heads/main/assets/gif/working.webp\" alt=\"working\" /></p>",
            "url": "https://apolzek.github.io/2024/12/23/mvp-nanopods-day1",
            
            
            
            "tags": ["devops","sre","project","startup"],
            
            "date_published": "2024-12-23T00:00:00+00:00",
            "date_modified": "2024-12-23T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2024/09/29/kafka-and-rabbitmq-on-kubernetes",
            "title": "Kafka and RabbitMQ on Kubernetes using Kind",
            "summary": null,
            "content_text": "saving your time: manifests to deploy kafka and rabbitmq as stateful set in kind (k8s local)Index  Index  prerequisites  Create kubernetes cluster with kind  Install Kafka          Kafka KRaft x Kafka with ZooKeeper        Install RabbitMQprerequisites            Item      Version                  kind      kind version 0.26.0              kubectl      Client Version: v1.32.1              kafka      docker.io/doughgle/kafka-kraft:latest              rabbitmq      docker.io/library/rabbitmq:3.13.7-management      Create kubernetes cluster with kind1 Create kind configuration(kind-config.yaml)cat &lt;&lt;EOF &gt; /tmp/kind-config.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  extraPortMappings:  - containerPort: 30092    hostPort: 30092    listenAddress: \"0.0.0.0\"    protocol: tcp- role: worker- role: worker- role: workerEOF2 Create a kind clusterkind create cluster --config /tmp/kind-config.yaml --name my-clusterInstall Kafka1 Apply yamlcat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Namespacemetadata:  name: kafka  labels:    name: kafka---apiVersion: apps/v1kind: StatefulSetmetadata:  name: kafka  namespace: kafka  labels:    app: kafka-appspec:  serviceName: kafka-svc  replicas: 3  selector:    matchLabels:      app: kafka-app  template:    metadata:      labels:        app: kafka-app    spec:      containers:        - name: kafka-container          image: doughgle/kafka-kraft          ports:            - containerPort: 9092            - containerPort: 9093          env:            - name: REPLICAS              value: '3'            - name: SERVICE              value: kafka-svc            - name: NAMESPACE              value: kafka            - name: SHARE_DIR              value: /mnt/kafka            - name: CLUSTER_ID              value: bXktY2x1c3Rlci0xMjM0NQ==            - name: DEFAULT_REPLICATION_FACTOR              value: '3'            - name: DEFAULT_MIN_INSYNC_REPLICAS              value: '2'          volumeMounts:            - name: data              mountPath: /mnt/kafka  volumeClaimTemplates:    - metadata:        name: data      spec:        accessModes:          - \"ReadWriteOnce\"        resources:          requests:            storage: \"1Gi\"---apiVersion: v1kind: Servicemetadata:  name: kafka-svc  namespace: kafka  labels:    app: kafka-appspec:  type: NodePort  ports:    - name: '9092'      port: 9092      protocol: TCP      targetPort: 9092      nodePort: 30092  selector:    app: kafka-appEOF  ðŸ•— Wait until all 3 Kafka pods are in the â€œRunningâ€ state.2 Create a topickubectl exec -it kafka-0 -n kafka -- bashkafka-topics.sh --create --topic my-topic --bootstrap-server kafka-svc:9092kafka-topics.sh --list --topic my-topic --bootstrap-server kafka-svc:9092# Produce messagekafka-console-producer.sh --bootstrap-server kafka-svc:9092 --topic my-topic3 Consume message# consumerkubectl exec -it kafka-1 -n kafka -- bashkafka-console-consumer.sh --bootstrap-server kafka-svc:9092 --topic my-topic  Run the producer and consumer in different Linux terminals4 Delete topickafka-topics.sh --delete --topic my-topic --bootstrap-server kafka-svc:9092Kafka KRaft x Kafka with ZooKeeperKafka KRaft Installation: KRaft is Kafkaâ€™s new built-in consensus mechanism that eliminates the need for ZooKeeper. In a KRaft-based installation, Kafka brokers manage metadata and leader election directly, which simplifies the architecture by reducing dependencies. KRaft is becoming the default option in newer Kafka versions because it provides better scalability, faster failover, and an overall more streamlined operation.Kafka with ZooKeeper: In traditional Kafka deployments, ZooKeeper is used to manage the clusterâ€™s metadata, such as broker details and topic configurations. ZooKeeper handles tasks like leader election and tracking which brokers are active. While this setup has been robust for years, it adds complexity by requiring an additional service (ZooKeeper) that must be installed, managed, and maintained alongside Kafka.Install RabbitMQ1 Create a YAML file /tmp/rabbitmq.yml with the content belowapiVersion: v1kind: Namespacemetadata:  name: rabbitmqspec: {}status: {}---apiVersion: v1kind: ServiceAccountmetadata:  name: rabbitmq  namespace: rabbitmq---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: rabbitmq  namespace: rabbitmqrules:- apiGroups:    - \"\"  resources:    - endpoints  verbs:    - get    - list    - watch---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: rabbitmq  namespace: rabbitmqsubjects:- kind: ServiceAccount  name: rabbitmqroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role   name: rabbitmq---apiVersion: v1kind: Secretmetadata:  name: rabbit-secret  namespace: rabbitmqtype: Opaquedata:  RABBITMQ_ERLANG_COOKIE: V0lXVkhDRFRDSVVBV0FOTE1RQVc=  RABBITMQ_DEFAULT_USER: Y29lbGhv  RABBITMQ_DEFAULT_PASS: Y29lbGhvQFBhc3M=---apiVersion: v1kind: ConfigMapmetadata:  name: rabbitmq-config  namespace: rabbitmqdata:  enabled_plugins: |    [rabbitmq_management,rabbitmq_peer_discovery_k8s].  rabbitmq.conf: |    ## Cluster formation. See http://www.rabbitmq.com/cluster-formation.html to learn more.    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local    ## Should RabbitMQ node name be computed from the pod's hostname or IP address?    ## IP addresses are not stable, so using [stable] hostnames is recommended when possible.    ## Set to \"hostname\" to use pod hostnames.    ## When this value is changed, so should the variable used to set the RABBITMQ_NODENAME    ## environment variable.    cluster_formation.k8s.address_type = hostname       ## Important - this is the suffix of the hostname, as each node gets \"rabbitmq-#\", we need to tell what's the suffix    ## it will give each new node that enters the way to contact the other peer node and join the cluster (if using hostname)    cluster_formation.k8s.hostname_suffix = .rabbitmq.test-rabbitmq.svc.cluster.local    ## How often should node cleanup checks run?    cluster_formation.node_cleanup.interval = 30    ## Set to false if automatic removal of unknown/absent nodes    ## is desired. This can be dangerous, see    ##  * http://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup    ##  * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ    cluster_formation.node_cleanup.only_log_warning = true    cluster_partition_handling = autoheal    ## See http://www.rabbitmq.com/ha.html#master-migration-data-locality    queue_master_locator=min-masters    ## See http://www.rabbitmq.com/access-control.html#loopback-users    loopback_users.guest = false---apiVersion: apps/v1kind: StatefulSetmetadata:  name: rabbitmq  namespace: rabbitmqspec:  serviceName: rabbitmq  replicas: 3  selector:    matchLabels:      app: rabbitmq  template:    metadata:      labels:        app: rabbitmq    spec:      serviceAccountName: rabbitmq      initContainers:      - name: config        image: busybox        command: ['/bin/sh', '-c', 'cp /tmp/config/rabbitmq.conf /config/rabbitmq.conf &amp;&amp; ls -l /config/ &amp;&amp; cp /tmp/config/enabled_plugins /etc/rabbitmq/enabled_plugins']        volumeMounts:         - name: config          mountPath: /tmp/config/          readOnly: false        - name: config-file          mountPath: /config/        - name: plugins-file          mountPath: /etc/rabbitmq/        resources: # QoS Guaranteed limit e request iguais          limits:            cpu: 1            memory: 2Gi          requests:            cpu: 1            memory: 2Gi      containers:      - name: rabbitmq        image: rabbitmq:3.13.7-management        ports:        - containerPort: 15672          name: discovery        - containerPort: 5672          name: amqp        env:        - name: RABBIT_POD_NAME          valueFrom:            fieldRef:              apiVersion: v1              fieldPath: metadata.name        - name: RABBIT_POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        - name: RABBITMQ_NODENAME          value: rabbit@$(RABBIT_POD_NAME).rabbitmq.$(RABBIT_POD_NAMESPACE).svc.cluster.local        - name: RABBITMQ_USE_LONGNAME          value: \"true\"        - name: RABBITMQ_CONFIG_FILE          value: \"/config/rabbitmq\"        - name: RABBITMQ_DEFAULT_USER          # value: \"user\"          valueFrom:            secretKeyRef:              name: rabbit-secret              key: RABBITMQ_DEFAULT_USER        - name: RABBITMQ_DEFAULT_PASS          # value: \"password\"          valueFrom:            secretKeyRef:              name: rabbit-secret              key: RABBITMQ_DEFAULT_PASS        - name: K8S_HOSTNAME_SUFFIX          value: .rabbitmq.$(RABBIT_POD_NAMESPACE).svc.cluster.local        volumeMounts:        - name: rabbitmq-pvc-data          mountPath: /var/lib/rabbitmq          readOnly: false        - name: config-file          mountPath: /config/        - name: plugins-file          mountPath: /etc/rabbitmq/        resources: # QoS Guaranteed limit e request iguais          limits:            cpu: 1            memory: 2Gi          # requests:          #   cpu: 0.2m          #   memory: 128Mi      volumes:      - name: config-file        emptyDir: {}      - name: plugins-file        emptyDir: {}      - name: config        configMap:          name: rabbitmq-config          defaultMode: 0755  volumeClaimTemplates:  - metadata:      name: rabbitmq-pvc-data    spec:      accessModes: [\"ReadWriteOnce\"]      # storageClassName: huawei-csi      storageClassName: \"standard\"      resources:        requests:          storage: 1Gi---apiVersion: v1kind: Servicemetadata:  name: rabbitmq  namespace: rabbitmqspec:  type: ClusterIP  ports:  - port: 15672    targetPort: 15672    name: discovery  - port: 5672    targetPort: 5672    name: amqp  selector:    app: rabbitmq2 Apply file /tmp/rabbitmq.ymlkubectl apply -f /tmp/rabbitmq.yml3 Port-forwardkubectl port-forward svc/rabbitmq 15672:15672 -n rabbitmq  Access http://localhost:15672/ User: coelho Password: coelho@Pass",
            "content_html": "<p><strong>saving your time</strong>: <em>manifests to deploy kafka and rabbitmq as stateful set in kind (k8s local)</em></p><h2 id=\"index\">Index</h2><ul>  <li><a href=\"#index\">Index</a></li>  <li><a href=\"#prerequisites\">prerequisites</a></li>  <li><a href=\"#create-kubernetes-cluster-with-kind\">Create kubernetes cluster with kind</a></li>  <li><a href=\"#install-kafka\">Install Kafka</a>    <ul>      <li><a href=\"#kafka-kraft-x-kafka-with-zookeeper\">Kafka KRaft x Kafka with ZooKeeper</a></li>    </ul>  </li>  <li><a href=\"#install-rabbitmq\">Install RabbitMQ</a></li></ul><h2 id=\"prerequisites\">prerequisites</h2><table>  <thead>    <tr>      <th>Item</th>      <th>Version</th>    </tr>  </thead>  <tbody>    <tr>      <td>kind</td>      <td>kind version 0.26.0</td>    </tr>    <tr>      <td>kubectl</td>      <td>Client Version: v1.32.1</td>    </tr>    <tr>      <td>kafka</td>      <td>docker.io/doughgle/kafka-kraft:latest</td>    </tr>    <tr>      <td>rabbitmq</td>      <td>docker.io/library/rabbitmq:3.13.7-management</td>    </tr>  </tbody></table><h2 id=\"create-kubernetes-cluster-with-kind\">Create kubernetes cluster with kind</h2><p>1 Create kind configuration(<em>kind-config.yaml</em>)</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"s\">cat &lt;&lt;EOF &gt; /tmp/kind-config.yaml</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">kind.x-k8s.io/v1alpha4</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Cluster</span><span class=\"na\">nodes</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">control-plane</span>  <span class=\"na\">extraPortMappings</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>    <span class=\"na\">hostPort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>    <span class=\"na\">listenAddress</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">0.0.0.0\"</span>    <span class=\"na\">protocol</span><span class=\"pi\">:</span> <span class=\"s\">tcp</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span><span class=\"s\">EOF</span></code></pre></div></div><p>2 Create a kind cluster</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kind create cluster <span class=\"nt\">--config</span> /tmp/kind-config.yaml <span class=\"nt\">--name</span> my-cluster</code></pre></div></div><h2 id=\"install-kafka\">Install Kafka</h2><p>1 Apply yaml</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"s\">cat &lt;&lt;EOF | kubectl apply -f -</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Namespace</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">StatefulSet</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">serviceName</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>  <span class=\"na\">replicas</span><span class=\"pi\">:</span> <span class=\"m\">3</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">matchLabels</span><span class=\"pi\">:</span>      <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span>  <span class=\"na\">template</span><span class=\"pi\">:</span>    <span class=\"na\">metadata</span><span class=\"pi\">:</span>      <span class=\"na\">labels</span><span class=\"pi\">:</span>        <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span>    <span class=\"na\">spec</span><span class=\"pi\">:</span>      <span class=\"na\">containers</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka-container</span>          <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">doughgle/kafka-kraft</span>          <span class=\"na\">ports</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>            <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">9093</span>          <span class=\"na\">env</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">REPLICAS</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">3'</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">SERVICE</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">NAMESPACE</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">SHARE_DIR</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">/mnt/kafka</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">CLUSTER_ID</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">bXktY2x1c3Rlci0xMjM0NQ==</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">DEFAULT_REPLICATION_FACTOR</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">3'</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">DEFAULT_MIN_INSYNC_REPLICAS</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">2'</span>          <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">data</span>              <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/mnt/kafka</span>  <span class=\"na\">volumeClaimTemplates</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span>        <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">data</span>      <span class=\"na\">spec</span><span class=\"pi\">:</span>        <span class=\"na\">accessModes</span><span class=\"pi\">:</span>          <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">ReadWriteOnce\"</span>        <span class=\"na\">resources</span><span class=\"pi\">:</span>          <span class=\"na\">requests</span><span class=\"pi\">:</span>            <span class=\"na\">storage</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">1Gi\"</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Service</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">NodePort</span>  <span class=\"na\">ports</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">9092'</span>      <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>      <span class=\"na\">protocol</span><span class=\"pi\">:</span> <span class=\"s\">TCP</span>      <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>      <span class=\"na\">nodePort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span><span class=\"s\">EOF</span></code></pre></div></div><blockquote>  <p>ðŸ•— Wait until all 3 Kafka pods are in the â€œRunningâ€ state.</p></blockquote><p>2 Create a topic</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl <span class=\"nb\">exec</span> <span class=\"nt\">-it</span> kafka-0 <span class=\"nt\">-n</span> kafka <span class=\"nt\">--</span> bashkafka-topics.sh <span class=\"nt\">--create</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092kafka-topics.sh <span class=\"nt\">--list</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092<span class=\"c\"># Produce message</span>kafka-console-producer.sh <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092 <span class=\"nt\">--topic</span> my-topic</code></pre></div></div><p>3 Consume message</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># consumer</span>kubectl <span class=\"nb\">exec</span> <span class=\"nt\">-it</span> kafka-1 <span class=\"nt\">-n</span> kafka <span class=\"nt\">--</span> bashkafka-console-consumer.sh <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092 <span class=\"nt\">--topic</span> my-topic</code></pre></div></div><blockquote>  <p>Run the producer and consumer in different Linux terminals</p></blockquote><p>4 Delete topic</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kafka-topics.sh <span class=\"nt\">--delete</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092</code></pre></div></div><h3 id=\"kafka-kraft-x-kafka-with-zookeeper\">Kafka KRaft x Kafka with ZooKeeper</h3><p><strong>Kafka KRaft Installation</strong>: KRaft is Kafkaâ€™s new built-in consensus mechanism that eliminates the need for ZooKeeper. In a KRaft-based installation, Kafka brokers manage metadata and leader election directly, which simplifies the architecture by reducing dependencies. KRaft is becoming the default option in newer Kafka versions because it provides better scalability, faster failover, and an overall more streamlined operation.</p><p><strong>Kafka with ZooKeeper</strong>: In traditional Kafka deployments, ZooKeeper is used to manage the clusterâ€™s metadata, such as broker details and topic configurations. ZooKeeper handles tasks like leader election and tracking which brokers are active. While this setup has been robust for years, it adds complexity by requiring an additional service (ZooKeeper) that must be installed, managed, and maintained alongside Kafka.</p><h2 id=\"install-rabbitmq\">Install RabbitMQ</h2><p>1 Create a YAML file <em>/tmp/rabbitmq.yml</em> with the content below</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Namespace</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span><span class=\"na\">status</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">ServiceAccount</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"nn\">---</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Role</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">rbac.authorization.k8s.io/v1</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">rules</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"na\">apiGroups</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">\"</span>  <span class=\"na\">resources</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"s\">endpoints</span>  <span class=\"na\">verbs</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"s\">get</span>    <span class=\"pi\">-</span> <span class=\"s\">list</span>    <span class=\"pi\">-</span> <span class=\"s\">watch</span><span class=\"nn\">---</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">RoleBinding</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">rbac.authorization.k8s.io/v1</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">subjects</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">ServiceAccount</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">roleRef</span><span class=\"pi\">:</span>  <span class=\"na\">apiGroup</span><span class=\"pi\">:</span> <span class=\"s\">rbac.authorization.k8s.io</span>  <span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Role</span>   <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Secret</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbit-secret</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">Opaque</span><span class=\"na\">data</span><span class=\"pi\">:</span>  <span class=\"na\">RABBITMQ_ERLANG_COOKIE</span><span class=\"pi\">:</span> <span class=\"s\">V0lXVkhDRFRDSVVBV0FOTE1RQVc=</span>  <span class=\"na\">RABBITMQ_DEFAULT_USER</span><span class=\"pi\">:</span> <span class=\"s\">Y29lbGhv</span>  <span class=\"na\">RABBITMQ_DEFAULT_PASS</span><span class=\"pi\">:</span> <span class=\"s\">Y29lbGhvQFBhc3M=</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">ConfigMap</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq-config</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">data</span><span class=\"pi\">:</span>  <span class=\"na\">enabled_plugins</span><span class=\"pi\">:</span> <span class=\"pi\">|</span>    <span class=\"s\">[rabbitmq_management,rabbitmq_peer_discovery_k8s].</span>  <span class=\"na\">rabbitmq.conf</span><span class=\"pi\">:</span> <span class=\"pi\">|</span>    <span class=\"s\">## Cluster formation. See http://www.rabbitmq.com/cluster-formation.html to learn more.</span>    <span class=\"s\">cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s</span>    <span class=\"s\">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local</span>    <span class=\"s\">## Should RabbitMQ node name be computed from the pod's hostname or IP address?</span>    <span class=\"s\">## IP addresses are not stable, so using [stable] hostnames is recommended when possible.</span>    <span class=\"s\">## Set to \"hostname\" to use pod hostnames.</span>    <span class=\"s\">## When this value is changed, so should the variable used to set the RABBITMQ_NODENAME</span>    <span class=\"s\">## environment variable.</span>    <span class=\"s\">cluster_formation.k8s.address_type = hostname   </span>    <span class=\"s\">## Important - this is the suffix of the hostname, as each node gets \"rabbitmq-#\", we need to tell what's the suffix</span>    <span class=\"s\">## it will give each new node that enters the way to contact the other peer node and join the cluster (if using hostname)</span>    <span class=\"s\">cluster_formation.k8s.hostname_suffix = .rabbitmq.test-rabbitmq.svc.cluster.local</span>    <span class=\"s\">## How often should node cleanup checks run?</span>    <span class=\"s\">cluster_formation.node_cleanup.interval = 30</span>    <span class=\"s\">## Set to false if automatic removal of unknown/absent nodes</span>    <span class=\"s\">## is desired. This can be dangerous, see</span>    <span class=\"s\">##  * http://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup</span>    <span class=\"s\">##  * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ</span>    <span class=\"s\">cluster_formation.node_cleanup.only_log_warning = true</span>    <span class=\"s\">cluster_partition_handling = autoheal</span>    <span class=\"s\">## See http://www.rabbitmq.com/ha.html#master-migration-data-locality</span>    <span class=\"s\">queue_master_locator=min-masters</span>    <span class=\"s\">## See http://www.rabbitmq.com/access-control.html#loopback-users</span>    <span class=\"s\">loopback_users.guest = false</span><span class=\"s\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">StatefulSet</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">serviceName</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">replicas</span><span class=\"pi\">:</span> <span class=\"m\">3</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">matchLabels</span><span class=\"pi\">:</span>      <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">template</span><span class=\"pi\">:</span>    <span class=\"na\">metadata</span><span class=\"pi\">:</span>      <span class=\"na\">labels</span><span class=\"pi\">:</span>        <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>    <span class=\"na\">spec</span><span class=\"pi\">:</span>      <span class=\"na\">serviceAccountName</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>      <span class=\"na\">initContainers</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config</span>        <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">busybox</span>        <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s1\">'</span><span class=\"s\">/bin/sh'</span><span class=\"pi\">,</span> <span class=\"s1\">'</span><span class=\"s\">-c'</span><span class=\"pi\">,</span> <span class=\"s1\">'</span><span class=\"s\">cp</span><span class=\"nv\"> </span><span class=\"s\">/tmp/config/rabbitmq.conf</span><span class=\"nv\"> </span><span class=\"s\">/config/rabbitmq.conf</span><span class=\"nv\"> </span><span class=\"s\">&amp;&amp;</span><span class=\"nv\"> </span><span class=\"s\">ls</span><span class=\"nv\"> </span><span class=\"s\">-l</span><span class=\"nv\"> </span><span class=\"s\">/config/</span><span class=\"nv\"> </span><span class=\"s\">&amp;&amp;</span><span class=\"nv\"> </span><span class=\"s\">cp</span><span class=\"nv\"> </span><span class=\"s\">/tmp/config/enabled_plugins</span><span class=\"nv\"> </span><span class=\"s\">/etc/rabbitmq/enabled_plugins'</span><span class=\"pi\">]</span>        <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span>         <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/tmp/config/</span>          <span class=\"na\">readOnly</span><span class=\"pi\">:</span> <span class=\"no\">false</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config-file</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/config/</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">plugins-file</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/etc/rabbitmq/</span>        <span class=\"na\">resources</span><span class=\"pi\">:</span> <span class=\"c1\"># QoS Guaranteed limit e request iguais</span>          <span class=\"na\">limits</span><span class=\"pi\">:</span>            <span class=\"na\">cpu</span><span class=\"pi\">:</span> <span class=\"m\">1</span>            <span class=\"na\">memory</span><span class=\"pi\">:</span> <span class=\"s\">2Gi</span>          <span class=\"na\">requests</span><span class=\"pi\">:</span>            <span class=\"na\">cpu</span><span class=\"pi\">:</span> <span class=\"m\">1</span>            <span class=\"na\">memory</span><span class=\"pi\">:</span> <span class=\"s\">2Gi</span>      <span class=\"na\">containers</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>        <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq:3.13.7-management</span>        <span class=\"na\">ports</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">15672</span>          <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">discovery</span>        <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">5672</span>          <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">amqp</span>        <span class=\"na\">env</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBIT_POD_NAME</span>          <span class=\"na\">valueFrom</span><span class=\"pi\">:</span>            <span class=\"na\">fieldRef</span><span class=\"pi\">:</span>              <span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span>              <span class=\"na\">fieldPath</span><span class=\"pi\">:</span> <span class=\"s\">metadata.name</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBIT_POD_NAMESPACE</span>          <span class=\"na\">valueFrom</span><span class=\"pi\">:</span>            <span class=\"na\">fieldRef</span><span class=\"pi\">:</span>              <span class=\"na\">fieldPath</span><span class=\"pi\">:</span> <span class=\"s\">metadata.namespace</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_NODENAME</span>          <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">rabbit@$(RABBIT_POD_NAME).rabbitmq.$(RABBIT_POD_NAMESPACE).svc.cluster.local</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_USE_LONGNAME</span>          <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">true\"</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_CONFIG_FILE</span>          <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">/config/rabbitmq\"</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_DEFAULT_USER</span>          <span class=\"c1\"># value: \"user\"</span>          <span class=\"na\">valueFrom</span><span class=\"pi\">:</span>            <span class=\"na\">secretKeyRef</span><span class=\"pi\">:</span>              <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbit-secret</span>              <span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_DEFAULT_USER</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_DEFAULT_PASS</span>          <span class=\"c1\"># value: \"password\"</span>          <span class=\"na\">valueFrom</span><span class=\"pi\">:</span>            <span class=\"na\">secretKeyRef</span><span class=\"pi\">:</span>              <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbit-secret</span>              <span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">RABBITMQ_DEFAULT_PASS</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">K8S_HOSTNAME_SUFFIX</span>          <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">.rabbitmq.$(RABBIT_POD_NAMESPACE).svc.cluster.local</span>        <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq-pvc-data</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/var/lib/rabbitmq</span>          <span class=\"na\">readOnly</span><span class=\"pi\">:</span> <span class=\"no\">false</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config-file</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/config/</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">plugins-file</span>          <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/etc/rabbitmq/</span>        <span class=\"na\">resources</span><span class=\"pi\">:</span> <span class=\"c1\"># QoS Guaranteed limit e request iguais</span>          <span class=\"na\">limits</span><span class=\"pi\">:</span>            <span class=\"na\">cpu</span><span class=\"pi\">:</span> <span class=\"m\">1</span>            <span class=\"na\">memory</span><span class=\"pi\">:</span> <span class=\"s\">2Gi</span>          <span class=\"c1\"># requests:</span>          <span class=\"c1\">#   cpu: 0.2m</span>          <span class=\"c1\">#   memory: 128Mi</span>      <span class=\"na\">volumes</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config-file</span>        <span class=\"na\">emptyDir</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span>      <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">plugins-file</span>        <span class=\"na\">emptyDir</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span>      <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">config</span>        <span class=\"na\">configMap</span><span class=\"pi\">:</span>          <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq-config</span>          <span class=\"na\">defaultMode</span><span class=\"pi\">:</span> <span class=\"m\">0755</span>  <span class=\"na\">volumeClaimTemplates</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span>      <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq-pvc-data</span>    <span class=\"na\">spec</span><span class=\"pi\">:</span>      <span class=\"na\">accessModes</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">\"</span><span class=\"s\">ReadWriteOnce\"</span><span class=\"pi\">]</span>      <span class=\"c1\"># storageClassName: huawei-csi</span>      <span class=\"na\">storageClassName</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">standard\"</span>      <span class=\"na\">resources</span><span class=\"pi\">:</span>        <span class=\"na\">requests</span><span class=\"pi\">:</span>          <span class=\"na\">storage</span><span class=\"pi\">:</span> <span class=\"s\">1Gi</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Service</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">ClusterIP</span>  <span class=\"na\">ports</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">15672</span>    <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">15672</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">discovery</span>  <span class=\"pi\">-</span> <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">5672</span>    <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">5672</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">amqp</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">rabbitmq</span></code></pre></div></div><p>2 Apply file <em>/tmp/rabbitmq.yml</em></p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl apply <span class=\"nt\">-f</span> /tmp/rabbitmq.yml</code></pre></div></div><p>3 Port-forward</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl port-forward svc/rabbitmq 15672:15672 <span class=\"nt\">-n</span> rabbitmq</code></pre></div></div><blockquote>  <p>Access <a href=\"http://localhost:15672/\">http://localhost:15672/</a> User: coelho Password: coelho@Pass</p></blockquote><p><img src=\"/assets/img/lab-kafka-and-rabbitmq.png\" alt=\"lab-kafka-and-rabbitmq\" /></p>",
            "url": "https://apolzek.github.io/2024/09/29/kafka-and-rabbitmq-on-kubernetes",
            
            
            
            "tags": ["kafka","kind","kubernetes","rabbitmq"],
            
            "date_published": "2024-09-29T00:00:00+00:00",
            "date_modified": "2024-09-29T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "https://apolzek.github.io/2024/09/26/hello-world",
            "title": "Hello World =)",
            "summary": null,
            "content_text": "saving your time: just a brief introduction and my intentionsABOUT  This blog is a personal project where I share my reflections on reliability engineering, professional growth, and related topics;  English is not my native language, and Iâ€™m using this blog to practice and improve my skills. Please bear with me if you find any grammatical or stylistic errors;  I am a reliability engineer with experience working in Brazil. The views expressed here are my own and do not represent the opinions of my employer or any other organization;  I write primarily for myself.GOALS  Talk about tools that are not well-known but have great potential(maybe this will turn into a new project);  Track my studies with short reflective articles;  Share my views on technologies, processes, and products;  Soon, I will revisit my notes to see how my views on certain topics have changed;  Share the results of some POCs and labs with friends/colleagues.REMARKS  When the ðŸ’¬ emoji appears, I want to introduce a point for reflection or an idea exchange;  When you see the ðŸ—£ï¸ emoji, it means Iâ€™m aware Iâ€™m saying something completely absurd;  There is ðŸ•µï¸ hidden content within this blog; you can find it by exploring or inspecting the source code;  Fixing is better than criticizing, but any interaction is valuable.Welcome aboard !!",
            "content_html": "<p><strong>saving your time</strong>: <em>just a brief introduction and my intentions</em></p><h2 id=\"about\">ABOUT</h2><ul>  <li>This blog is a personal project where I share my reflections on reliability engineering, professional growth, and related topics;</li>  <li>English is <strong>not</strong> my native language, and Iâ€™m using this blog to practice and improve my skills. Please bear with me if you find any grammatical or stylistic errors;</li>  <li>I am a reliability engineer with experience working in Brazil. The views expressed here are my own and do not represent the opinions of my employer or any other organization;</li>  <li>I write primarily for myself.</li></ul><h2 id=\"goals\">GOALS</h2><ul>  <li>Talk about tools that are not well-known but have great potential(<em>maybe this will turn into a new project</em>);</li>  <li>Track my studies with short reflective articles;</li>  <li>Share my views on technologies, processes, and products;</li>  <li>Soon, I will revisit my notes to see how my views on certain topics have changed;</li>  <li>Share the results of some POCs and labs with friends/colleagues.</li></ul><h2 id=\"remarks\">REMARKS</h2><ul>  <li>When the ðŸ’¬ emoji appears, I want to introduce a point for reflection or an idea exchange;</li>  <li>When you see the ðŸ—£ï¸ emoji, it means Iâ€™m aware Iâ€™m saying something completely absurd;</li>  <li>There is ðŸ•µï¸ hidden content within this blog; you can find it by exploring or inspecting the source code;</li>  <li>Fixing is better than criticizing, but any interaction is valuable.</li></ul><p><strong>Welcome aboard !!</strong></p><p><img src=\"/assets/gif/study.webp\" alt=\"study\" /></p><!-- ## OTHER PROJECTS- **@ENV666** is a channel where I will share the results of some proof of concepts (POCs) focused on reliability engineering. The idea is to take down environments and make things break ðŸ”— [https://www.youtube.com/@ENV666](https://www.youtube.com/@ENV666)- **@achadosdogithub** is a channel where I aim to give visibility to projects that are highly useful, whether for learning code or discovering cool tools ðŸ”— [https://www.youtube.com/@achadosdogithub](https://www.youtube.com/@achadosdogithub) -->",
            "url": "https://apolzek.github.io/2024/09/26/hello-world",
            
            
            
            "tags": ["blog","about"],
            
            "date_published": "2024-09-26T00:00:00+00:00",
            "date_modified": "2024-09-26T00:00:00+00:00",
            
                "author":  {
                "name": "apolzek",
                "url": null,
                "avatar": null
                }
                
            
        }
    
    ]
}