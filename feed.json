{
    "version": "https://jsonfeed.org/version/1",
    "title": "apolzek",
    "home_page_url": "/",
    "feed_url": "/feed.json",
    "description": "üî•",
    "icon": "/apple-touch-icon.png",
    "favicon": "/favicon.ico",
    "expired": false,
    
"items": [
    
        {
            "id": "/2025/10/10/special-ops-team.html",
            "title": "Special Ops Team",
            "summary": null,
            "content_text": "",
            "content_html": "",
            "url": "/2025/10/10/special-ops-team.html",
            
            
            
            "tags": ["team","ops"],
            
            "date_published": "2025-10-10T00:00:00+00:00",
            "date_modified": "2025-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/10/10/review-of-computer-networks.html",
            "title": "Review of computer networks",
            "summary": null,
            "content_text": "",
            "content_html": "",
            "url": "/2025/10/10/review-of-computer-networks.html",
            
            
            
            
            
            "date_published": "2025-10-10T00:00:00+00:00",
            "date_modified": "2025-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/10/10/nixos.html",
            "title": "NixOS",
            "summary": null,
            "content_text": "scala examplecurl -L https://nixos.org/nix/install | shsource ~/.nix-profile/etc/profile.d/nix.shmkdir hello-scalacd hello-scala{ pkgs ? import &lt;nixpkgs&gt; {} }:pkgs.mkShell {  buildInputs = [ pkgs.scala pkgs.sbt ];}  shell.nixnix-shellsbt new scala/scala-seed.g8cd   # Substitua pelo nome do seu projetocd src/main/scalatouch HelloWorld.scalaobject HelloWorld {  def main(args: Array[String]): Unit = {    println(\"Hello, World!\")  }}  HelloWorld.scalacd ../../..sbt run",
            "content_html": "<h2 id=\"scala-example\">scala example</h2><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl -L https://nixos.org/nix/install | shsource ~/.nix-profile/etc/profile.d/nix.shmkdir hello-scalacd hello-scala</code></pre></div></div><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{ pkgs ? import &lt;nixpkgs&gt; {} }:pkgs.mkShell {  buildInputs = [ pkgs.scala pkgs.sbt ];}</code></pre></div></div><blockquote>  <p>shell.nix</p></blockquote><p>nix-shellsbt new scala/scala-seed.g8cd <nome-do-projeto>  # Substitua pelo nome do seu projetocd src/main/scalatouch HelloWorld.scala</nome-do-projeto></p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>object HelloWorld {  def main(args: Array[String]): Unit = {    println(\"Hello, World!\")  }}</code></pre></div></div><blockquote>  <p>HelloWorld.scala</p></blockquote><p>cd ../../..<br />sbt run</p>",
            "url": "/2025/10/10/nixos.html",
            
            
            
            "tags": ["nix","os","distro","linux"],
            
            "date_published": "2025-10-10T00:00:00+00:00",
            "date_modified": "2025-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/10/10/entropy-in-the-corporate-world.html",
            "title": "Entropy in the Corporate World",
            "summary": null,
            "content_text": "What results from not cleaning your house regularly ?The subject discussed here is entropy, obviously‚Ä¶ so I‚Äôll start by saying the obvious and then pull out some threads for reflection.Entropy in a corporate environment refers to the natural tendency for things to become disorganized or chaotic over time. It‚Äôs like how your desk gets messy if you don‚Äôt clean it up regularly, or how projects can spin out of control without proper management. In the business world, this concept applies to everything from company processes to team dynamics.As businesses grow or face changes‚Äîwhether it‚Äôs new technology, market shifts, or internal restructuring‚Äîthings can become increasingly complex and harder to manage. Without regular attention, processes can slow down, communication can break down, and overall efficiency drops. This is corporate entropy in action: the gradual decline into disorder unless steps are taken to maintain structure.Leaders and managers often have to deal with this by putting systems in place to keep the ‚Äúchaos‚Äù under control. They use strategies like improving communication, streamlining workflows, or even bringing in new tech to reduce entropy and keep the company running smoothly.In essence, entropy is just a reminder that without constant effort to maintain order, things in the business world can quickly become chaotic, leading to inefficiency and potential failure. It‚Äôs why companies need to stay proactive in managing change and complexity.Different roles and what they really wantIn the world of technology, there are numerous roles such as Software Engineer, DevOps Engineer, Security Analyst, Project Manager, Software Architect, Data Analyst, Data Engineer, Data Scientist, Network Engineer, IT Manager, User Experience Designer, Technical Support Analyst, AI Specialist, Database Administrator, Cloud Specialist, Quality Engineer, Systems Analyst, and many more. To keep it concise, we‚Äôll focus on detailing just a few of these. There‚Äôs a purpose behind this, and I‚Äôll explain it to you shortly.SREAs SREs, our main goal is to make sure our service levels are always met. We want to maintain a healthy and stable system/product, one that can be updated without causing disruptions. It‚Äôs crucial to have visibility into its state and all its components. We need to collaborate effectively with a strong focus on security, stay within budget, automate repetitive tasks, and be prepared to handle any potential incidents swiftly and efficiently.DBREA Database Reliability Engineer (DBRE) aims to ensure that databases are always reliable, highly available, and optimized for performance. Their main goal is to maintain the stability and scalability of database systems, ensuring they can handle increasing loads without sacrificing efficiency. They focus on automating database management tasks like backups, scaling, and monitoring, while ensuring data security and integrity. A DBRE wants a database that is resilient, can be updated or maintained without downtime, and operates efficiently under all conditions, providing a solid foundation for applications to run smoothly.Software EngineerA Software Engineer focuses on designing, developing, and maintaining reliable and efficient software solutions. Their main goal is to build applications that are scalable, robust, and user-friendly, addressing both functionality and performance. They write clean, maintainable code and collaborate with teams to solve complex problems through software. A Software Engineer aims to deliver products that meet user needs, integrate seamlessly with other systems, and can evolve over time with minimal technical debt, ensuring the software remains flexible and adaptable as requirements change.Security AnalystA Security Analyst is responsible for protecting an organization‚Äôs information systems and sensitive data from cyber threats. Their main goal is to identify vulnerabilities, monitor for security breaches, and respond to incidents to mitigate risks. They conduct regular security assessments, implement security policies, and ensure compliance with regulations and standards. A Security Analyst analyzes security events, investigates potential threats, and collaborates with other teams to strengthen the organization‚Äôs overall security posture. Ultimately, they aim to create a secure environment that safeguards the organization‚Äôs assets and maintains the trust of customers and stakeholders.ManagerA Manager plays a crucial role in guiding and overseeing teams to achieve organizational goals. Their main responsibility is to plan, coordinate, and execute projects while ensuring that team members have the resources and support they need to succeed. Managers focus on setting clear objectives, monitoring progress, and fostering a positive work environment that encourages collaboration and productivity. They also handle budgeting, performance evaluations, and conflict resolution, ensuring that the team operates efficiently and effectively. Ultimately, a Manager aims to align team efforts with the company‚Äôs vision, drive results, and facilitate professional development for team members.Demotivating technical people",
            "content_html": "<h2 id=\"what-results-from-not-cleaning-your-house-regularly-\">What results from not cleaning your house regularly ?</h2><p>The subject discussed here is entropy, obviously‚Ä¶ so I‚Äôll start by saying the obvious and then pull out some threads for reflection.</p><p>Entropy in a corporate environment refers to the natural tendency for things to become disorganized or chaotic over time. It‚Äôs like how your desk gets messy if you don‚Äôt clean it up regularly, or how projects can spin out of control without proper management. In the business world, this concept applies to everything from company processes to team dynamics.</p><p>As businesses grow or face changes‚Äîwhether it‚Äôs new technology, market shifts, or internal restructuring‚Äîthings can become increasingly complex and harder to manage. Without regular attention, processes can slow down, communication can break down, and overall efficiency drops. This is corporate entropy in action: the gradual decline into disorder unless steps are taken to maintain structure.</p><p>Leaders and managers often have to deal with this by putting systems in place to keep the ‚Äúchaos‚Äù under control. They use strategies like improving communication, streamlining workflows, or even bringing in new tech to reduce entropy and keep the company running smoothly.</p><p>In essence, entropy is just a reminder that without constant effort to maintain order, things in the business world can quickly become chaotic, leading to inefficiency and potential failure. It‚Äôs why companies need to stay proactive in managing change and complexity.</p><h2 id=\"different-roles-and-what-they-really-want\">Different roles and what they really want</h2><p>In the world of technology, there are numerous roles such as Software Engineer, DevOps Engineer, Security Analyst, Project Manager, Software Architect, Data Analyst, Data Engineer, Data Scientist, Network Engineer, IT Manager, User Experience Designer, Technical Support Analyst, AI Specialist, Database Administrator, Cloud Specialist, Quality Engineer, Systems Analyst, and many more. To keep it concise, we‚Äôll focus on detailing just a few of these. There‚Äôs a purpose behind this, and I‚Äôll explain it to you shortly.</p><h3 id=\"sre\">SRE</h3><p>As SREs, our main goal is to make sure our service levels are always met. We want to maintain a healthy and stable system/product, one that can be updated without causing disruptions. It‚Äôs crucial to have visibility into its state and all its components. We need to collaborate effectively with a strong focus on security, stay within budget, automate repetitive tasks, and be prepared to handle any potential incidents swiftly and efficiently.</p><h3 id=\"dbre\">DBRE</h3><p>A Database Reliability Engineer (DBRE) aims to ensure that databases are always reliable, highly available, and optimized for performance. Their main goal is to maintain the stability and scalability of database systems, ensuring they can handle increasing loads without sacrificing efficiency. They focus on automating database management tasks like backups, scaling, and monitoring, while ensuring data security and integrity. A DBRE wants a database that is resilient, can be updated or maintained without downtime, and operates efficiently under all conditions, providing a solid foundation for applications to run smoothly.</p><h3 id=\"software-engineer\">Software Engineer</h3><p>A Software Engineer focuses on designing, developing, and maintaining reliable and efficient software solutions. Their main goal is to build applications that are scalable, robust, and user-friendly, addressing both functionality and performance. They write clean, maintainable code and collaborate with teams to solve complex problems through software. A Software Engineer aims to deliver products that meet user needs, integrate seamlessly with other systems, and can evolve over time with minimal technical debt, ensuring the software remains flexible and adaptable as requirements change.</p><h3 id=\"security-analyst\">Security Analyst</h3><p>A Security Analyst is responsible for protecting an organization‚Äôs information systems and sensitive data from cyber threats. Their main goal is to identify vulnerabilities, monitor for security breaches, and respond to incidents to mitigate risks. They conduct regular security assessments, implement security policies, and ensure compliance with regulations and standards. A Security Analyst analyzes security events, investigates potential threats, and collaborates with other teams to strengthen the organization‚Äôs overall security posture. Ultimately, they aim to create a secure environment that safeguards the organization‚Äôs assets and maintains the trust of customers and stakeholders.</p><h3 id=\"manager\">Manager</h3><p>A Manager plays a crucial role in guiding and overseeing teams to achieve organizational goals. Their main responsibility is to plan, coordinate, and execute projects while ensuring that team members have the resources and support they need to succeed. Managers focus on setting clear objectives, monitoring progress, and fostering a positive work environment that encourages collaboration and productivity. They also handle budgeting, performance evaluations, and conflict resolution, ensuring that the team operates efficiently and effectively. Ultimately, a Manager aims to align team efforts with the company‚Äôs vision, drive results, and facilitate professional development for team members.</p><h2 id=\"demotivating-technical-people\">Demotivating technical people</h2>",
            "url": "/2025/10/10/entropy-in-the-corporate-world.html",
            
            
            
            
            
            "date_published": "2025-10-10T00:00:00+00:00",
            "date_modified": "2025-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/10/10/aws-iam.html",
            "title": "AWS IAM",
            "summary": null,
            "content_text": "IAM OverviewAWS Identity and Access Management (IAM) is a crucial service within Amazon Web Services (AWS) that helps organizations securely manage access to their cloud resources. Effective access management is vital for maintaining security and operational efficiency, especially in production environments where misconfigurations can lead to vulnerabilities.IAM Users, Groups, and Roles      IAM Users: These are individual identities within AWS, typically representing people or applications. Each user has specific credentials, such as access keys for programmatic access or passwords for the AWS Management Console. Users are granted permissions tailored to their roles‚Äîfor instance, a developer may access AWS Lambda and S3, while an application user might only need API Gateway access.        IAM Groups: Groups are collections of IAM users that share the same permissions. By assigning permissions to a group rather than individual users, organizations can simplify permission management, especially in larger teams. For example, a ‚ÄúDevelopers‚Äù group can be assigned permissions to deploy code, ensuring efficient access control.        IAM Roles: Roles are temporary identities that can be assumed by users or services. Unlike IAM users, roles do not have long-term credentials. They provide temporary security credentials, making them ideal for scenarios like allowing EC2 instances to access S3 without hardcoding credentials. Roles are also useful for cross-account access, enhancing security by limiting the exposure of long-term credentials.  Best Practices for IAM  Principle of Least Privilege: Grant users only the permissions they need to perform their tasks, reducing the risk of accidental or malicious misuse.  Avoid Using Root Account: The root account has unrestricted access and should be reserved for specific administrative tasks.  Implement Multi-Factor Authentication (MFA): MFA adds an extra layer of security, requiring users to verify their identity with a second factor, like a mobile device.  Regular Audits and Monitoring: Utilize tools like AWS CloudTrail to track who accesses what resources, helping to identify excessive permissions and unusual activity.By following these best practices, organizations can create a secure and efficient environment in AWS.Advanced IAM ConceptsAs organizations grow their use of AWS, managing permissions becomes more complex. Advanced features such as Permission Boundaries and Service Control Policies (SCPs) are essential for maintaining security in larger, multi-account setups.Permission BoundariesPermission Boundaries define the maximum permissions that IAM users or roles can have. They act as guardrails to prevent users from granting themselves excessive permissions. For instance, a DevOps team might be allowed to create new IAM roles but restricted from managing critical infrastructure.Service Control Policies (SCPs)SCPs work within AWS Organizations to control access across multiple accounts. They don‚Äôt grant permissions themselves but set the maximum allowable permissions in an account. SCPs are particularly useful for enforcing security policies, like restricting access to certain AWS regions or preventing risky actions across accounts.Advanced IAM Security Practices  IAM Policy Conditions: These allow for fine-tuning permissions based on attributes like IP addresses or specific time frames.  IAM Access Analyzer: This tool helps identify publicly shared resources or access to external accounts, highlighting potential security risks.  MFA Enforcement: Require users, especially those with privileged roles, to authenticate using MFA for sensitive actions.ConclusionAWS IAM offers a comprehensive set of tools for managing permissions in cloud environments. As organizations scale, leveraging advanced features like Permission Boundaries and SCPs becomes crucial for maintaining security and compliance. By implementing best practices and advanced techniques, organizations can effectively mitigate risks and ensure a robust security posture in their AWS environments.",
            "content_html": "<h2 id=\"iam-overview\">IAM Overview</h2><p>AWS Identity and Access Management (IAM) is a crucial service within Amazon Web Services (AWS) that helps organizations securely manage access to their cloud resources. Effective access management is vital for maintaining security and operational efficiency, especially in production environments where misconfigurations can lead to vulnerabilities.</p><h3 id=\"iam-users-groups-and-roles\">IAM Users, Groups, and Roles</h3><ul>  <li>    <p><strong>IAM Users</strong>: These are individual identities within AWS, typically representing people or applications. Each user has specific credentials, such as access keys for programmatic access or passwords for the AWS Management Console. Users are granted permissions tailored to their roles‚Äîfor instance, a developer may access AWS Lambda and S3, while an application user might only need API Gateway access.</p>  </li>  <li>    <p><strong>IAM Groups</strong>: Groups are collections of IAM users that share the same permissions. By assigning permissions to a group rather than individual users, organizations can simplify permission management, especially in larger teams. For example, a ‚ÄúDevelopers‚Äù group can be assigned permissions to deploy code, ensuring efficient access control.</p>  </li>  <li>    <p><strong>IAM Roles</strong>: Roles are temporary identities that can be assumed by users or services. Unlike IAM users, roles do not have long-term credentials. They provide temporary security credentials, making them ideal for scenarios like allowing EC2 instances to access S3 without hardcoding credentials. Roles are also useful for cross-account access, enhancing security by limiting the exposure of long-term credentials.</p>  </li></ul><h3 id=\"best-practices-for-iam\">Best Practices for IAM</h3><ol>  <li><strong>Principle of Least Privilege</strong>: Grant users only the permissions they need to perform their tasks, reducing the risk of accidental or malicious misuse.</li>  <li><strong>Avoid Using Root Account</strong>: The root account has unrestricted access and should be reserved for specific administrative tasks.</li>  <li><strong>Implement Multi-Factor Authentication (MFA)</strong>: MFA adds an extra layer of security, requiring users to verify their identity with a second factor, like a mobile device.</li>  <li><strong>Regular Audits and Monitoring</strong>: Utilize tools like AWS CloudTrail to track who accesses what resources, helping to identify excessive permissions and unusual activity.</li></ol><p>By following these best practices, organizations can create a secure and efficient environment in AWS.</p><h2 id=\"advanced-iam-concepts\">Advanced IAM Concepts</h2><p>As organizations grow their use of AWS, managing permissions becomes more complex. Advanced features such as <strong>Permission Boundaries</strong> and <strong>Service Control Policies (SCPs)</strong> are essential for maintaining security in larger, multi-account setups.</p><h3 id=\"permission-boundaries\">Permission Boundaries</h3><p>Permission Boundaries define the maximum permissions that IAM users or roles can have. They act as guardrails to prevent users from granting themselves excessive permissions. For instance, a DevOps team might be allowed to create new IAM roles but restricted from managing critical infrastructure.</p><h3 id=\"service-control-policies-scps\">Service Control Policies (SCPs)</h3><p>SCPs work within AWS Organizations to control access across multiple accounts. They don‚Äôt grant permissions themselves but set the maximum allowable permissions in an account. SCPs are particularly useful for enforcing security policies, like restricting access to certain AWS regions or preventing risky actions across accounts.</p><h3 id=\"advanced-iam-security-practices\">Advanced IAM Security Practices</h3><ol>  <li><strong>IAM Policy Conditions</strong>: These allow for fine-tuning permissions based on attributes like IP addresses or specific time frames.</li>  <li><strong>IAM Access Analyzer</strong>: This tool helps identify publicly shared resources or access to external accounts, highlighting potential security risks.</li>  <li><strong>MFA Enforcement</strong>: Require users, especially those with privileged roles, to authenticate using MFA for sensitive actions.</li></ol><h3 id=\"conclusion\">Conclusion</h3><p>AWS IAM offers a comprehensive set of tools for managing permissions in cloud environments. As organizations scale, leveraging advanced features like Permission Boundaries and SCPs becomes crucial for maintaining security and compliance. By implementing best practices and advanced techniques, organizations can effectively mitigate risks and ensure a robust security posture in their AWS environments.</p>",
            "url": "/2025/10/10/aws-iam.html",
            
            
            
            
            
            "date_published": "2025-10-10T00:00:00+00:00",
            "date_modified": "2025-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/10/09/shortcuts.html",
            "title": "shortcuts",
            "summary": null,
            "content_text": "VIM            Shortcut      Description                  i      Enter insert mode at cursor position.              I      Enter insert mode at the beginning of the current line.              a      Enter insert mode after the cursor position.              A      Enter insert mode at the end of the current line.              o      Open a new line below and enter insert mode.              O      Open a new line above and enter insert mode.              :w      Save (write) the current file.              :q      Quit Vim.              :wq      Save and quit.              :q!      Quit without saving.              u      Undo the last change.              Ctrl + r      Redo the last undone change.              yy      Yank (copy) the entire current line.              p      Paste the yanked text after the cursor.              P      Paste the yanked text before the cursor.              dd      Delete (cut) the entire current line.              dw      Delete (cut) from the cursor to the end of the current word.              d$      Delete (cut) from the cursor to the end of the line.              x      Delete (cut) the character under the cursor.              r      Replace the character under the cursor with a new one.              v      Enter visual mode (text selection).              V      Enter visual line mode (select entire lines).              Ctrl + v      Enter visual block mode (select rectangular blocks of text).              :s/foo/bar/g      Replace all occurrences of ‚Äúfoo‚Äù with ‚Äúbar‚Äù in the current line.              :%s/foo/bar/g      Replace all occurrences of ‚Äúfoo‚Äù with ‚Äúbar‚Äù in the entire file.              /pattern      Search for a pattern in the file.              n      Jump to the next occurrence of the search pattern.              N      Jump to the previous occurrence of the search pattern.              gg      Go to the beginning of the file.              G      Go to the end of the file.              :e filename      Open a new file named ‚Äúfilename‚Äù.              :bd      Close the current buffer (file).              Ctrl + o      Go to the last cursor position.              Ctrl + i      Go forward to the next cursor position.              Ctrl + z      Suspend Vim (send to background in terminal).              J      Join the current line with the line below it.              .      Repeat the last command.              %      Jump to the matching pair (e.g., parentheses, brackets).              :!command      Run a shell command from within Vim.              Ctrl + f      Scroll one page forward.              Ctrl + b      Scroll one page backward.              Ctrl + u      Scroll half a page up.              Ctrl + d      Scroll half a page down.              H      Move the cursor to the top of the screen.              M      Move the cursor to the middle of the screen.              L      Move the cursor to the bottom of the screen.              :help      Open Vim‚Äôs help documentation.      ",
            "content_html": "<h2 id=\"vim\">VIM</h2><table>  <thead>    <tr>      <th><strong>Shortcut</strong></th>      <th><strong>Description</strong></th>    </tr>  </thead>  <tbody>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">i</code></td>      <td>Enter insert mode at cursor position.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">I</code></td>      <td>Enter insert mode at the beginning of the current line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">a</code></td>      <td>Enter insert mode after the cursor position.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">A</code></td>      <td>Enter insert mode at the end of the current line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">o</code></td>      <td>Open a new line below and enter insert mode.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">O</code></td>      <td>Open a new line above and enter insert mode.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:w</code></td>      <td>Save (write) the current file.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:q</code></td>      <td>Quit Vim.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:wq</code></td>      <td>Save and quit.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:q!</code></td>      <td>Quit without saving.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">u</code></td>      <td>Undo the last change.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + r</code></td>      <td>Redo the last undone change.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">yy</code></td>      <td>Yank (copy) the entire current line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">p</code></td>      <td>Paste the yanked text after the cursor.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">P</code></td>      <td>Paste the yanked text before the cursor.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">dd</code></td>      <td>Delete (cut) the entire current line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">dw</code></td>      <td>Delete (cut) from the cursor to the end of the current word.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">d$</code></td>      <td>Delete (cut) from the cursor to the end of the line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">x</code></td>      <td>Delete (cut) the character under the cursor.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">r</code></td>      <td>Replace the character under the cursor with a new one.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">v</code></td>      <td>Enter visual mode (text selection).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">V</code></td>      <td>Enter visual line mode (select entire lines).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + v</code></td>      <td>Enter visual block mode (select rectangular blocks of text).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:s/foo/bar/g</code></td>      <td>Replace all occurrences of ‚Äúfoo‚Äù with ‚Äúbar‚Äù in the current line.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:%s/foo/bar/g</code></td>      <td>Replace all occurrences of ‚Äúfoo‚Äù with ‚Äúbar‚Äù in the entire file.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">/pattern</code></td>      <td>Search for a pattern in the file.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">n</code></td>      <td>Jump to the next occurrence of the search pattern.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">N</code></td>      <td>Jump to the previous occurrence of the search pattern.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">gg</code></td>      <td>Go to the beginning of the file.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">G</code></td>      <td>Go to the end of the file.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:e filename</code></td>      <td>Open a new file named ‚Äúfilename‚Äù.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:bd</code></td>      <td>Close the current buffer (file).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + o</code></td>      <td>Go to the last cursor position.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + i</code></td>      <td>Go forward to the next cursor position.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + z</code></td>      <td>Suspend Vim (send to background in terminal).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">J</code></td>      <td>Join the current line with the line below it.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">.</code></td>      <td>Repeat the last command.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">%</code></td>      <td>Jump to the matching pair (e.g., parentheses, brackets).</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:!command</code></td>      <td>Run a shell command from within Vim.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + f</code></td>      <td>Scroll one page forward.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + b</code></td>      <td>Scroll one page backward.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + u</code></td>      <td>Scroll half a page up.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">Ctrl + d</code></td>      <td>Scroll half a page down.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">H</code></td>      <td>Move the cursor to the top of the screen.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">M</code></td>      <td>Move the cursor to the middle of the screen.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">L</code></td>      <td>Move the cursor to the bottom of the screen.</td>    </tr>    <tr>      <td><code class=\"language-plaintext highlighter-rouge\">:help</code></td>      <td>Open Vim‚Äôs help documentation.</td>    </tr>  </tbody></table>",
            "url": "/2025/10/09/shortcuts.html",
            
            
            
            
            
            "date_published": "2025-10-09T00:00:00+00:00",
            "date_modified": "2025-10-09T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/09/29/sre-kit-for-the-end-of-the-world.html",
            "title": "üõ†Ô∏è SRE Kit for the End of the World",
            "summary": "Chaos is Inevitable",
            "content_text": "            Tool      Description                  ping      Sends ICMP echo requests to a network host to test connectivity and latency. Useful for checking if a host is reachable.              traceroute      Traces the path packets take to a destination, identifying network hops and where delays or packet losses occur.              netstat      Displays network connections, routing tables, and interface statistics. Helpful for diagnosing network issues and checking active connections.              ss      A modern alternative to netstat, used for displaying more detailed socket and network connection information.              tcpdump      Captures network traffic for analysis. Useful for diagnosing low-level network issues and packet inspection.              dig      Queries DNS servers for domain information, such as IP addresses. Helpful for diagnosing DNS resolution issues.              nslookup      Similar to dig, used for querying DNS information. Useful for simple DNS queries and debugging domain resolution issues.              top      Displays real-time information about system processes, CPU, and memory usage. Useful for identifying resource-hungry processes.              htop      An improved version of top with an interactive, user-friendly interface. Useful for visualizing system resource usage.              iostat      Reports CPU and I/O statistics for devices. Useful for diagnosing disk I/O performance issues.              vmstat      Reports virtual memory statistics, including processes, memory, paging, and CPU usage. Useful for spotting memory bottlenecks.              df      Displays disk space usage for file systems. Useful for checking if any file system is running out of space.              du      Summarizes disk usage of files and directories. Useful for identifying large files or directories consuming disk space.              free      Shows the amount of free and used memory in the system. Useful for diagnosing memory availability issues.              lsof      Lists open files and network connections. Useful for identifying which processes have specific files or sockets open.              strace      Traces system calls made by a process. Useful for debugging issues where a program is failing due to system calls.              dstat      Combines system resource statistics in real-time (disk, network, CPU, memory). Useful for holistic system performance monitoring.              systemctl      Manages system services on Linux. Useful for starting, stopping, or checking the status of services and troubleshooting service issues.              journalctl      Queries and displays system logs from systemd. Useful for troubleshooting service issues, crashes, and other system events.              ps      Lists running processes. Useful for investigating which processes are running, hung, or consuming too many resources.              grep      Searches for patterns within text. Useful for filtering logs or command output for specific keywords or patterns.              awk      A powerful text processing tool used for extracting and transforming data from files or input streams.              sed      Stream editor for filtering and transforming text. Often used to edit configuration files or command outputs in scripts.              curl      Transfers data to or from a server using various protocols (HTTP, FTP). Useful for testing API endpoints or downloading files.              wget      Retrieves files from web servers. Can be used for downloading files or mirroring websites.              ip      Configures network interfaces and displays network configuration. Replaces older ifconfig for advanced network diagnostics.              hostnamectl      Controls and configures system hostname and related settings. Useful for managing the system identity over the network.              uptime      Displays how long the system has been running along with the system load average. Useful for diagnosing system stability issues.              nc (netcat)      A versatile networking tool for reading, writing, and redirecting data over TCP/IP networks. Useful for debugging connectivity issues.              arp      Displays or manipulates the ARP (Address Resolution Protocol) table. Useful for diagnosing issues with IP to MAC address resolution.              iptraf-ng      A real-time network monitoring utility that provides detailed statistics about network traffic. Useful for diagnosing traffic issues.              iftop      Displays bandwidth usage on an interface by host. Useful for identifying bandwidth-intensive connections.              mtr      Combines the functionality of ping and traceroute in a single tool, continuously analyzing the network route. Useful for long-term network diagnostics.              whois      Queries the WHOIS database for domain information such as ownership, expiration, and registrar data. Useful for domain-related troubleshooting.              sar      Collects, reports, and saves system activity information, including CPU, memory, I/O, and network statistics. Useful for long-term performance analysis.              perf      Performance analysis tool for Linux that measures CPU and system performance metrics. Useful for in-depth performance troubleshooting.              nmap      A network scanning tool that discovers devices and services on a network. Useful for security audits and network diagnostics.              ipmitool      Allows management and monitoring of hardware devices using IPMI. Useful for hardware troubleshooting and server health monitoring.              sshd      Secure Shell (SSH) daemon for remote system management. Useful for securely accessing systems to troubleshoot remotely.              rkhunter      Scans for rootkits and security vulnerabilities. Useful for identifying security breaches or malware.              rsync      Efficiently synchronizes files between systems over a network. Useful for backups, data migration, or troubleshooting file transfer issues.              ethtool      Displays and modifies network interface card (NIC) settings. Useful for diagnosing or tuning network performance issues at the hardware level.      ",
            "content_html": "<table>  <thead>    <tr>      <th><strong>Tool</strong></th>      <th><strong>Description</strong></th>    </tr>  </thead>  <tbody>    <tr>      <td><strong>ping</strong></td>      <td>Sends ICMP echo requests to a network host to test connectivity and latency. Useful for checking if a host is reachable.</td>    </tr>    <tr>      <td><strong>traceroute</strong></td>      <td>Traces the path packets take to a destination, identifying network hops and where delays or packet losses occur.</td>    </tr>    <tr>      <td><strong>netstat</strong></td>      <td>Displays network connections, routing tables, and interface statistics. Helpful for diagnosing network issues and checking active connections.</td>    </tr>    <tr>      <td><strong>ss</strong></td>      <td>A modern alternative to <code class=\"language-plaintext highlighter-rouge\">netstat</code>, used for displaying more detailed socket and network connection information.</td>    </tr>    <tr>      <td><strong>tcpdump</strong></td>      <td>Captures network traffic for analysis. Useful for diagnosing low-level network issues and packet inspection.</td>    </tr>    <tr>      <td><strong>dig</strong></td>      <td>Queries DNS servers for domain information, such as IP addresses. Helpful for diagnosing DNS resolution issues.</td>    </tr>    <tr>      <td><strong>nslookup</strong></td>      <td>Similar to <code class=\"language-plaintext highlighter-rouge\">dig</code>, used for querying DNS information. Useful for simple DNS queries and debugging domain resolution issues.</td>    </tr>    <tr>      <td><strong>top</strong></td>      <td>Displays real-time information about system processes, CPU, and memory usage. Useful for identifying resource-hungry processes.</td>    </tr>    <tr>      <td><strong>htop</strong></td>      <td>An improved version of <code class=\"language-plaintext highlighter-rouge\">top</code> with an interactive, user-friendly interface. Useful for visualizing system resource usage.</td>    </tr>    <tr>      <td><strong>iostat</strong></td>      <td>Reports CPU and I/O statistics for devices. Useful for diagnosing disk I/O performance issues.</td>    </tr>    <tr>      <td><strong>vmstat</strong></td>      <td>Reports virtual memory statistics, including processes, memory, paging, and CPU usage. Useful for spotting memory bottlenecks.</td>    </tr>    <tr>      <td><strong>df</strong></td>      <td>Displays disk space usage for file systems. Useful for checking if any file system is running out of space.</td>    </tr>    <tr>      <td><strong>du</strong></td>      <td>Summarizes disk usage of files and directories. Useful for identifying large files or directories consuming disk space.</td>    </tr>    <tr>      <td><strong>free</strong></td>      <td>Shows the amount of free and used memory in the system. Useful for diagnosing memory availability issues.</td>    </tr>    <tr>      <td><strong>lsof</strong></td>      <td>Lists open files and network connections. Useful for identifying which processes have specific files or sockets open.</td>    </tr>    <tr>      <td><strong>strace</strong></td>      <td>Traces system calls made by a process. Useful for debugging issues where a program is failing due to system calls.</td>    </tr>    <tr>      <td><strong>dstat</strong></td>      <td>Combines system resource statistics in real-time (disk, network, CPU, memory). Useful for holistic system performance monitoring.</td>    </tr>    <tr>      <td><strong>systemctl</strong></td>      <td>Manages system services on Linux. Useful for starting, stopping, or checking the status of services and troubleshooting service issues.</td>    </tr>    <tr>      <td><strong>journalctl</strong></td>      <td>Queries and displays system logs from <code class=\"language-plaintext highlighter-rouge\">systemd</code>. Useful for troubleshooting service issues, crashes, and other system events.</td>    </tr>    <tr>      <td><strong>ps</strong></td>      <td>Lists running processes. Useful for investigating which processes are running, hung, or consuming too many resources.</td>    </tr>    <tr>      <td><strong>grep</strong></td>      <td>Searches for patterns within text. Useful for filtering logs or command output for specific keywords or patterns.</td>    </tr>    <tr>      <td><strong>awk</strong></td>      <td>A powerful text processing tool used for extracting and transforming data from files or input streams.</td>    </tr>    <tr>      <td><strong>sed</strong></td>      <td>Stream editor for filtering and transforming text. Often used to edit configuration files or command outputs in scripts.</td>    </tr>    <tr>      <td><strong>curl</strong></td>      <td>Transfers data to or from a server using various protocols (HTTP, FTP). Useful for testing API endpoints or downloading files.</td>    </tr>    <tr>      <td><strong>wget</strong></td>      <td>Retrieves files from web servers. Can be used for downloading files or mirroring websites.</td>    </tr>    <tr>      <td><strong>ip</strong></td>      <td>Configures network interfaces and displays network configuration. Replaces older <code class=\"language-plaintext highlighter-rouge\">ifconfig</code> for advanced network diagnostics.</td>    </tr>    <tr>      <td><strong>hostnamectl</strong></td>      <td>Controls and configures system hostname and related settings. Useful for managing the system identity over the network.</td>    </tr>    <tr>      <td><strong>uptime</strong></td>      <td>Displays how long the system has been running along with the system load average. Useful for diagnosing system stability issues.</td>    </tr>    <tr>      <td><strong>nc (netcat)</strong></td>      <td>A versatile networking tool for reading, writing, and redirecting data over TCP/IP networks. Useful for debugging connectivity issues.</td>    </tr>    <tr>      <td><strong>arp</strong></td>      <td>Displays or manipulates the ARP (Address Resolution Protocol) table. Useful for diagnosing issues with IP to MAC address resolution.</td>    </tr>    <tr>      <td><strong>iptraf-ng</strong></td>      <td>A real-time network monitoring utility that provides detailed statistics about network traffic. Useful for diagnosing traffic issues.</td>    </tr>    <tr>      <td><strong>iftop</strong></td>      <td>Displays bandwidth usage on an interface by host. Useful for identifying bandwidth-intensive connections.</td>    </tr>    <tr>      <td><strong>mtr</strong></td>      <td>Combines the functionality of <code class=\"language-plaintext highlighter-rouge\">ping</code> and <code class=\"language-plaintext highlighter-rouge\">traceroute</code> in a single tool, continuously analyzing the network route. Useful for long-term network diagnostics.</td>    </tr>    <tr>      <td><strong>whois</strong></td>      <td>Queries the WHOIS database for domain information such as ownership, expiration, and registrar data. Useful for domain-related troubleshooting.</td>    </tr>    <tr>      <td><strong>sar</strong></td>      <td>Collects, reports, and saves system activity information, including CPU, memory, I/O, and network statistics. Useful for long-term performance analysis.</td>    </tr>    <tr>      <td><strong>perf</strong></td>      <td>Performance analysis tool for Linux that measures CPU and system performance metrics. Useful for in-depth performance troubleshooting.</td>    </tr>    <tr>      <td><strong>nmap</strong></td>      <td>A network scanning tool that discovers devices and services on a network. Useful for security audits and network diagnostics.</td>    </tr>    <tr>      <td><strong>ipmitool</strong></td>      <td>Allows management and monitoring of hardware devices using IPMI. Useful for hardware troubleshooting and server health monitoring.</td>    </tr>    <tr>      <td><strong>sshd</strong></td>      <td>Secure Shell (SSH) daemon for remote system management. Useful for securely accessing systems to troubleshoot remotely.</td>    </tr>    <tr>      <td><strong>rkhunter</strong></td>      <td>Scans for rootkits and security vulnerabilities. Useful for identifying security breaches or malware.</td>    </tr>    <tr>      <td><strong>rsync</strong></td>      <td>Efficiently synchronizes files between systems over a network. Useful for backups, data migration, or troubleshooting file transfer issues.</td>    </tr>    <tr>      <td><strong>ethtool</strong></td>      <td>Displays and modifies network interface card (NIC) settings. Useful for diagnosing or tuning network performance issues at the hardware level.</td>    </tr>  </tbody></table>",
            "url": "/2025/09/29/sre-kit-for-the-end-of-the-world.html",
            
            
            
            "tags": ["tools","sre","toolbox","linux"],
            
            "date_published": "2025-09-29T00:00:00+00:00",
            "date_modified": "2025-09-29T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/09/29/rabbitmq-on-kubernetes-using-kind.html",
            "title": "RabbitMQ on Kubernetes using Kind",
            "summary": null,
            "content_text": "",
            "content_html": "",
            "url": "/2025/09/29/rabbitmq-on-kubernetes-using-kind.html",
            
            
            
            "tags": ["rabbitmq","kind","kubernetes"],
            
            "date_published": "2025-09-29T00:00:00+00:00",
            "date_modified": "2025-09-29T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2025/09/28/containers-mastery.html",
            "title": "üì¶ Containers Mastery",
            "summary": "Tips and Tricks for Containers",
            "content_text": "What is Linux container ?A Linux container is a lightweight and portable unit that encapsulates an application and its dependencies. Containers utilize key Linux kernel features for process isolation and resource management, including namespaces (for isolating resources like PID, network, and filesystem) and control groups (cgroups) (for limiting and prioritizing resource usage). Additionally, union file systems enable efficient layering and storage of container images. Together, these features allow containers to operate efficiently and securely on a shared host. Basically a container image is composed ofDependencies  + Application code + Container configuration + Base imageContainer runtimeA container runtime is a software component that is responsible for running and managing containers.  Docker  containerd  CRI-O  runc  Podman  LXC/LXD  systemd-nspawn (Don‚Äôt be angry with me)Don‚Äôt compare Docker with KubernetesIt just doesn‚Äôt make sense.. comparing Docker and Kubernetes can be confusing for beginners because they serve different purposes: Docker implements the concept of containers, while Kubernetes orchestrates those containers. An analogy is comparing a car (Docker) to a traffic system (Kubernetes); the car allows you to drive (create and run containers), while the traffic system manages the flow of multiple cars (orchestrates the deployment and scaling of containers). This distinction is crucial, as it clarifies that Docker focuses on the containerization process itself, while Kubernetes handles the management of containerized applications at scale.Container SecurityHaving secure base images for containers is crucial for maintaining the overall security of the host system. Images without elevated privileges help minimize potential attack vectors, reducing the risk of container escape and unauthorized access to the host. While concerns about insecure images are valid, it‚Äôs important to note that this is not the worst-case scenario, as containers typically run in a private network. For an attacker to exploit these vulnerabilities significantly, they would need to compromise additional components of the infrastructure, making it a more complex attack path. Therefore, while secure base images are essential, the overall security posture can still be robust with proper network isolation and access controls in place.Now, talking about the application code that is inside the container.. SAST (Static Application Security Testing) and SCA (Software Composition Analysis) are essential complementary approaches for ensuring application security. While SAST analyzes the source code for vulnerabilities before execution, SCA focuses on third-party libraries and dependencies, checking for known flaws. Integrating both practices is crucial, as it allows for the identification of internal code issues and external risks associated with software components. Tools like SonarQube and Fortify for SAST, and Black Duck and Snyk for SCA, provide robust solutions to mitigate vulnerabilities, offering more comprehensive security throughout the software development lifecycle.In summary, ensure that you use secure base images that prevent vulnerabilities and do not allow root access. Conduct thorough security testing and utilize a scanning tool with an up-to-date CVE database. Keep your images minimal, as even a simple curl command can be exploited by skilled attackers. Implement read-only filesystems to enhance security and adopt practices that make updating images simple and efficient.  Quay  trivy  Docker Desktop (Docker Scout) Vulnerability ScanContainer registryA container registry is a centralized repository where container images are stored, managed, and distributed. Examples of usage include:  Docker Hub  Google Container Registry (GCR)  Amazon Elastic Container Registry (ECR)  Azure Container Registry (ACR)  Harbor  Quay  GitLab Container Registry  JFrog Container RegistryTools for working with containers  dive  Docker CLI  ctr  lazydockerSpecial images  dbeaver/cloudbeaver  netdata/netdata  lscr.io/linuxserver/wireshark:latest  nicolargo/glances  minio/minioBuilding my imagesTypically, each programming language adheres to certain conventions for images. The goal is usually to create a lightweight and secure image. We can leverage concepts such as multi-stage builds and base images with community support. Some best practices include:  Use Multi-Stage Builds: This approach allows you to minimize the final image size by separating the build environment from the production environment.  Choose Official Base Images: Opt for official images from reputable sources to ensure security and reliability.  Keep Images Lightweight: Remove unnecessary files and dependencies to reduce the image size and improve performance.  Regularly Update Images: Stay current with updates to base images and dependencies to mitigate security vulnerabilities.  Use Specific Version Tags: Instead of using ‚Äúlatest,‚Äù specify the exact version of images to avoid unexpected changes in your application.  Scan for Vulnerabilities: Regularly scan your images for known vulnerabilities to maintain security.  Document Image Purpose and Usage: Include clear documentation about the image‚Äôs purpose, usage, and configuration to facilitate easier maintenance and onboarding.Deep dive ? not today..but you need to knowNamespaces and Cgroups ArchitectureNamespaces are a fundamental aspect of containerization, providing isolation for various resources on a Linux system. Each namespace creates a distinct environment for processes, ensuring that they only interact with their own set of resources. For example, the PID namespace allows processes to have their own process IDs, making it appear as though they are the only ones running on the system. Similarly, network namespaces enable containers to have unique network interfaces and IP addresses, preventing interference between containers. Understanding how these namespaces work is essential for developers to effectively manage resource allocation and maintain a secure environment.Control groups (cgroups) complement namespaces by allowing for fine-grained resource management. They enable administrators to limit and prioritize CPU, memory, and I/O usage for groups of processes, ensuring that no single container can monopolize system resources. With cgroups, users can define resource limits, monitor usage, and enforce constraints in a way that is transparent to the applications running within the containers. This architecture not only optimizes resource allocation but also enhances overall system stability by preventing resource contention.Overlay Filesystem and Copy-on-WriteThe Overlay filesystem is a crucial technology in containerization, facilitating the creation of layered filesystems that optimize storage and performance. By allowing multiple layers to be stacked, OverlayFS enables efficient management of container images, where each layer can be modified without affecting the underlying layers. This Copy-on-Write (CoW) mechanism ensures that changes made to a file in a container do not overwrite the original file in the base image. Instead, the container creates a new layer for modifications, allowing for quick and efficient updates while conserving disk space.Using OverlayFS not only improves storage efficiency but also enhances the speed of container operations. As containers are deployed, they only need to load the layers that have changed, significantly reducing the time required to start a container. Additionally, this layering approach allows for easy version control and rollback capabilities. If a change introduces an issue, reverting to a previous version can be done swiftly by switching back to the corresponding base image, thereby minimizing downtime and potential disruptions.Container Runtimes ComparisonContainer runtimes are essential components that facilitate the execution and management of containers, each with unique features and capabilities. Docker, for instance, is a widely recognized runtime that simplifies the process of building, running, and sharing containers. On the other hand, containerd serves as a high-level container runtime, providing a robust platform for managing the complete lifecycle of containers. CRI-O, specifically designed for Kubernetes, focuses on optimizing the performance and resource utilization of containerized applications within orchestration environments. Each runtime caters to different needs, making it important for developers to choose the right one based on their specific use cases and operational requirements.When comparing these runtimes, one must consider factors such as performance, compatibility, and community support. While Docker provides an all-in-one solution for container management, it may introduce overhead that isn‚Äôt present in lighter runtimes like runc, which focuses solely on running containers. Additionally, Podman offers a daemonless experience that enables users to run containers without needing a central service, appealing to those who prioritize security and simplicity. Ultimately, understanding the distinctions between these runtimes helps developers select the most appropriate tools for their containerization strategies.Network Namespaces and Networking ModelsNetwork namespaces are critical for ensuring that containers can communicate while remaining isolated from one another. Each network namespace has its own network stack, including interfaces, routing tables, and firewall rules, allowing containers to function as if they are on separate hosts. This isolation is essential for security, as it prevents unauthorized access between containers and enhances overall system integrity. Additionally, networking models such as bridge networking, overlay networking, and macvlan provide different levels of connectivity and isolation, enabling users to tailor their networking setup based on application needs and deployment scenarios.Understanding these networking models is crucial for optimizing communication between containers. For instance, bridge networking is often used for simpler applications that require direct communication with the host, while overlay networking is ideal for applications deployed across multiple hosts in a cluster, such as those orchestrated by Kubernetes. By leveraging tools like Flannel, Calico, or Cilium, developers can create robust networking solutions that enhance container security and performance. The choice of networking model significantly impacts the architecture of containerized applications and their ability to scale effectively.Advanced Container SecurityAdvanced security measures extend beyond just using secure images; they also encompass implementing Linux capabilities to limit permissions, using Seccomp to filter system calls, and configuring AppArmor profiles for enhanced security. Rootless containers further elevate security by allowing users to run containers without root privileges, significantly reducing the risk of privilege escalation attacks. By integrating these practices, organizations can create a layered security approach that effectively mitigates potential risks while ensuring that containerized applications remain robust and resilient against emerging threats.Volume Management and Data PersistenceEffective volume management is essential for ensuring data persistence in containerized applications. Unlike traditional virtual machines, containers are ephemeral, meaning any data stored within a container is lost once it is stopped or removed. To address this challenge, Docker and other container orchestration platforms provide mechanisms for managing volumes, which allow data to persist independently of the container lifecycle. Volumes can be created and managed easily, enabling developers to store important data, such as databases or user uploads, securely.There are two primary types of storage options for containers: bind mounts and named volumes. Bind mounts allow specific directories on the host to be mounted into a container, providing direct access to host files. However, they can introduce complexity and potential security risks if not managed properly. In contrast, named volumes are managed by the container runtime, offering a more abstracted approach that simplifies data management. By adopting best practices for volume management, such as isolating data from application logic and regularly backing up volumes, developers can ensure that their applications maintain data integrity and resilience in production environments.Advanced Container OrchestrationAdvanced container orchestration is crucial for managing the deployment, scaling, and operation of containerized applications in complex environments. Kubernetes, as a leading orchestration platform, provides robust features for automating the management of containerized applications across clusters. It facilitates load balancing, automated scaling, and self-healing capabilities, allowing organizations to maintain high availability and performance in their applications. Understanding Kubernetes internals, such as the roles of the kubelet, kube-scheduler, and controller manager, empowers developers to optimize their deployment strategies and resource allocation effectively.In addition to Kubernetes, modern orchestration frameworks also support advanced deployment strategies, such as blue-green deployments and canary releases. These methods enable teams to introduce new features gradually, minimizing risk and ensuring a smooth user experience. By leveraging ConfigMaps and Secrets, developers can manage application configurations and sensitive data securely within the orchestration platform. Ultimately, mastering advanced orchestration techniques enhances the efficiency and reliability of containerized applications, driving innovation and agility in software development and deployment.",
            "content_html": "<h2 id=\"what-is-linux-container-\">What is Linux container ?</h2><p>A Linux container is a lightweight and portable unit that encapsulates an application and its dependencies. Containers utilize key Linux kernel features for process isolation and resource management, including namespaces (for isolating resources like PID, network, and filesystem) and control groups (cgroups) (for limiting and prioritizing resource usage). Additionally, union file systems enable efficient layering and storage of container images. Together, these features allow containers to operate efficiently and securely on a shared host. Basically a container image is composed of</p><p><code class=\"language-plaintext highlighter-rouge\">Dependencies  + Application code + Container configuration + Base image</code></p><h2 id=\"container-runtime\">Container runtime</h2><p>A container runtime is a software component that is responsible for running and managing containers.</p><ul>  <li>Docker</li>  <li>containerd</li>  <li>CRI-O</li>  <li>runc</li>  <li>Podman</li>  <li>LXC/LXD</li>  <li>systemd-nspawn (Don‚Äôt be angry with me)</li></ul><h2 id=\"dont-compare-docker-with-kubernetes\">Don‚Äôt compare Docker with Kubernetes</h2><p>It just doesn‚Äôt make sense.. comparing Docker and Kubernetes can be confusing for beginners because they serve different purposes: Docker implements the concept of containers, while Kubernetes orchestrates those containers. An analogy is comparing a car (Docker) to a traffic system (Kubernetes); the car allows you to drive (create and run containers), while the traffic system manages the flow of multiple cars (orchestrates the deployment and scaling of containers). This distinction is crucial, as it clarifies that Docker focuses on the containerization process itself, while Kubernetes handles the management of containerized applications at scale.</p><h2 id=\"container-security\">Container Security</h2><p>Having secure base images for containers is crucial for maintaining the overall security of the host system. Images without elevated privileges help minimize potential attack vectors, reducing the risk of container escape and unauthorized access to the host. While concerns about insecure images are valid, it‚Äôs important to note that this is not the worst-case scenario, as containers typically run in a private network. For an attacker to exploit these vulnerabilities significantly, they would need to compromise additional components of the infrastructure, making it a more complex attack path. Therefore, while secure base images are essential, the overall security posture can still be robust with proper network isolation and access controls in place.</p><p>Now, talking about the application code that is inside the container.. SAST (Static Application Security Testing) and SCA (Software Composition Analysis) are essential complementary approaches for ensuring application security. While SAST analyzes the source code for vulnerabilities before execution, SCA focuses on third-party libraries and dependencies, checking for known flaws. Integrating both practices is crucial, as it allows for the identification of internal code issues and external risks associated with software components. Tools like SonarQube and Fortify for SAST, and Black Duck and Snyk for SCA, provide robust solutions to mitigate vulnerabilities, offering more comprehensive security throughout the software development lifecycle.</p><p>In summary, ensure that you use secure base images that prevent vulnerabilities and do not allow root access. Conduct thorough security testing and utilize a scanning tool with an up-to-date CVE database. Keep your images minimal, as even a simple curl command can be exploited by skilled attackers. Implement read-only filesystems to enhance security and adopt practices that make updating images simple and efficient.</p><ul>  <li>Quay</li>  <li>trivy</li>  <li>Docker Desktop (Docker Scout) Vulnerability Scan</li></ul><h2 id=\"container-registry\">Container registry</h2><p>A container registry is a centralized repository where container images are stored, managed, and distributed. Examples of usage include:</p><ul>  <li>Docker Hub</li>  <li>Google Container Registry (GCR)</li>  <li>Amazon Elastic Container Registry (ECR)</li>  <li>Azure Container Registry (ACR)</li>  <li>Harbor</li>  <li>Quay</li>  <li>GitLab Container Registry</li>  <li>JFrog Container Registry</li></ul><h2 id=\"tools-for-working-with-containers\">Tools for working with containers</h2><ul>  <li>dive</li>  <li>Docker CLI</li>  <li>ctr</li>  <li>lazydocker</li></ul><h2 id=\"special-images\">Special images</h2><ul>  <li>dbeaver/cloudbeaver</li>  <li>netdata/netdata</li>  <li>lscr.io/linuxserver/wireshark:latest</li>  <li>nicolargo/glances</li>  <li>minio/minio</li></ul><h2 id=\"building-my-images\">Building my images</h2><p>Typically, each programming language adheres to certain conventions for images. The goal is usually to create a lightweight and secure image. We can leverage concepts such as multi-stage builds and base images with community support. Some best practices include:</p><ul>  <li>Use Multi-Stage Builds: This approach allows you to minimize the final image size by separating the build environment from the production environment.</li>  <li>Choose Official Base Images: Opt for official images from reputable sources to ensure security and reliability.</li>  <li>Keep Images Lightweight: Remove unnecessary files and dependencies to reduce the image size and improve performance.</li>  <li>Regularly Update Images: Stay current with updates to base images and dependencies to mitigate security vulnerabilities.</li>  <li>Use Specific Version Tags: Instead of using ‚Äúlatest,‚Äù specify the exact version of images to avoid unexpected changes in your application.</li>  <li>Scan for Vulnerabilities: Regularly scan your images for known vulnerabilities to maintain security.</li>  <li>Document Image Purpose and Usage: Include clear documentation about the image‚Äôs purpose, usage, and configuration to facilitate easier maintenance and onboarding.</li></ul><h2 id=\"deep-dive--not-todaybut-you-need-to-know\">Deep dive ? not today..but you need to know</h2><h4 id=\"namespaces-and-cgroups-architecture\">Namespaces and Cgroups Architecture</h4><p>Namespaces are a fundamental aspect of containerization, providing isolation for various resources on a Linux system. Each namespace creates a distinct environment for processes, ensuring that they only interact with their own set of resources. For example, the PID namespace allows processes to have their own process IDs, making it appear as though they are the only ones running on the system. Similarly, network namespaces enable containers to have unique network interfaces and IP addresses, preventing interference between containers. Understanding how these namespaces work is essential for developers to effectively manage resource allocation and maintain a secure environment.</p><p>Control groups (cgroups) complement namespaces by allowing for fine-grained resource management. They enable administrators to limit and prioritize CPU, memory, and I/O usage for groups of processes, ensuring that no single container can monopolize system resources. With cgroups, users can define resource limits, monitor usage, and enforce constraints in a way that is transparent to the applications running within the containers. This architecture not only optimizes resource allocation but also enhances overall system stability by preventing resource contention.</p><h4 id=\"overlay-filesystem-and-copy-on-write\">Overlay Filesystem and Copy-on-Write</h4><p>The Overlay filesystem is a crucial technology in containerization, facilitating the creation of layered filesystems that optimize storage and performance. By allowing multiple layers to be stacked, OverlayFS enables efficient management of container images, where each layer can be modified without affecting the underlying layers. This Copy-on-Write (CoW) mechanism ensures that changes made to a file in a container do not overwrite the original file in the base image. Instead, the container creates a new layer for modifications, allowing for quick and efficient updates while conserving disk space.</p><p>Using OverlayFS not only improves storage efficiency but also enhances the speed of container operations. As containers are deployed, they only need to load the layers that have changed, significantly reducing the time required to start a container. Additionally, this layering approach allows for easy version control and rollback capabilities. If a change introduces an issue, reverting to a previous version can be done swiftly by switching back to the corresponding base image, thereby minimizing downtime and potential disruptions.</p><h4 id=\"container-runtimes-comparison\">Container Runtimes Comparison</h4><p>Container runtimes are essential components that facilitate the execution and management of containers, each with unique features and capabilities. Docker, for instance, is a widely recognized runtime that simplifies the process of building, running, and sharing containers. On the other hand, containerd serves as a high-level container runtime, providing a robust platform for managing the complete lifecycle of containers. CRI-O, specifically designed for Kubernetes, focuses on optimizing the performance and resource utilization of containerized applications within orchestration environments. Each runtime caters to different needs, making it important for developers to choose the right one based on their specific use cases and operational requirements.</p><p>When comparing these runtimes, one must consider factors such as performance, compatibility, and community support. While Docker provides an all-in-one solution for container management, it may introduce overhead that isn‚Äôt present in lighter runtimes like runc, which focuses solely on running containers. Additionally, Podman offers a daemonless experience that enables users to run containers without needing a central service, appealing to those who prioritize security and simplicity. Ultimately, understanding the distinctions between these runtimes helps developers select the most appropriate tools for their containerization strategies.</p><h4 id=\"network-namespaces-and-networking-models\">Network Namespaces and Networking Models</h4><p>Network namespaces are critical for ensuring that containers can communicate while remaining isolated from one another. Each network namespace has its own network stack, including interfaces, routing tables, and firewall rules, allowing containers to function as if they are on separate hosts. This isolation is essential for security, as it prevents unauthorized access between containers and enhances overall system integrity. Additionally, networking models such as bridge networking, overlay networking, and macvlan provide different levels of connectivity and isolation, enabling users to tailor their networking setup based on application needs and deployment scenarios.</p><p>Understanding these networking models is crucial for optimizing communication between containers. For instance, bridge networking is often used for simpler applications that require direct communication with the host, while overlay networking is ideal for applications deployed across multiple hosts in a cluster, such as those orchestrated by Kubernetes. By leveraging tools like Flannel, Calico, or Cilium, developers can create robust networking solutions that enhance container security and performance. The choice of networking model significantly impacts the architecture of containerized applications and their ability to scale effectively.</p><h4 id=\"advanced-container-security\">Advanced Container Security</h4><p>Advanced security measures extend beyond just using secure images; they also encompass implementing Linux capabilities to limit permissions, using Seccomp to filter system calls, and configuring AppArmor profiles for enhanced security. Rootless containers further elevate security by allowing users to run containers without root privileges, significantly reducing the risk of privilege escalation attacks. By integrating these practices, organizations can create a layered security approach that effectively mitigates potential risks while ensuring that containerized applications remain robust and resilient against emerging threats.</p><h4 id=\"volume-management-and-data-persistence\">Volume Management and Data Persistence</h4><p>Effective volume management is essential for ensuring data persistence in containerized applications. Unlike traditional virtual machines, containers are ephemeral, meaning any data stored within a container is lost once it is stopped or removed. To address this challenge, Docker and other container orchestration platforms provide mechanisms for managing volumes, which allow data to persist independently of the container lifecycle. Volumes can be created and managed easily, enabling developers to store important data, such as databases or user uploads, securely.</p><p>There are two primary types of storage options for containers: bind mounts and named volumes. Bind mounts allow specific directories on the host to be mounted into a container, providing direct access to host files. However, they can introduce complexity and potential security risks if not managed properly. In contrast, named volumes are managed by the container runtime, offering a more abstracted approach that simplifies data management. By adopting best practices for volume management, such as isolating data from application logic and regularly backing up volumes, developers can ensure that their applications maintain data integrity and resilience in production environments.</p><h4 id=\"advanced-container-orchestration\">Advanced Container Orchestration</h4><p>Advanced container orchestration is crucial for managing the deployment, scaling, and operation of containerized applications in complex environments. Kubernetes, as a leading orchestration platform, provides robust features for automating the management of containerized applications across clusters. It facilitates load balancing, automated scaling, and self-healing capabilities, allowing organizations to maintain high availability and performance in their applications. Understanding Kubernetes internals, such as the roles of the kubelet, kube-scheduler, and controller manager, empowers developers to optimize their deployment strategies and resource allocation effectively.</p><p>In addition to Kubernetes, modern orchestration frameworks also support advanced deployment strategies, such as blue-green deployments and canary releases. These methods enable teams to introduce new features gradually, minimizing risk and ensuring a smooth user experience. By leveraging ConfigMaps and Secrets, developers can manage application configurations and sensitive data securely within the orchestration platform. Ultimately, mastering advanced orchestration techniques enhances the efficiency and reliability of containerized applications, driving innovation and agility in software development and deployment.</p>",
            "url": "/2025/09/28/containers-mastery.html",
            
            
            
            "tags": ["linux","containers","docker"],
            
            "date_published": "2025-09-28T00:00:00+00:00",
            "date_modified": "2025-09-28T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/10/13/sre-interview-questions.html",
            "title": "SRE interview questions",
            "summary": null,
            "content_text": "Although I have a solid foundation in various technology topics and in-depth knowledge in many of them, I still lack the confidence to conduct interviews. You never know what you might encounter on the other side. Regardless, I would like to share my thought process for conducting a technical interview for the position of Site Reliability Engineer or DevOps Engineer.NetworkingSuppose you have a proxy that operates at the TCP level and another that operates only at the HTTP level. What are the main implications of this configuration? How can these differences affect error handling, security, and network scalability ?KubernetesIn the context of Kubernetes, you are designing an application that requires different pod management strategies to meet specific requirements. Explain how you would utilize ReplicaSets, Deployments, StatefulSets, and DaemonSets to address these needs. In particular, discuss the ideal use cases for each of these objects and how they behave in relation to scalability, data persistence, and version updates. Additionally, what considerations should you keep in mind when choosing between these types of pod controllers for different components of your application ?PrometheusIn the context of monitoring with Prometheus, you encounter a cardinality problem where the number of time series becomes excessive, resulting in performance degradation and excessive resource usage. What are the common causes of cardinality issues in Prometheus, and what strategies can you employ to mitigate these problems ?CI/CDIn a continuous integration and continuous delivery (CI/CD) environment, the execution time of pipelines can significantly impact the speed of development and software delivery. What techniques and practices would you implement to optimize the execution time of CI/CD pipelines?You are managing a project that uses Jenkins for continuous integration and GitLab CI for continuous delivery. Discuss how you would integrate these two tools into a unified CI/CD workflow. What considerations would you have regarding pipeline configuration, credential management, and infrastructure versioning? Additionally, analyze the advantages and disadvantages of using Jenkins alongside GitLab CI, considering aspects such as flexibility, scalability, and maintenance complexity.",
            "content_html": "<p>Although I have a solid foundation in various technology topics and in-depth knowledge in many of them, I still lack the confidence to conduct interviews. You never know what you might encounter on the other side. Regardless, I would like to share my thought process for conducting a technical interview for the position of Site Reliability Engineer or DevOps Engineer.</p><h2 id=\"networking\">Networking</h2><p>Suppose you have a proxy that operates at the TCP level and another that operates only at the HTTP level. What are the main implications of this configuration? How can these differences affect error handling, security, and network scalability ?</p><h2 id=\"kubernetes\">Kubernetes</h2><p>In the context of Kubernetes, you are designing an application that requires different pod management strategies to meet specific requirements. Explain how you would utilize ReplicaSets, Deployments, StatefulSets, and DaemonSets to address these needs. In particular, discuss the ideal use cases for each of these objects and how they behave in relation to scalability, data persistence, and version updates. Additionally, what considerations should you keep in mind when choosing between these types of pod controllers for different components of your application ?</p><h2 id=\"prometheus\">Prometheus</h2><p>In the context of monitoring with Prometheus, you encounter a cardinality problem where the number of time series becomes excessive, resulting in performance degradation and excessive resource usage. What are the common causes of cardinality issues in Prometheus, and what strategies can you employ to mitigate these problems ?</p><h2 id=\"cicd\">CI/CD</h2><p>In a continuous integration and continuous delivery (CI/CD) environment, the execution time of pipelines can significantly impact the speed of development and software delivery. What techniques and practices would you implement to optimize the execution time of CI/CD pipelines?</p><p>You are managing a project that uses Jenkins for continuous integration and GitLab CI for continuous delivery. Discuss how you would integrate these two tools into a unified CI/CD workflow. What considerations would you have regarding pipeline configuration, credential management, and infrastructure versioning? Additionally, analyze the advantages and disadvantages of using Jenkins alongside GitLab CI, considering aspects such as flexibility, scalability, and maintenance complexity.</p>",
            "url": "/2024/10/13/sre-interview-questions.html",
            
            
            
            "tags": ["sre","interview"],
            
            "date_published": "2024-10-13T00:00:00+00:00",
            "date_modified": "2024-10-13T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/10/13/endeavourOS-i3wm-why.html",
            "title": "EndeavourOS i3wm, why ?",
            "summary": null,
            "content_text": "Recently, I came across a provocative phrase: ‚ÄúLinux is only free if your time has no value.‚Äù I know some people around me would get a bit annoyed by this, and honestly, a few years ago, I would‚Äôve been one of them. But today, with a little more experience (and patience), I still disagree, but I get the point. I‚Äôve always been a fan of working with good tools, having a well-configured OS, customized, easy to restore, and that makes me more productive. The truth is, we spend time to save time, right? And that orks‚Ä¶ to a certain extent.The issue is that after a while, I realized something about myself: I get bored of everything, even when it‚Äôs ‚Äúalmost perfect.‚Äù I think a lot of people can relate to this. We‚Äôre always tweaking, adjusting, trying to make things better. But in the end, the time and effort you put in don‚Äôt always pay off. When you sit down to do the math, it doesn‚Äôt always add up. Sure, we learn and improve with each new experience, the learning curve gets shorter‚Ä¶ but then the question hit me: what do we really need to get work done? For me, the answer is simple: running the applications we need.At the companies I‚Äôve worked at, I‚Äôve used both MacBooks with macOS and Dell laptops running Ubuntu. And you know what? Both worked just fine! I‚Äôm always trying to improve in different areas ‚Äî physically, gaining new skills, or fixing mistakes. But then come the trade-offs: I need time! And that got me thinking about how much time I‚Äôve spent in the past messing around with Linux distros, customizing tools, switching window managers, tweaking every little detail that only I would notice.In the end, I decided to stop and simplify. With the least amount of effort, I found a way to focus on what really matters: solving the problems I‚Äôm paid to solve. No need to reinvent the wheel.            Tool      Description      Category                  VSCode      A powerful source code editor from Microsoft.      Development              LazyVim      A preconfigured Neovim setup for lazy editing.      Development              Neovim      An extensible Vim-based text editor.      Development              Git      A version control system for tracking changes.      Version Control              Discord      A communication platform for communities.      Communication              Telegram      A cloud-based messaging app with a focus on speed.      Communication              Tor Browser      A web browser that enables anonymous browsing.      Privacy              NordVPN      A VPN service for secure and private browsing.      Privacy              Chrome      A fast web browser developed by Google.      Web Browsing              Firefox      An open-source web browser with privacy features.      Web Browsing              Meld      A visual diff and merge tool.      Development              Golang      A statically typed, compiled programming language.      Development              Python      A high-level programming language known for readability.      Development              Node.js      A JavaScript runtime built on Chrome‚Äôs V8 engine.      Development              Obsidian      A note-taking app that uses Markdown.      Productivity              VirtualBox      A powerful x86 and AMD64 virtualization product.      Virtualization              Timeshift      A backup tool for Linux systems.      Backup              Arduino IDE      An integrated development environment for Arduino.      Development              Lens      A Kubernetes IDE for managing clusters.      Development              Btop      A resource monitor that shows usage stats.      System Monitoring              DBeaver      A universal database tool for developers.      Database Management              Postman      A collaboration platform for API development.      Development              Insomnia      A REST client for API testing.      Development              OBS Studio      Open-source software for video recording and live streaming.      Streaming              Wireshark      A network protocol analyzer for troubleshooting.      Networking              Burp Suite Community      A platform for web application security testing.      Security              Anki      A flashcard app for memorization.      Education              Slack      A collaboration hub for team communication.      Communication              Teams      A collaboration tool by Microsoft for workplaces.      Communication              Spotify      A digital music service for streaming music.      Entertainment              VLC      A free and open-source multimedia player.      Media      ",
            "content_html": "<p>Recently, I came across a provocative phrase: ‚ÄúLinux is only free if your time has no value.‚Äù I know some people around me would get a bit annoyed by this, and honestly, a few years ago, I would‚Äôve been one of them. But today, with a little more experience (and patience), I still disagree, but I get the point. I‚Äôve always been a fan of working with good tools, having a well-configured OS, customized, easy to restore, and that makes me more productive. The truth is, we spend time to save time, right? And that orks‚Ä¶ to a certain extent.</p><p>The issue is that after a while, I realized something about myself: I get bored of everything, even when it‚Äôs ‚Äúalmost perfect.‚Äù I think a lot of people can relate to this. We‚Äôre always tweaking, adjusting, trying to make things better. But in the end, the time and effort you put in don‚Äôt always pay off. When you sit down to do the math, it doesn‚Äôt always add up. Sure, we learn and improve with each new experience, the learning curve gets shorter‚Ä¶ but then the question hit me: what do we really need to get work done? For me, the answer is simple: running the applications we need.</p><p>At the companies I‚Äôve worked at, I‚Äôve used both MacBooks with macOS and Dell laptops running Ubuntu. And you know what? Both worked just fine! I‚Äôm always trying to improve in different areas ‚Äî physically, gaining new skills, or fixing mistakes. But then come the trade-offs: I need time! And that got me thinking about how much time I‚Äôve spent in the past messing around with Linux distros, customizing tools, switching window managers, tweaking every little detail that only I would notice.</p><p>In the end, I decided to stop and simplify. With the least amount of effort, I found a way to focus on what really matters: solving the problems I‚Äôm paid to solve. No need to reinvent the wheel.</p><table>  <thead>    <tr>      <th>Tool</th>      <th>Description</th>      <th>Category</th>    </tr>  </thead>  <tbody>    <tr>      <td>VSCode</td>      <td>A powerful source code editor from Microsoft.</td>      <td>Development</td>    </tr>    <tr>      <td>LazyVim</td>      <td>A preconfigured Neovim setup for lazy editing.</td>      <td>Development</td>    </tr>    <tr>      <td>Neovim</td>      <td>An extensible Vim-based text editor.</td>      <td>Development</td>    </tr>    <tr>      <td>Git</td>      <td>A version control system for tracking changes.</td>      <td>Version Control</td>    </tr>    <tr>      <td>Discord</td>      <td>A communication platform for communities.</td>      <td>Communication</td>    </tr>    <tr>      <td>Telegram</td>      <td>A cloud-based messaging app with a focus on speed.</td>      <td>Communication</td>    </tr>    <tr>      <td>Tor Browser</td>      <td>A web browser that enables anonymous browsing.</td>      <td>Privacy</td>    </tr>    <tr>      <td>NordVPN</td>      <td>A VPN service for secure and private browsing.</td>      <td>Privacy</td>    </tr>    <tr>      <td>Chrome</td>      <td>A fast web browser developed by Google.</td>      <td>Web Browsing</td>    </tr>    <tr>      <td>Firefox</td>      <td>An open-source web browser with privacy features.</td>      <td>Web Browsing</td>    </tr>    <tr>      <td>Meld</td>      <td>A visual diff and merge tool.</td>      <td>Development</td>    </tr>    <tr>      <td>Golang</td>      <td>A statically typed, compiled programming language.</td>      <td>Development</td>    </tr>    <tr>      <td>Python</td>      <td>A high-level programming language known for readability.</td>      <td>Development</td>    </tr>    <tr>      <td>Node.js</td>      <td>A JavaScript runtime built on Chrome‚Äôs V8 engine.</td>      <td>Development</td>    </tr>    <tr>      <td>Obsidian</td>      <td>A note-taking app that uses Markdown.</td>      <td>Productivity</td>    </tr>    <tr>      <td>VirtualBox</td>      <td>A powerful x86 and AMD64 virtualization product.</td>      <td>Virtualization</td>    </tr>    <tr>      <td>Timeshift</td>      <td>A backup tool for Linux systems.</td>      <td>Backup</td>    </tr>    <tr>      <td>Arduino IDE</td>      <td>An integrated development environment for Arduino.</td>      <td>Development</td>    </tr>    <tr>      <td>Lens</td>      <td>A Kubernetes IDE for managing clusters.</td>      <td>Development</td>    </tr>    <tr>      <td>Btop</td>      <td>A resource monitor that shows usage stats.</td>      <td>System Monitoring</td>    </tr>    <tr>      <td>DBeaver</td>      <td>A universal database tool for developers.</td>      <td>Database Management</td>    </tr>    <tr>      <td>Postman</td>      <td>A collaboration platform for API development.</td>      <td>Development</td>    </tr>    <tr>      <td>Insomnia</td>      <td>A REST client for API testing.</td>      <td>Development</td>    </tr>    <tr>      <td>OBS Studio</td>      <td>Open-source software for video recording and live streaming.</td>      <td>Streaming</td>    </tr>    <tr>      <td>Wireshark</td>      <td>A network protocol analyzer for troubleshooting.</td>      <td>Networking</td>    </tr>    <tr>      <td>Burp Suite Community</td>      <td>A platform for web application security testing.</td>      <td>Security</td>    </tr>    <tr>      <td>Anki</td>      <td>A flashcard app for memorization.</td>      <td>Education</td>    </tr>    <tr>      <td>Slack</td>      <td>A collaboration hub for team communication.</td>      <td>Communication</td>    </tr>    <tr>      <td>Teams</td>      <td>A collaboration tool by Microsoft for workplaces.</td>      <td>Communication</td>    </tr>    <tr>      <td>Spotify</td>      <td>A digital music service for streaming music.</td>      <td>Entertainment</td>    </tr>    <tr>      <td>VLC</td>      <td>A free and open-source multimedia player.</td>      <td>Media</td>    </tr>  </tbody></table>",
            "url": "/2024/10/13/endeavourOS-i3wm-why.html",
            
            
            
            "tags": ["os","i3wm","endeavouros,","linux,","distro"],
            
            "date_published": "2024-10-13T00:00:00+00:00",
            "date_modified": "2024-10-13T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/10/10/l0calh0t-startup-day2.html",
            "title": "üí° l0calh0t Startup - day 2",
            "summary": "Building the l0calh0t Startup",
            "content_text": "render.com needs to be said.. they are goodSince my idea has A LOT to do with what render.com offers, it makes total sense for me to learn from them. First off, just know that they‚Äôre good. I‚Äôve already liked a lot of what I found on their free tier. I‚Äôll dive into the details here and connect it to some technical concepts just to get the wheels turning on architecture and product thinking.",
            "content_html": "<h2 id=\"rendercom-needs-to-be-said-they-are-good\">render.com needs to be said.. they are good</h2><p>Since my idea has A LOT to do with what <strong>render.com</strong> offers, it makes total sense for me to learn from them. First off, just know that they‚Äôre good. I‚Äôve already liked a lot of what I found on their free tier. I‚Äôll dive into the details here and connect it to some technical concepts just to get the wheels turning on architecture and product thinking.</p>",
            "url": "/2024/10/10/l0calh0t-startup-day2.html",
            
            
            
            "tags": ["devops","sre","project","startup"],
            
            "date_published": "2024-10-10T00:00:00+00:00",
            "date_modified": "2024-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/10/10/getting-started-x-production-ready.html",
            "title": "The distance between \"getting started\" and \"production ready\"",
            "summary": null,
            "content_text": "When embarking on the journey of implementing a new tool, it is essential to recognize the significant gap between the initial ‚Äúgetting started‚Äù phase and achieving a fully functional production-ready system. In the beginning, users often interact with a simplified version of the tool, which showcases its basic functionalities. However, to fully harness the capabilities of the tool and integrate it into the organization‚Äôs ecosystem, several critical aspects must be addressed.Deployment is one of the first challenges encountered when moving from a basic setup to a production-ready state. In the initial stages, deployment may be as straightforward as running a local instance. However, in a production environment, organizations must consider robust deployment strategies, including automated deployment pipelines, continuous integration/continuous deployment (CI/CD) practices, and the use of containerization technologies such as Docker. These practices ensure that the tool can be deployed seamlessly, with minimal downtime and maximum efficiency.Scalability is another crucial factor that differentiates basic usage from a production-ready application. While a basic setup may function adequately for a small number of users or limited workloads, production environments must accommodate growth. This requires careful planning for horizontal and vertical scaling, load balancing, and resource allocation to ensure that the tool can handle increasing demands without compromising performance or user experience.The adaptation to company processes is also vital in this transition. A basic implementation might showcase generic use cases, but to be effective in a production environment, the tool must align with the specific workflows and procedures of the organization. This may involve customizing features, integrating with existing systems, and training employees to ensure that the tool complements their daily tasks and contributes to overall productivity.Security is a paramount concern when moving towards production readiness. Initial setups may overlook critical security protocols, while a production-ready tool must implement stringent security measures. This includes user authentication and authorization, data encryption, secure APIs, and compliance with industry regulations. Organizations must also conduct regular security audits and vulnerability assessments to protect sensitive data and maintain trust with users.Traceability is another important aspect that sets a production-ready tool apart from its basic counterpart. In a production environment, tracking changes, actions, and data flows becomes essential for accountability and compliance. Implementing features like audit logs, change tracking, and comprehensive reporting enables organizations to maintain oversight and transparency in their operations.Permissioning is a vital component of ensuring that users have appropriate access levels to various functionalities and data within the tool. In the basic mode, permission settings may be minimal, but in a production environment, fine-grained access control is essential. This allows organizations to enforce the principle of least privilege, ensuring that users can only access the information and functions necessary for their roles.Lastly, organizations must consider the Software Development Operations (SDOX) aspect, which encompasses the entire lifecycle of the tool‚Äîfrom development and deployment to maintenance and support. In a production-ready setup, this requires a well-defined process for software updates, user feedback, and incident management to ensure continuous improvement and responsiveness to user needs.In conclusion, while the ‚Äúgetting started‚Äù phase of a tool provides a valuable introduction to its functionalities, the journey towards a production-ready state is complex and multifaceted. Organizations must invest time and resources to address deployment strategies, scalability, process adaptation, security measures, traceability, permissioning, and SDOX practices. Only then can they fully leverage the tool‚Äôs potential and ensure it aligns with their operational goals.",
            "content_html": "<p>When embarking on the journey of implementing a new tool, it is essential to recognize the significant gap between the initial ‚Äúgetting started‚Äù phase and achieving a fully functional production-ready system. In the beginning, users often interact with a simplified version of the tool, which showcases its basic functionalities. However, to fully harness the capabilities of the tool and integrate it into the organization‚Äôs ecosystem, several critical aspects must be addressed.</p><p>Deployment is one of the first challenges encountered when moving from a basic setup to a production-ready state. In the initial stages, deployment may be as straightforward as running a local instance. However, in a production environment, organizations must consider robust deployment strategies, including automated deployment pipelines, continuous integration/continuous deployment (CI/CD) practices, and the use of containerization technologies such as Docker. These practices ensure that the tool can be deployed seamlessly, with minimal downtime and maximum efficiency.</p><p>Scalability is another crucial factor that differentiates basic usage from a production-ready application. While a basic setup may function adequately for a small number of users or limited workloads, production environments must accommodate growth. This requires careful planning for horizontal and vertical scaling, load balancing, and resource allocation to ensure that the tool can handle increasing demands without compromising performance or user experience.</p><p>The adaptation to company processes is also vital in this transition. A basic implementation might showcase generic use cases, but to be effective in a production environment, the tool must align with the specific workflows and procedures of the organization. This may involve customizing features, integrating with existing systems, and training employees to ensure that the tool complements their daily tasks and contributes to overall productivity.</p><p>Security is a paramount concern when moving towards production readiness. Initial setups may overlook critical security protocols, while a production-ready tool must implement stringent security measures. This includes user authentication and authorization, data encryption, secure APIs, and compliance with industry regulations. Organizations must also conduct regular security audits and vulnerability assessments to protect sensitive data and maintain trust with users.</p><p>Traceability is another important aspect that sets a production-ready tool apart from its basic counterpart. In a production environment, tracking changes, actions, and data flows becomes essential for accountability and compliance. Implementing features like audit logs, change tracking, and comprehensive reporting enables organizations to maintain oversight and transparency in their operations.</p><p>Permissioning is a vital component of ensuring that users have appropriate access levels to various functionalities and data within the tool. In the basic mode, permission settings may be minimal, but in a production environment, fine-grained access control is essential. This allows organizations to enforce the principle of least privilege, ensuring that users can only access the information and functions necessary for their roles.</p><p>Lastly, organizations must consider the Software Development Operations (SDOX) aspect, which encompasses the entire lifecycle of the tool‚Äîfrom development and deployment to maintenance and support. In a production-ready setup, this requires a well-defined process for software updates, user feedback, and incident management to ensure continuous improvement and responsiveness to user needs.</p><p>In conclusion, while the ‚Äúgetting started‚Äù phase of a tool provides a valuable introduction to its functionalities, the journey towards a production-ready state is complex and multifaceted. Organizations must invest time and resources to address deployment strategies, scalability, process adaptation, security measures, traceability, permissioning, and SDOX practices. Only then can they fully leverage the tool‚Äôs potential and ensure it aligns with their operational goals.</p>",
            "url": "/2024/10/10/getting-started-x-production-ready.html",
            
            
            
            
            
            "date_published": "2024-10-10T00:00:00+00:00",
            "date_modified": "2024-10-10T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/09/29/spells-for-linux-shell.html",
            "title": "ü™Ñ Spells for Linux Shell",
            "summary": null,
            "content_text": "ü™Ñ Spells for Linux Shell1. LOOPING: command x scriptcommandfor i in {1..5}; do echo $i; donecount=1; while [ $count -le 5 ]; do echo $count; ((count++)); donescript#!/bin/bashecho \"Counting to 5 with a for loop:\"for i in {1..5}; do    echo $idoneecho \"Counting to 5 with a while loop:\"count=1while [ $count -le 5 ]; do    echo $count    ((count++))done2. signalsSome scenarios that make sense to deal with signals..  Cleaning Temporary Files  Graceful Interruption of Services  Avoiding Database Corruption  Releasing Network or Hardware Resources  Maintaining Consistent Variable State-SIGINTThis script creates a temporary file and displays a goodbye message when it receives a SIGINT signal (e.g., when the user presses Ctrl+C).#!/bin/bash# Function that will be called when the script receives the SIGINT signalfunction exitMsg {    echo \"you sent a signal to end, byby !!\"    # command here    exit}# Sets up the trap to call the cleanup function when the script receives SIGINTtrap exitMsg SIGINTsleep 5echo \"Creating a temporary file...\"touch /tmp/apolzek || exit 1  # Creates a temporary file or exits with an errorSIGTERMThis script starts a web server that runs indefinitely and shuts down gracefully when it receives a SIGTERM signal. kill &lt;PID&gt;#!/bin/bash# Function called upon receiving SIGTERMfunction terminateMsg {    echo \"Received SIGTERM, shutting down the server gracefully...\"    # Here you can add commands to close connections or save the state    exit}# Sets up the trap for SIGTERMtrap terminateMsg SIGTERMecho \"Starting web server...\"while true; do    echo \"Server is running... (PID: $$)\"    sleep 2  # Simulates the server's running timedoneSIGHUBThis script starts a daemon that runs indefinitely and reloads its configuration when it receives a SIGHUP signal. kill -HUP &lt;PID&gt;#!/bin/bash# Function called upon receiving SIGHUPfunction reloadMsg {    echo \"Received SIGHUP, reloading configuration...\"    # Here you can add commands to reload the configurations    # Example: source /etc/mydaemon/config.conf}# Sets up the trap for SIGHUPtrap reloadMsg SIGHUPecho \"Starting my daemon...\"while true; do    echo \"Daemon is running... (PID: $$)\"    sleep 5  # Simulates the daemon's running timedone3. background processes#!/bin/bashecho \"Starting background processes...\"# Process 1sleep 3 &amp;  # This simulates a long-running taskpid1=$!  # Get the process ID of process 1# Process 2sleep 9 &amp;  # This simulates a shorter taskpid2=$!  # Get the process ID of process 2# Wait for process 1 to finish and notifywait $pid1echo \"Process 1 has completed.\"# Wait for process 2 to finish and notifywait $pid2echo \"Process 2 has completed.\"echo \"All processes have finished.\"4. debugging#!/bin/bashset -x  # Enable debugging modeecho \"Starting the script...\"echo \"Doing something...\"sleep 1echo \"Ending the script.\"set +x  # Disable debugging modeecho \"now debugging mode is disable\"echo \"did you understand ?\"5. String Manipulation and Substitution#!/bin/bash# Defining an original stringoriginal=\"Linux is amazing!\"# Converting to uppercaseuppercase=${original^^}echo \"Uppercase: $uppercase\"# Output: Uppercase: LINUX IS AMAZING!# Converting to lowercaselowercase=${original,,}echo \"Lowercase: $lowercase\"# Output: Lowercase: linux is amazing!# Replacing part of the stringmodified=${original//amazing/extravagant}echo \"Substitution: $modified\"# Output: Substitution: Linux is extravagant!# Extracting a substringsubstring=${original:7:9}  # Extracts \"is amazing\"echo \"Substring: $substring\"# Output: Substring: is amazing# Checking the length of the stringlength=${#original}echo \"Length of the string: $length characters\"# Output: Length of the string: 20 characters",
            "content_html": "<h2 id=\"-spells-for-linux-shell\">ü™Ñ Spells for Linux Shell</h2><h3 id=\"1-looping-command-x-script\">1. LOOPING: command x script</h3><h4 id=\"command\">command</h4><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..5<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do </span><span class=\"nb\">echo</span> <span class=\"nv\">$i</span><span class=\"p\">;</span> <span class=\"k\">done</span><span class=\"nv\">count</span><span class=\"o\">=</span>1<span class=\"p\">;</span> <span class=\"k\">while</span> <span class=\"o\">[</span> <span class=\"nv\">$count</span> <span class=\"nt\">-le</span> 5 <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">do </span><span class=\"nb\">echo</span> <span class=\"nv\">$count</span><span class=\"p\">;</span> <span class=\"o\">((</span>count++<span class=\"o\">))</span><span class=\"p\">;</span> <span class=\"k\">done</span></code></pre></div></div><h4 id=\"script\">script</h4><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Counting to 5 with a for loop:\"</span><span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..5<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"nv\">$i</span><span class=\"k\">done</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Counting to 5 with a while loop:\"</span><span class=\"nv\">count</span><span class=\"o\">=</span>1<span class=\"k\">while</span> <span class=\"o\">[</span> <span class=\"nv\">$count</span> <span class=\"nt\">-le</span> 5 <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"nv\">$count</span>    <span class=\"o\">((</span>count++<span class=\"o\">))</span><span class=\"k\">done</span></code></pre></div></div><h3 id=\"2-signals\">2. signals</h3><p>Some scenarios that make sense to deal with signals..</p><ul>  <li>Cleaning Temporary Files</li>  <li>Graceful Interruption of Services</li>  <li>Avoiding Database Corruption</li>  <li>Releasing Network or Hardware Resources</li>  <li>Maintaining Consistent Variable State-</li></ul><h4 id=\"sigint\">SIGINT</h4><p>This script creates a temporary file and displays a goodbye message when it receives a SIGINT signal (e.g., when the user presses Ctrl+C).</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function that will be called when the script receives the SIGINT signal</span><span class=\"k\">function </span>exitMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"you sent a signal to end, byby !!\"</span>    <span class=\"c\"># command here</span>    <span class=\"nb\">exit</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap to call the cleanup function when the script receives SIGINT</span><span class=\"nb\">trap </span>exitMsg SIGINT<span class=\"nb\">sleep </span>5<span class=\"nb\">echo</span> <span class=\"s2\">\"Creating a temporary file...\"</span><span class=\"nb\">touch</span> /tmp/apolzek <span class=\"o\">||</span> <span class=\"nb\">exit </span>1  <span class=\"c\"># Creates a temporary file or exits with an error</span></code></pre></div></div><h4 id=\"sigterm\">SIGTERM</h4><p>This script starts a web server that runs indefinitely and shuts down gracefully when it receives a SIGTERM signal. <code class=\"language-plaintext highlighter-rouge\">kill &lt;PID&gt;</code></p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function called upon receiving SIGTERM</span><span class=\"k\">function </span>terminateMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"Received SIGTERM, shutting down the server gracefully...\"</span>    <span class=\"c\"># Here you can add commands to close connections or save the state</span>    <span class=\"nb\">exit</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap for SIGTERM</span><span class=\"nb\">trap </span>terminateMsg SIGTERM<span class=\"nb\">echo</span> <span class=\"s2\">\"Starting web server...\"</span><span class=\"k\">while </span><span class=\"nb\">true</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"Server is running... (PID: </span><span class=\"nv\">$$</span><span class=\"s2\">)\"</span>    <span class=\"nb\">sleep </span>2  <span class=\"c\"># Simulates the server's running time</span><span class=\"k\">done</span></code></pre></div></div><h4 id=\"sighub\">SIGHUB</h4><p>This script starts a daemon that runs indefinitely and reloads its configuration when it receives a SIGHUP signal. <code class=\"language-plaintext highlighter-rouge\">kill -HUP &lt;PID&gt;</code></p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Function called upon receiving SIGHUP</span><span class=\"k\">function </span>reloadMsg <span class=\"o\">{</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"Received SIGHUP, reloading configuration...\"</span>    <span class=\"c\"># Here you can add commands to reload the configurations</span>    <span class=\"c\"># Example: source /etc/mydaemon/config.conf</span><span class=\"o\">}</span><span class=\"c\"># Sets up the trap for SIGHUP</span><span class=\"nb\">trap </span>reloadMsg SIGHUP<span class=\"nb\">echo</span> <span class=\"s2\">\"Starting my daemon...\"</span><span class=\"k\">while </span><span class=\"nb\">true</span><span class=\"p\">;</span> <span class=\"k\">do    </span><span class=\"nb\">echo</span> <span class=\"s2\">\"Daemon is running... (PID: </span><span class=\"nv\">$$</span><span class=\"s2\">)\"</span>    <span class=\"nb\">sleep </span>5  <span class=\"c\"># Simulates the daemon's running time</span><span class=\"k\">done</span></code></pre></div></div><h3 id=\"3-background-processes\">3. background processes</h3><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Starting background processes...\"</span><span class=\"c\"># Process 1</span><span class=\"nb\">sleep </span>3 &amp;  <span class=\"c\"># This simulates a long-running task</span><span class=\"nv\">pid1</span><span class=\"o\">=</span><span class=\"nv\">$!</span>  <span class=\"c\"># Get the process ID of process 1</span><span class=\"c\"># Process 2</span><span class=\"nb\">sleep </span>9 &amp;  <span class=\"c\"># This simulates a shorter task</span><span class=\"nv\">pid2</span><span class=\"o\">=</span><span class=\"nv\">$!</span>  <span class=\"c\"># Get the process ID of process 2</span><span class=\"c\"># Wait for process 1 to finish and notify</span><span class=\"nb\">wait</span> <span class=\"nv\">$pid1</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Process 1 has completed.\"</span><span class=\"c\"># Wait for process 2 to finish and notify</span><span class=\"nb\">wait</span> <span class=\"nv\">$pid2</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Process 2 has completed.\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"All processes have finished.\"</span></code></pre></div></div><h3 id=\"4-debugging\">4. debugging</h3><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"nb\">set</span> <span class=\"nt\">-x</span>  <span class=\"c\"># Enable debugging mode</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Starting the script...\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Doing something...\"</span><span class=\"nb\">sleep </span>1<span class=\"nb\">echo</span> <span class=\"s2\">\"Ending the script.\"</span><span class=\"nb\">set</span> +x  <span class=\"c\"># Disable debugging mode</span><span class=\"nb\">echo</span> <span class=\"s2\">\"now debugging mode is disable\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"did you understand ?\"</span></code></pre></div></div><h3 id=\"5-string-manipulation-and-substitution\">5. String Manipulation and Substitution</h3><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span><span class=\"c\"># Defining an original string</span><span class=\"nv\">original</span><span class=\"o\">=</span><span class=\"s2\">\"Linux is amazing!\"</span><span class=\"c\"># Converting to uppercase</span><span class=\"nv\">uppercase</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">^^</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Uppercase: </span><span class=\"nv\">$uppercase</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Uppercase: LINUX IS AMAZING!</span><span class=\"c\"># Converting to lowercase</span><span class=\"nv\">lowercase</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">,,</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Lowercase: </span><span class=\"nv\">$lowercase</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Lowercase: linux is amazing!</span><span class=\"c\"># Replacing part of the string</span><span class=\"nv\">modified</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span><span class=\"p\">//amazing/extravagant</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Substitution: </span><span class=\"nv\">$modified</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Substitution: Linux is extravagant!</span><span class=\"c\"># Extracting a substring</span><span class=\"nv\">substring</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">original</span>:7:9<span class=\"k\">}</span>  <span class=\"c\"># Extracts \"is amazing\"</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Substring: </span><span class=\"nv\">$substring</span><span class=\"s2\">\"</span><span class=\"c\"># Output: Substring: is amazing</span><span class=\"c\"># Checking the length of the string</span><span class=\"nv\">length</span><span class=\"o\">=</span><span class=\"k\">${#</span><span class=\"nv\">original</span><span class=\"k\">}</span><span class=\"nb\">echo</span> <span class=\"s2\">\"Length of the string: </span><span class=\"nv\">$length</span><span class=\"s2\"> characters\"</span><span class=\"c\"># Output: Length of the string: 20 characters</span></code></pre></div></div>",
            "url": "/2024/09/29/spells-for-linux-shell.html",
            
            
            
            "tags": ["linux","shell","script","bash"],
            
            "date_published": "2024-09-29T00:00:00+00:00",
            "date_modified": "2024-09-29T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/09/29/kafka-on-kubernetes-using-kind.html",
            "title": "Kafka on Kubernetes using Kind",
            "summary": null,
            "content_text": "Kafka on Kubernetes using KindRecently, I came across an article on Medium that explains how to set up a local environment with Kafka in a test folder, using the Kubernetes StatefulSet concept. I decided to bring it to my blog, adding some tips and additional information. Although my knowledge of Kafka is still basic, I already have a few applications in production using this tool ‚Äî and it‚Äôs amazing how well they perform. I was also responsible for creating a solution using Kafka Connect, Kafka, and CDC for the Boleto product.What follows is a simple and easy way to set up a local development environment with Kafka. I‚Äôll also provide a brief introduction to Kafka and explain how you can use this solution in your projects. It‚Äôs important to note that this article is not a ‚ÄúHow to run Kafka in production‚Äù guide, so keep that in mind. I may also add a Kafka command cheatsheet to make my life easier in the future.If you‚Äôre not familiar with Kind, it‚Äôs an easy way to run Kubernetes locally. With Kind, you can use specific Kubernetes versions, configure the network to expose the API server or ports to the host, upload images, and do many other things. Simply install Docker or Podman first, and then install Kind. It‚Äôs a straightforward and easy process.1) Create kind configurationkind-config.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  extraPortMappings:  - containerPort: 30092    hostPort: 30092    listenAddress: \"0.0.0.0\" # Optional, defaults to \"0.0.0.0\"    protocol: tcp # Optional, defaults to tcp- role: worker- role: worker- role: worker2) Create a kind clusterkind create cluster --config kind-config.yaml --name my-cluster3) Create kafka StatefulSet and Namespacekafka.yamlapiVersion: v1kind: Namespacemetadata:  name: kafka  labels:    name: kafka---apiVersion: apps/v1kind: StatefulSetmetadata:  name: kafka  namespace: kafka  labels:    app: kafka-appspec:  serviceName: kafka-svc  replicas: 3  selector:    matchLabels:      app: kafka-app  template:    metadata:      labels:        app: kafka-app    spec:      containers:        - name: kafka-container          image: doughgle/kafka-kraft          ports:            - containerPort: 9092            - containerPort: 9093          env:            - name: REPLICAS              value: '3'            - name: SERVICE              value: kafka-svc            - name: NAMESPACE              value: kafka            - name: SHARE_DIR              value: /mnt/kafka            - name: CLUSTER_ID              value: bXktY2x1c3Rlci0xMjM0NQ==            - name: DEFAULT_REPLICATION_FACTOR              value: '3'            - name: DEFAULT_MIN_INSYNC_REPLICAS              value: '2'          volumeMounts:            - name: data              mountPath: /mnt/kafka  volumeClaimTemplates:    - metadata:        name: data      spec:        accessModes:          - \"ReadWriteOnce\"        resources:          requests:            storage: \"1Gi\"---apiVersion: v1kind: Servicemetadata:  name: kafka-svc  namespace: kafka  labels:    app: kafka-appspec:  type: NodePort  ports:    - name: '9092'      port: 9092      protocol: TCP      targetPort: 9092      nodePort: 30092  selector:    app: kafka-app4) Create a topickubectl exec -it kafka-0 -n kafka -- bashkafka-topics.sh --create --topic my-topic --bootstrap-server kafka-svc:9092kafka-topics.sh --list --topic my-topic --bootstrap-server kafka-svc:90925) Produce and consume messagekubectl exec -it kafka-1 -n kafka -- bashkafka-console-producer.sh  --bootstrap-server kafka-svc:9092 --topic my-topickafka-console-consumer.sh --bootstrap-server kafka-svc:9092 --topic my-topic6) Delete topickafka-topics.sh --delete --topic my-topic --bootstrap-server kafka-svc:9092Kafka KRaft x Kafka with ZooKeeperKafka KRaft Installation: KRaft is Kafka‚Äôs new built-in consensus mechanism that eliminates the need for ZooKeeper. In a KRaft-based installation, Kafka brokers manage metadata and leader election directly, which simplifies the architecture by reducing dependencies. KRaft is becoming the default option in newer Kafka versions because it provides better scalability, faster failover, and an overall more streamlined operation.Kafka with ZooKeeper: In traditional Kafka deployments, ZooKeeper is used to manage the cluster‚Äôs metadata, such as broker details and topic configurations. ZooKeeper handles tasks like leader election and tracking which brokers are active. While this setup has been robust for years, it adds complexity by requiring an additional service (ZooKeeper) that must be installed, managed, and maintained alongside Kafka.Reviewing            Term      Definition                  Topic      A category or feed name to which records are published.              Partition      A topic is divided into multiple partitions, which are the fundamental unit of parallelism in Kafka.              Producer      An application that publishes messages (records) to a Kafka topic.              Consumer      An application that subscribes to topics and processes the published messages.              Broker      A Kafka server that stores messages and serves client requests.              Cluster      A group of one or more brokers working together to handle data replication and load balancing.              Offset      A unique identifier for each record within a partition, used for tracking the position of messages.              Consumer Group      A group of consumers that work together to consume messages from a topic, ensuring load balancing.              Replication      The process of storing copies of partitions across multiple brokers for fault tolerance.              Zookeeper      A centralized service that manages and coordinates the Kafka brokers and maintains cluster metadata.              Log      An ordered, append-only sequence of records for each partition that stores the actual messages.              Retention Policy      A policy that determines how long Kafka retains messages in a topic before they are deleted.              Throughput      A measure of how many messages can be processed per unit of time, often expressed in messages per second.              Latency      The time it takes for a message to travel from a producer to a consumer.              Load Balancing      The distribution of partitions across multiple brokers to evenly distribute the workload.              Partitioning Strategy      The method used to assign messages to partitions, based on keys or round-robin distribution.              Replication Factor      The number of copies of a partition maintained across different brokers, enhancing fault tolerance.              Consumer Lag      The difference between the last produced message offset and the last consumed message offset in a consumer group.              Auto-Scaling      The ability of a Kafka cluster to dynamically adjust its size based on workload and resource utilization.              Compact Topics      A feature that retains only the most recent value for each key in a topic, reducing storage requirements.              Kafka Streams      A client library for building real-time applications and microservices that process data in Kafka.              State Store      A key-value store used in Kafka Streams for maintaining local state during processing.              Transaction Support      Kafka‚Äôs ability to handle multi-producer and multi-consumer transactions, ensuring data integrity.              Cross-Cluster Replication      The ability to replicate data across different Kafka clusters for disaster recovery and geo-replication.              Schema Registry      A centralized repository for managing data schemas, allowing producers and consumers to handle data evolution.              Backpressure Handling      Mechanisms for managing the flow of data between producers and consumers to prevent overwhelming consumers.              Topic Compaction      The process of removing older records with the same key, retaining only the latest record for each key.      ",
            "content_html": "<h2 id=\"kafka-on-kubernetes-using-kind\">Kafka on Kubernetes using Kind</h2><p>Recently, I came across an article on Medium that explains how to set up a local environment with Kafka in a test folder, using the Kubernetes StatefulSet concept. I decided to bring it to my blog, adding some tips and additional information. Although my knowledge of Kafka is still basic, I already have a few applications in production using this tool ‚Äî and it‚Äôs amazing how well they perform. I was also responsible for creating a solution using Kafka Connect, Kafka, and CDC for the Boleto product.</p><p>What follows is a simple and easy way to set up a local development environment with Kafka. I‚Äôll also provide a brief introduction to Kafka and explain how you can use this solution in your projects. It‚Äôs important to note that this article is not a ‚ÄúHow to run Kafka in production‚Äù guide, so keep that in mind. I may also add a Kafka command cheatsheet to make my life easier in the future.</p><p>If you‚Äôre not familiar with Kind, it‚Äôs an easy way to run Kubernetes locally. With Kind, you can use specific Kubernetes versions, configure the network to expose the API server or ports to the host, upload images, and do many other things. Simply install Docker or Podman first, and then install Kind. It‚Äôs a straightforward and easy process.</p><p>1) Create kind configuration</p><p><em>kind-config.yaml</em></p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">kind.x-k8s.io/v1alpha4</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Cluster</span><span class=\"na\">nodes</span><span class=\"pi\">:</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">control-plane</span>  <span class=\"na\">extraPortMappings</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>    <span class=\"na\">hostPort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>    <span class=\"na\">listenAddress</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">0.0.0.0\"</span> <span class=\"c1\"># Optional, defaults to \"0.0.0.0\"</span>    <span class=\"na\">protocol</span><span class=\"pi\">:</span> <span class=\"s\">tcp</span> <span class=\"c1\"># Optional, defaults to tcp</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span><span class=\"pi\">-</span> <span class=\"na\">role</span><span class=\"pi\">:</span> <span class=\"s\">worker</span></code></pre></div></div><p>2) Create a kind cluster</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kind create cluster <span class=\"nt\">--config</span> kind-config.yaml <span class=\"nt\">--name</span> my-cluster</code></pre></div></div><p>3) Create kafka StatefulSet and Namespace</p><p><em>kafka.yaml</em></p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Namespace</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">StatefulSet</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">serviceName</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>  <span class=\"na\">replicas</span><span class=\"pi\">:</span> <span class=\"m\">3</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">matchLabels</span><span class=\"pi\">:</span>      <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span>  <span class=\"na\">template</span><span class=\"pi\">:</span>    <span class=\"na\">metadata</span><span class=\"pi\">:</span>      <span class=\"na\">labels</span><span class=\"pi\">:</span>        <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span>    <span class=\"na\">spec</span><span class=\"pi\">:</span>      <span class=\"na\">containers</span><span class=\"pi\">:</span>        <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka-container</span>          <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">doughgle/kafka-kraft</span>          <span class=\"na\">ports</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>            <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"m\">9093</span>          <span class=\"na\">env</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">REPLICAS</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">3'</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">SERVICE</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">NAMESPACE</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">SHARE_DIR</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">/mnt/kafka</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">CLUSTER_ID</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s\">bXktY2x1c3Rlci0xMjM0NQ==</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">DEFAULT_REPLICATION_FACTOR</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">3'</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">DEFAULT_MIN_INSYNC_REPLICAS</span>              <span class=\"na\">value</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">2'</span>          <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span>            <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">data</span>              <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/mnt/kafka</span>  <span class=\"na\">volumeClaimTemplates</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span>        <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">data</span>      <span class=\"na\">spec</span><span class=\"pi\">:</span>        <span class=\"na\">accessModes</span><span class=\"pi\">:</span>          <span class=\"pi\">-</span> <span class=\"s2\">\"</span><span class=\"s\">ReadWriteOnce\"</span>        <span class=\"na\">resources</span><span class=\"pi\">:</span>          <span class=\"na\">requests</span><span class=\"pi\">:</span>            <span class=\"na\">storage</span><span class=\"pi\">:</span> <span class=\"s2\">\"</span><span class=\"s\">1Gi\"</span><span class=\"nn\">---</span><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Service</span><span class=\"na\">metadata</span><span class=\"pi\">:</span>  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">kafka-svc</span>  <span class=\"na\">namespace</span><span class=\"pi\">:</span> <span class=\"s\">kafka</span>  <span class=\"na\">labels</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span><span class=\"na\">spec</span><span class=\"pi\">:</span>  <span class=\"na\">type</span><span class=\"pi\">:</span> <span class=\"s\">NodePort</span>  <span class=\"na\">ports</span><span class=\"pi\">:</span>    <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">9092'</span>      <span class=\"na\">port</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>      <span class=\"na\">protocol</span><span class=\"pi\">:</span> <span class=\"s\">TCP</span>      <span class=\"na\">targetPort</span><span class=\"pi\">:</span> <span class=\"m\">9092</span>      <span class=\"na\">nodePort</span><span class=\"pi\">:</span> <span class=\"m\">30092</span>  <span class=\"na\">selector</span><span class=\"pi\">:</span>    <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">kafka-app</span></code></pre></div></div><p>4) Create a topic</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl <span class=\"nb\">exec</span> <span class=\"nt\">-it</span> kafka-0 <span class=\"nt\">-n</span> kafka <span class=\"nt\">--</span> bashkafka-topics.sh <span class=\"nt\">--create</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092kafka-topics.sh <span class=\"nt\">--list</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092</code></pre></div></div><p>5) Produce and consume message</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl <span class=\"nb\">exec</span> <span class=\"nt\">-it</span> kafka-1 <span class=\"nt\">-n</span> kafka <span class=\"nt\">--</span> bashkafka-console-producer.sh  <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092 <span class=\"nt\">--topic</span> my-topickafka-console-consumer.sh <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092 <span class=\"nt\">--topic</span> my-topic</code></pre></div></div><p>6) Delete topic</p><div class=\"language-sh highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kafka-topics.sh <span class=\"nt\">--delete</span> <span class=\"nt\">--topic</span> my-topic <span class=\"nt\">--bootstrap-server</span> kafka-svc:9092</code></pre></div></div><h2 id=\"kafka-kraft-x-kafka-with-zookeeper\">Kafka KRaft x Kafka with ZooKeeper</h2><p><strong>Kafka KRaft Installation</strong>: KRaft is Kafka‚Äôs new built-in consensus mechanism that eliminates the need for ZooKeeper. In a KRaft-based installation, Kafka brokers manage metadata and leader election directly, which simplifies the architecture by reducing dependencies. KRaft is becoming the default option in newer Kafka versions because it provides better scalability, faster failover, and an overall more streamlined operation.</p><p><strong>Kafka with ZooKeeper</strong>: In traditional Kafka deployments, ZooKeeper is used to manage the cluster‚Äôs metadata, such as broker details and topic configurations. ZooKeeper handles tasks like leader election and tracking which brokers are active. While this setup has been robust for years, it adds complexity by requiring an additional service (ZooKeeper) that must be installed, managed, and maintained alongside Kafka.</p><h2 id=\"reviewing\">Reviewing</h2><table>  <thead>    <tr>      <th>Term</th>      <th>Definition</th>    </tr>  </thead>  <tbody>    <tr>      <td><strong>Topic</strong></td>      <td>A category or feed name to which records are published.</td>    </tr>    <tr>      <td><strong>Partition</strong></td>      <td>A topic is divided into multiple partitions, which are the fundamental unit of parallelism in Kafka.</td>    </tr>    <tr>      <td><strong>Producer</strong></td>      <td>An application that publishes messages (records) to a Kafka topic.</td>    </tr>    <tr>      <td><strong>Consumer</strong></td>      <td>An application that subscribes to topics and processes the published messages.</td>    </tr>    <tr>      <td><strong>Broker</strong></td>      <td>A Kafka server that stores messages and serves client requests.</td>    </tr>    <tr>      <td><strong>Cluster</strong></td>      <td>A group of one or more brokers working together to handle data replication and load balancing.</td>    </tr>    <tr>      <td><strong>Offset</strong></td>      <td>A unique identifier for each record within a partition, used for tracking the position of messages.</td>    </tr>    <tr>      <td><strong>Consumer Group</strong></td>      <td>A group of consumers that work together to consume messages from a topic, ensuring load balancing.</td>    </tr>    <tr>      <td><strong>Replication</strong></td>      <td>The process of storing copies of partitions across multiple brokers for fault tolerance.</td>    </tr>    <tr>      <td><strong>Zookeeper</strong></td>      <td>A centralized service that manages and coordinates the Kafka brokers and maintains cluster metadata.</td>    </tr>    <tr>      <td><strong>Log</strong></td>      <td>An ordered, append-only sequence of records for each partition that stores the actual messages.</td>    </tr>    <tr>      <td><strong>Retention Policy</strong></td>      <td>A policy that determines how long Kafka retains messages in a topic before they are deleted.</td>    </tr>    <tr>      <td><strong>Throughput</strong></td>      <td>A measure of how many messages can be processed per unit of time, often expressed in messages per second.</td>    </tr>    <tr>      <td><strong>Latency</strong></td>      <td>The time it takes for a message to travel from a producer to a consumer.</td>    </tr>    <tr>      <td><strong>Load Balancing</strong></td>      <td>The distribution of partitions across multiple brokers to evenly distribute the workload.</td>    </tr>    <tr>      <td><strong>Partitioning Strategy</strong></td>      <td>The method used to assign messages to partitions, based on keys or round-robin distribution.</td>    </tr>    <tr>      <td><strong>Replication Factor</strong></td>      <td>The number of copies of a partition maintained across different brokers, enhancing fault tolerance.</td>    </tr>    <tr>      <td><strong>Consumer Lag</strong></td>      <td>The difference between the last produced message offset and the last consumed message offset in a consumer group.</td>    </tr>    <tr>      <td><strong>Auto-Scaling</strong></td>      <td>The ability of a Kafka cluster to dynamically adjust its size based on workload and resource utilization.</td>    </tr>    <tr>      <td><strong>Compact Topics</strong></td>      <td>A feature that retains only the most recent value for each key in a topic, reducing storage requirements.</td>    </tr>    <tr>      <td><strong>Kafka Streams</strong></td>      <td>A client library for building real-time applications and microservices that process data in Kafka.</td>    </tr>    <tr>      <td><strong>State Store</strong></td>      <td>A key-value store used in Kafka Streams for maintaining local state during processing.</td>    </tr>    <tr>      <td><strong>Transaction Support</strong></td>      <td>Kafka‚Äôs ability to handle multi-producer and multi-consumer transactions, ensuring data integrity.</td>    </tr>    <tr>      <td><strong>Cross-Cluster Replication</strong></td>      <td>The ability to replicate data across different Kafka clusters for disaster recovery and geo-replication.</td>    </tr>    <tr>      <td><strong>Schema Registry</strong></td>      <td>A centralized repository for managing data schemas, allowing producers and consumers to handle data evolution.</td>    </tr>    <tr>      <td><strong>Backpressure Handling</strong></td>      <td>Mechanisms for managing the flow of data between producers and consumers to prevent overwhelming consumers.</td>    </tr>    <tr>      <td><strong>Topic Compaction</strong></td>      <td>The process of removing older records with the same key, retaining only the latest record for each key.</td>    </tr>  </tbody></table>",
            "url": "/2024/09/29/kafka-on-kubernetes-using-kind.html",
            
            
            
            "tags": ["kafka","kind","kubernetes"],
            
            "date_published": "2024-09-29T00:00:00+00:00",
            "date_modified": "2024-09-29T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/09/27/l0calh0t-startup-day1.html",
            "title": "üí° l0calh0t Startup - day 1",
            "summary": "Building the l0calh0t Startup",
            "content_text": "About the projectI recently saw a project on YouTube where the YouTuber created a complete technology environment to support a fake product. I liked the idea and thought about creating a real project‚Ä¶ starting from localhost to production. I don‚Äôt consider myself a good programmer, but I‚Äôm looking for a well-made bean and rice. My main focus is to launch a stable product with acceptable security standards and that simply implements the idea as it is now in my mind. I present to you the l0calh0t startup. You can follow the progress of this through a series of articles on this blog!note: don‚Äôt take me too seriouslyI know there are already similar projects on the internet, but most of them focus on development and not on operations. I have two main goals. The first is to test technologies, discuss decisions and show concerns that usually arise when our project goes into production. The second is to perhaps make some money with this. My initial focus is Brazilian software developers. In general, I get along well with developers. I believe it is because every day I help them solve problems. Developers change, but the problems are usually the same.Let‚Äôs get started. What follows is information about the startup. To be quite honest, what I intend to build already exists, something similar to render.com.. but my ideas go a little further(if I can implement it, of course xD).l0calh0t üöÄüöÄüöÄAbout the startupl0calh0t is not introducing an innovative solution but rather offering a new way of delivering container-based application hosting, designed for developers of all levels, SREs, DevOps, and QAs. By abstracting infrastructure and networking, it enables fast, cost-effective container deployment. Users can make applications publicly accessible or control access as needed. Unlike traditional solutions focused on large enterprises with strict SLAs, l0calh0t prioritizes simplicity and accessibility, creating an agile environment for rapid testing and iteration, without the complexity of managing traditional servers.I‚Äôm still thinking about the legal issues..About the ideaThe idea is basically a render.com with some differences. I want to do something more ‚Äúapi first‚Äù. I want to do it in a way that is similar to the way people work with containers locally and I also want to offer a way for them to have details about their applications by adding components to the infrastructure that abstract away the complexity of doing so.What do I need?Considering that I have no money, no computing resources, and no advanced programming skills (that makes it hard ü§£ü§£), I need to focus on something simple that works. I want to create a business abstraction on top of Kubernetes and put all the complexity into it using tools I have experience with. I need a cheap domain like localhot.io. I need a payment method and reasonable bandwidth. I want to physically separate the servers where the startup‚Äôs applications are from the servers where the end-user containers will be. I need to think about security and a business model that is viable for Brazilian developers.For the first poc‚Äôs, the simpler the better. I intend to use python or golang for this. I will use kind as the local kubernetes environment, the local registry of my machine for the images and also my local network (I run a certain risk).The networking part is very important in this project, but first I want a functional MVP.About my localhost (workstation)My computer settings are  Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz;  16 RAM;  440G SSD;  EndeavourOS[2024-10-02 15:27:45] UPDATE - I‚Äôve decided that a hardware upgrade is unfeasible‚Ä¶ It‚Äôs time to build a new PC. !!!End(?)So, that‚Äôs it. I‚Äôve reached the end of my first article. I hope I‚Äôve been clear about my idea. In the end, even if everything goes wrong, I will have gained valuable knowledge. These notes may be useful to someone else, and even to my future self. What‚Äôs coming next? A series of reflections, articles about random tools, and some not-so-professional code (:See you around !",
            "content_html": "<h2 id=\"about-the-project\">About the project</h2><p>I recently saw a project on YouTube where the YouTuber created a complete technology environment to support a fake product. I liked the idea and thought about creating a real project‚Ä¶ starting from localhost to production. I don‚Äôt consider myself a good programmer, but I‚Äôm looking for a well-made bean and rice. My main focus is to launch a stable product with acceptable security standards and that simply implements the idea as it is now in my mind. I present to you the <strong>l0calh0t</strong> startup. You can follow the progress of this through a series of articles on this blog!</p><p><strong>note</strong>: <em>don‚Äôt take me too seriously</em></p><p>I know there are already similar projects on the internet, but most of them focus on <em>development</em> and not on <em>operations</em>. I have two main goals. The first is to test technologies, discuss decisions and show concerns that usually arise when our project goes into production. The second is to perhaps make some money with this. My initial focus is Brazilian software developers. In general, I get along well with developers. I believe it is because every day I help them solve problems. Developers change, but the problems are usually the same.</p><p>Let‚Äôs get started. What follows is information about the startup. To be quite honest, what I intend to build already exists, something similar to <em>render.com</em>.. but my ideas go a little further(if I can implement it, of course xD).</p><h2 id=\"l0calh0t-\">l0calh0t üöÄüöÄüöÄ</h2><h3 id=\"about-the-startup\">About the startup</h3><p><strong>l0calh0t</strong> is not introducing an innovative solution but rather offering <strong>a new way of delivering container-based application hosting</strong>, designed for developers of all levels, SREs, DevOps, and QAs. By abstracting infrastructure and networking, it enables fast, cost-effective container deployment. Users can make applications publicly accessible or control access as needed. Unlike traditional solutions focused on large enterprises with strict SLAs, <strong>l0calh0t</strong> prioritizes simplicity and accessibility, creating an agile environment for rapid testing and iteration, without the complexity of managing traditional servers.</p><p>I‚Äôm still thinking about the legal issues..</p><h3 id=\"about-the-idea\">About the idea</h3><p>The idea is basically a <em>render.com</em> with some differences. I want to do something more ‚Äúapi first‚Äù. I want to do it in a way that is similar to the way people work with containers locally and I also want to offer a way for them to have details about their applications by adding components to the infrastructure that abstract away the complexity of doing so.</p><h3 id=\"what-do-i-need\">What do I need?</h3><p>Considering that I have no money, no computing resources, and no advanced programming skills (that makes it hard ü§£ü§£), I need to focus on something simple that works. I want to create a business abstraction on top of Kubernetes and put all the complexity into it using tools I have experience with. I need a cheap domain like <strong>localhot.io</strong>. I need a payment method and reasonable bandwidth. I want to physically separate the servers where the startup‚Äôs applications are from the servers where the end-user containers will be. I need to think about security and a business model that is viable for Brazilian developers.</p><p>For the first poc‚Äôs, the simpler the better. I intend to use python or golang for this. I will use kind as the local kubernetes environment, the local registry of my machine for the images and also my local network (I run a certain risk).The networking part is very important in this project, but first I want a functional MVP.</p><h3 id=\"about-my-localhost-workstation\">About my localhost (workstation)</h3><p>My computer settings are</p><ul>  <li>Intel(R) Core(TM) i5-9400 CPU @ 2.90GHz;</li>  <li>16 RAM;</li>  <li>440G SSD;</li>  <li>EndeavourOS</li></ul><p><strong>[2024-10-02 15:27:45]</strong> UPDATE - <em>I‚Äôve decided that a hardware upgrade is unfeasible‚Ä¶ It‚Äôs time to build a new PC. !!!</em></p><h3 id=\"end\">End(?)</h3><p>So, that‚Äôs it. I‚Äôve reached the end of my first article. I hope I‚Äôve been clear about my idea. In the end, even if everything goes wrong, I will have gained valuable knowledge. These notes may be useful to someone else, and even to my future self. What‚Äôs coming next? A series of reflections, articles about random tools, and some not-so-professional code <strong>(</strong>:</p><p>See you around !</p><p><img src=\"https://raw.githubusercontent.com/apolzek/apolzek.github.io/refs/heads/main/assets/gif/working.webp\" alt=\"working\" /></p>",
            "url": "/2024/09/27/l0calh0t-startup-day1.html",
            
            
            
            "tags": ["devops","sre","project","startup"],
            
            "date_published": "2024-09-27T00:00:00+00:00",
            "date_modified": "2024-09-27T00:00:00+00:00",
            
                "author": 
                ""
                
            
        },
    
        {
            "id": "/2024/09/26/DISCLAIMER.html",
            "title": "DISCLAIMER *.*",
            "summary": "DISCLAIMER and GOALS",
            "content_text": "DISCLAIMER  I write for myself !!  My first language is Brazilian Portuguese, but I‚Äôm learning English and will use it here. You will likely see some writing mistakes; it‚Äôs part of learning  I have worked as a Reliability Engineer only in Brazil (until now)  I plan to discuss reliability topics and other subjects like security, processes, governance, and soft skills  Feedback is always welcome as long as it‚Äôs constructive. Feel free to contact me on Telegram or Discord, but please be respectful (@apolzek)  Remember, everything here is open to discussion. You should form your own opinionsGOALS  Talk about tools that are not well-known but have great potential  Track my studies with short reflective articles  Share my views on technologies, processes, and products  Soon, I will revisit my notes to see how my views on certain topics have changedREMEMBERüá∫üá∏ ‚ÄúThe only way to make things work well is by understanding why they break.‚Äùüáßüá∑ ‚ÄúA √∫nica maneira de fazer as coisas funcionarem bem √© entendendo o motivo pelo qual elas quebram.‚Äù",
            "content_html": "<h2 id=\"disclaimer\">DISCLAIMER</h2><ul>  <li>I write for myself !!</li>  <li>My first language is Brazilian Portuguese, but I‚Äôm learning English and will use it here. You will likely see some writing mistakes; it‚Äôs part of learning</li>  <li>I have worked as a Reliability Engineer only in Brazil (<em>until now</em>)</li>  <li>I plan to discuss reliability topics and other subjects like security, processes, governance, and soft skills</li>  <li>Feedback is always welcome as long as it‚Äôs constructive. Feel free to contact me on Telegram or Discord, but please be respectful (<em>@apolzek</em>)</li>  <li>Remember, everything here is open to discussion. You should form your <strong>own opinions</strong></li></ul><h2 id=\"goals\">GOALS</h2><ul>  <li>Talk about tools that are not well-known but have great potential</li>  <li>Track my studies with short reflective articles</li>  <li>Share my views on technologies, processes, and products</li>  <li>Soon, I will revisit my notes to see how my views on certain topics have changed</li></ul><h2 id=\"remember\">REMEMBER</h2><p>üá∫üá∏ ‚ÄúThe only way to make things work well is by understanding why they break.‚Äù</p><p>üáßüá∑ ‚ÄúA √∫nica maneira de fazer as coisas funcionarem bem √© entendendo o motivo pelo qual elas quebram.‚Äù</p>",
            "url": "/2024/09/26/DISCLAIMER.html",
            
            
            
            "tags": ["disclaimer","goals"],
            
            "date_published": "2024-09-26T00:00:00+00:00",
            "date_modified": "2024-09-26T00:00:00+00:00",
            
                "author": 
                ""
                
            
        }
    
    ]
}